<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Design Patterns for Orchestrating Black-Box Code Agents — noface</title>
  <meta name="description" content="Design Patterns for Orchestrating Black-Box Code Agents">
  <meta name="author" content="noface">
  <meta name="monowiki-base-url" content="/">
  <meta name="monowiki-note-slug" content="orchestration-survey">
  
  <link rel="stylesheet" href="/css/reset.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/previews.css">
  <link rel="stylesheet" href="/css/graph.css">
  <link rel="stylesheet" href="/css/search.css">

  <!-- Frontend bundle -->
  <script type="module" src="/js/bundle.js"></script>
</head>
<body>
  <header class="header">
    <div class="header-content">
      <nav class="nav">
        <a href="/index.html">home</a>
        
        <a href="https://github.com/femtomc/noface">github</a>
        <button id="search-trigger" class="search-trigger" aria-label="Search">
          <span class="search-trigger-text">search</span>
          <span class="search-trigger-hint">⌘K</span>
        </button>
      </nav>
    </div>
  </header>

  <main>
    <article>
      
      

      
      <p>
        Tags: <code>research</code> <code>agents</code> <code>orchestration</code> <code>survey</code> 
      </p>
      

      
        <div id="toc" class="toc-container">
          <nav class="toc-nav"><h3>Contents</h3><ul class="toc-list"><li class="toc-level-1"><a href="#design-patterns-for-orchestrating-black-box-code-agents-a-literature-survey">Design Patterns for Orchestrating Black-Box Code Agents: A Literature Survey</a></li><li class="toc-level-2"><a href="#task-assignment-and-granularity">Task Assignment and Granularity</a></li><li class="toc-level-2"><a href="#context-curation-providing-helpful-context-and-avoiding-harmful-noise">Context Curation: Providing Helpful Context (and Avoiding Harmful Noise)</a></li><li class="toc-level-2"><a href="#prompt-engineering-at-the-orchestration-level">Prompt Engineering at the Orchestration Level</a></li><li class="toc-level-2"><a href="#parallelism-conflict-avoidance">Parallelism &amp; Conflict Avoidance</a></li><li class="toc-level-2"><a href="#verification-acceptance-criteria">Verification &amp; Acceptance Criteria</a></li><li class="toc-level-2"><a href="#failure-modes-recovery-strategies">Failure Modes &amp; Recovery Strategies</a></li><li class="toc-level-2"><a href="#multi-pass-architectures-planner-implementer-reviewer-etc">Multi-Pass Architectures (Planner, Implementer, Reviewer, etc.)</a></li><li class="toc-level-2"><a href="#human-in-the-loop-in-orchestration">Human-in-the-Loop in Orchestration</a></li><li class="toc-level-2"><a href="#conclusion">Conclusion</a></li><li class="toc-level-2"><a href="#references">References</a></li></ul></nav>
        </div>
        

      <div class="page-subheader">
        
          <button id="copy-page-source" class="copy-page-btn" type="button" aria-label="Copy page Markdown">Copy page source</button>
          
        <button id="global-graph-toggle" class="graph-btn" type="button" aria-label="Open graph">Graph</button>
      </div>

      <h1 id="design-patterns-for-orchestrating-black-box-code-agents-a-literature-survey">Design Patterns for Orchestrating Black-Box Code Agents: A Literature Survey<a class="heading-anchor" href="#design-patterns-for-orchestrating-black-box-code-agents-a-literature-survey" aria-label="Link to heading">#</a></h1>
<p>Orchestrating black-box code-generation agents requires careful planning and robust design patterns to ensure these AI "developers" work effectively. Researchers and practitioners have proposed various strategies to assign tasks, manage context, engineer prompts, run agents in parallel safely, verify outputs, handle failures, and integrate multiple passes or human oversight. This survey reviews key patterns from academic literature and real-world case studies, organized by orchestration concern, and provides a pattern catalog of proven approaches.</p>
<h2 id="task-assignment-and-granularity">Task Assignment and Granularity<a class="heading-anchor" href="#task-assignment-and-granularity" aria-label="Link to heading">#</a></h2>
<p><strong>Dynamic Task Decomposition:</strong> A central pattern is to use an orchestrator agent (or component) that breaks down a software project into manageable subtasks and assigns them to specialized agents. Rather than having one AI attempt an entire project, orchestrators analyze the request and determine necessary subtasks (e.g. implement a feature, write tests, update docs). In Microsoft's taxonomy of agent architectures, an Orchestrator-Workers pattern is defined where a lead agent "dynamically decomposes tasks, delegates them to worker agents, and synthesizes their outputs". For example, in a coding context the orchestrator could decide which files or modules need changes and coordinate sub-agents for code generation, testing, and documentation. This dynamic assignment allows tackling large development goals by partitioning work.</p>
<p><strong>Specialization and Routing:</strong> Task assignment often leverages specialization. The orchestrator may classify each task and route it to an agent best suited for that category (using a Routing Workflow pattern) [@pujals2025agentic]. For instance, code-generation requests might go to a "Coder" agent, while bug-fix tasks route to a "Debugger" agent, and documentation tasks to a "Doc Writer" agent. The input can be analyzed (via rules or an LLM) to decide which agent is most appropriate. This ensures that each agent works on tasks aligned with its expertise, improving effectiveness. Ellipsis's production code review system is an example: it runs multiple specialized agents in parallel, each looking for a specific type of issue in a pull request (security, style, duplication, etc.), rather than one monolithic agent [@zenml2024ellipsis].</p>
<p><strong>Granularity – Sizing the Task:</strong> Deciding how large a task to give a single agent call is crucial. Literature suggests that tasks should be fine-grained enough to fit the agent's context window and capabilities, yet not so granular that the overhead of orchestration dominates. If a task is too large (e.g. "implement an entire feature spanning many files"), agents may struggle with context or produce incomplete solutions. Researchers have addressed this by explicit task planning. Self-planning code generation has the LLM first outline a plan of steps for a complex request, then tackle each step sequentially [@jiang2024selfplanning]. This two-phase approach of planning then coding yields up to 25.4% higher success (Pass@1) than one-shot generation for complex problems. Similarly, the Prompt-Chaining workflow breaks a complex job into a sequence of smaller subtasks, each handled by a dedicated prompt/agent call [@pujals2025agentic]. This not only makes each step easier for the model to execute but also improves transparency and debuggability. In practice, a rule of thumb is to treat one well-defined issue or function as a unit of work for an agent, and if the request spans multiple concerns or components, the orchestrator should subdivide it.</p>
<p><strong>Handling Dependencies and Readiness:</strong> An orchestrator must also respect task dependencies when assigning work. Some tasks can run in parallel (e.g. implement UI and backend separately), while others depend on earlier outputs (e.g. tests should run after code is written). One practical approach is to maintain a shared task board with statuses and dependencies. In a user's multi-agent Claude setup, tasks were tracked in a Markdown plan file with fields for Assigned Agent, Status, and Dependencies – e.g. "Write Integration Tests – Assigned to Validator – Pending (waiting for Builder to complete auth module)". This allows the orchestrator (and agents themselves) to know when a task is ready to start. If a task is too big or not well-defined, a planner agent might refine it further or ask for clarification before assignment. Ultimately, effective task assignment balances priority (which issues are most important), readiness (are prerequisites done?), and size (fits in one agent invocation).</p>
<h2 id="context-curation-providing-helpful-context-and-avoiding-harmful-noise">Context Curation: Providing Helpful Context (and Avoiding Harmful Noise)<a class="heading-anchor" href="#context-curation-providing-helpful-context-and-avoiding-harmful-noise" aria-label="Link to heading">#</a></h2>
<p><strong>Relevant Context Injections:</strong> The information an agent gets in its prompt (code context, design docs, examples) profoundly impacts its success. A key pattern is Retrieval-Augmented Code Generation (RACG), where the orchestrator fetches relevant snippets from the codebase or documentation to include in the agent's prompt. Real-world software tasks often require understanding cross-file dependencies and global architecture. Since an LLM has a limited context window, the orchestrator must be selective: for example, retrieving the function signature and related config file when asking the agent to implement a new feature in that function. A recent survey emphasizes that repository-level code generation needs to ensure global semantic consistency across files, and integrating a retrieval mechanism (e.g. searching a knowledge base of code) helps the LLM reason beyond a single file. In practice, tools like vector databases are used to find and feed the most relevant code pieces to the agent. Ellipsis's code review agents, for instance, use advanced code search backed by embeddings to find pertinent pieces of the codebase when analyzing a PR [@zenml2024ellipsis].</p>
<p><strong>Curating Helpful Examples and Constraints:</strong> Apart from code context, orchestrators often supply design documents, API usage examples, or style guides as context if they will guide the agent toward the right solution. Providing a few-shot example of a similar function or test can dramatically improve accuracy, as the agent can analogize from the given pattern. Additionally, context can include explicit constraints – e.g. a list of files that are in scope, or performance requirements. By giving the agent a clear problem statement along with these auxiliary materials, the orchestrator sets it up for success. For example, if generating a new module, the orchestrator might include an outline of the module's design or a simplified spec in the prompt. In PerfOrch (Performance-Oriented Orchestration), contextual knowledge about which LLMs perform best for a given language and problem type is stored and used to pick the right model for the task [@wang2025perforch]. This "strategic memory" of model strengths is a form of context that improves outcomes by dynamically selecting the optimal agent for each subtask.</p>
<p><strong>Avoiding Context Overload:</strong> Too much context can hurt agent performance. It might be tempting to provide every possibly relevant file to be safe, but large context size can confuse the model or dilute its focus. Recent studies have shown that even if all provided context is relevant, performance can degrade substantially (by 13–85%) as input length grows. In other words, beyond a certain point, the sheer volume of text can overwhelm the LLM's reasoning ("context dilution"). Thus, orchestrators should be judicious: include only the most relevant code and information. Irrelevant or contradictory context is even worse – it can lead the agent to draw false inferences or focus on the wrong problem. Best practices include using retrieval algorithms to filter context, and possibly summarizing or abstracting context to fit the window. For instance, rather than inserting an entire 500-line file, an orchestrator might insert a summary or the specific function signature needed. The Context-Augmentation pattern described by Hugging Face highlights that context expansion should be iterative and need-based: the agent identifies what extra info it needs, the orchestrator fetches it, and this cycle repeats, rather than dumping everything at once [@pujals2025agentic]. Techniques like embedding search, docstrings extraction, or code property graphs can help select which context truly matters.</p>
<p><strong>Detecting and Removing Harmful Context:</strong> Orchestrators may also implement "context pruning" – removing any information from the prompt that isn't consistent or necessary. This can include stale data (e.g. outdated function definitions if a newer version exists) or irrelevant conversation history in iterative settings. The goal is to minimize noise. If an agent will be doing a focused code edit, the orchestrator might strip out any unrelated instructions from the conversation and supply a fresh prompt with just the needed background. In summary, curated context is a double-edged sword: the right snippets and examples can greatly enhance success, but too much or wrong context can mislead the agent. Effective orchestration employs retrieval, filtering, and pruning to optimize context.</p>
<h2 id="prompt-engineering-at-the-orchestration-level">Prompt Engineering at the Orchestration Level<a class="heading-anchor" href="#prompt-engineering-at-the-orchestration-level" aria-label="Link to heading">#</a></h2>
<p><strong>Structured Instructions vs. Plain Language:</strong> Orchestration involves not only what tasks agents do, but how those tasks are described in prompts. A recurring pattern is to use structured, formal instructions rather than leaving everything to freeform natural language. For example, instead of saying "Please write some code to do X", an orchestrator-generated prompt might include a template: a step-by-step checklist, or a specific format like pseudo-code or JSON plan that the agent must follow. This structure can guide the model's output and reduce ambiguity. A concrete case is the self-planning approach: the orchestrator first asks the LLM to output a formatted plan (e.g. a numbered list of steps), and then feeds that plan into the next prompt for code generation [@jiang2024selfplanning]. By explicitly separating "thinking" from "coding," the instructions become more systematic, leading to better results. Similarly, multi-agent frameworks often start with a role acknowledgment and task breakdown. In a four-agent setup using Claude (Architect, Builder, Validator, Scribe), each agent's system prompt begins with a role declaration and primary tasks in bullet form (e.g. "You are Agent 1 – The Architect. Primary Tasks: requirements analysis, architecture planning…"). This kind of structured prompt ensures each agent knows its exact scope and constraints.</p>
<p><strong>Communicating Constraints Clearly:</strong> The orchestration layer must convey any hard constraints or "rules of engagement" to the agent in the prompt. This can include: "Do not modify code outside of Function A in file X", "Follow the project's style guide for naming", or "Make sure to run the provided tests and ensure they pass". Explicitly listing files or sections not to touch can prevent collateral changes. However, LLMs may not always obey implicitly phrased constraints, so phrasing is important. Some systems adopt a manifest-driven approach, where the orchestrator provides a manifest of intended changes. For instance, the manifest might enumerate which functions to implement or which files to create, and the agent is instructed that any output will be checked against this manifest. A recent methodology called Manifest-Driven AI Development (MAID) takes this further by treating the manifest as an enforceable contract: after generation, a validator checks that only the declared files/functions were altered and no undeclared edits occurred. By front-loading these constraints (and later verifying them), the orchestrator greatly reduces the chance of the agent "coloring outside the lines." Another way to impose structure is using tool-assisted prompting languages (like Microsoft's Guidance library) that allow templates with placeholders and even regex checks on the LLM's output, ensuring the agent adheres to formats.</p>
<p><strong>Success Criteria and Framing:</strong> The prompt should also frame what constitutes success. A good orchestration prompt might end with a clear success condition, e.g.: "Your solution will be considered correct if all given unit tests pass and it meets the performance constraint of &lt;2s runtime." By stating these criteria, the agent can self-evaluate its output against them, leading to better alignment with the goal. This is related to the Evaluator-Optimizer pattern [@carpintero2025evaluator], where the prompt or subsequent agent pass includes evaluation guidelines. For example, an orchestrator might instruct: "First, implement the function. Then double-check that the function handles edge cases (null input, etc.) and meets the description. Only finalize if these criteria are satisfied." Such structured prompts push the agent to reflect on its answer against requirements, effectively building a spec into the prompt. Research by Anthropic also suggests that using systematic prompt patterns often outperforms ad-hoc prompting [@anthropic2024agents]. In their experience, many teams succeeded by "simple, composable patterns" in prompts (like stepwise reasoning or tool usage specifications) rather than overly open-ended prompts. In sum, the orchestration layer benefits from acting like a technical writer: providing the right scaffolding in the prompt (roles, steps, examples, constraints, and success checks) so the black-box agent is guided toward the desired outcome.</p>
<p><strong>Example – Claude Subagents:</strong> As a practical example, Claude's subagents feature allows configuring specialized system prompts for different sub-tasks. Each subagent has "a custom system prompt that guides its behavior" and a defined expertise. The main agent automatically delegates to these subagents when appropriate. In effect, the orchestrator (Claude's internal logic) appends the specialized prompt and context for that sub-task. This demonstrates prompt engineering at scale: rather than one giant prompt for everything, the orchestration chooses a tailored prompt per task type. Overall, structured and deliberate prompt engineering at the orchestration level — using instructions, role context, examples, and criteria — is key to maximizing agent reliability.</p>
<h2 id="parallelism-conflict-avoidance">Parallelism &amp; Conflict Avoidance<a class="heading-anchor" href="#parallelism-conflict-avoidance" aria-label="Link to heading">#</a></h2>
<p><strong>When to Run Agents in Parallel:</strong> Orchestrating multiple agents in parallel can speed up development, but only if their tasks won't conflict. The Parallelization Workflow follows a divide-and-conquer approach: identify subtasks that are truly independent and execute them concurrently [@pujals2025agentic]. For example, an orchestrator might split a feature into frontend and backend implementations and assign each to a different agent at the same time. Another parallel pattern is "voting" – run multiple agents on the same task in parallel (perhaps with different prompting styles or different model seeds) and then choose the best result. In coding, this could mean generating multiple candidate solutions concurrently and later selecting one that passes tests ("N-best" approach). However, if parallel agents operate on shared artifacts (e.g. editing the same file), conflicts can arise. Thus, orchestrators should only parallelize tasks that either target separate components or use a strategy to reconcile outputs.</p>
<p><strong>Locking and Work Queues:</strong> A common strategy for conflict avoidance is file-level locking or resource locking. Before an agent begins work, the orchestrator can lock the files or modules it will touch, preventing other agents from editing them until completion. A community-built "Claude code agent farm" demonstrates this with a coordination protocol: each agent must create a lock file listing the files/features it intends to work on, and consult a shared active work registry to ensure no overlap. If a conflict is detected (e.g. two agents want to modify utils.py), one agent will wait (queue the task) until the file is free. This effectively serializes conflicting sections while still allowing non-conflicting tasks in parallel. The registry and lock files also help avoid duplicate work – agents check the completed task log so they don't redo something another agent already finished. Such locking is a conservative heuristic but very effective: it "eliminates merge conflicts" and "prevents wasted work" in concurrent development. The downside is that overly coarse locks (e.g. locking an entire project) nullify the speed benefit, so orchestrators must strike a balance by locking at a sensible granularity (file or feature level).</p>
<p><strong>Advanced Conflict Resolution (CRDTs):</strong> Recent research explores lock-free coordination using principles from distributed systems. CodeCRDT introduces an observation-driven approach where agents don't explicitly lock resources, but instead work on a shared state (a codebase) that automatically merges changes in a conflict-free manner [@pugachev2025codecrdt]. It uses Conflict-Free Replicated Data Types to ensure that as agents make edits, their updates converge deterministically without stepping on each other. In trials, this achieved 100% merge convergence with zero low-level conflicts, eliminating the need for manual conflict resolution. Essentially, each agent can edit concurrently and the CRDT mechanism integrates their changes (like Google Docs but for code). While CRDT-based orchestration can avoid the bottleneck of locks (which can become contention points as agent count N grows), it introduces complexity: agents must deal with semantic conflicts (e.g. two agents might implement the same feature in different ways, both syntactically merged but logically redundant). CodeCRDT reported about 5–10% of such higher-level conflicts, meaning an orchestrator or later review still has to reconcile duplicate or inconsistent implementations. Nonetheless, this approach is promising for truly scalable parallel agent teams, as it sidesteps the O(N×L) contention of lock-based systems.</p>
<p><strong>Heuristics for Parallel Task Selection:</strong> In practice, orchestrators often use simple heuristics to decide parallelism: e.g. no two agents on the same file or closely related module. Some systems use static analysis of code dependency graphs to ensure two parallel tasks don't overlap in call graphs or data models. Others rely on the developer to specify which tasks are independent. For example, an orchestrator might know that adding a new API endpoint (task A) is largely independent of refactoring the logging library (task B), so it schedules those simultaneously. If there is uncertainty, a conservative orchestrator will serialize tasks to avoid the risk of conflict. In the worst case that a conflict does occur (say two agents inadvertently changed the same file), the orchestrator must have a conflict-resolution step: it could auto-merge the code (if changes are in different regions), prefer one agent's output over the other, or fall back to human intervention to reconcile differences. Generally, the goal is conflict avoidance through design: split work such that each agent has its own domain of responsibility. As one user's experience with 4 parallel Claude agents showed, dividing roles by concern (architecture vs. coding vs. testing vs. docs) inherently reduced direct conflicts and even brought positive effects like built-in peer review. Each agent had a clear lane, enabling parallel development with quality checks due to the separation of concerns.</p>
<p>In summary, orchestrating parallel code agents requires careful coordination: lock or isolate shared resources, or adopt novel merging strategies, so that parallelism yields speed-up without chaos. When done right (e.g. with file locks or CRDTs), teams of agents can work "like a well-oiled machine" in parallel.</p>
<h2 id="verification-acceptance-criteria">Verification &amp; Acceptance Criteria<a class="heading-anchor" href="#verification-acceptance-criteria" aria-label="Link to heading">#</a></h2>
<p><strong>Automated Testing as Ground Truth:</strong> One of the most powerful verification techniques is running tests on the agent's output. If the task comes with a test suite or at least some example cases, the orchestrator can automatically execute the generated code and check for correctness. This pattern is evident in many frameworks. AgentCoder is explicitly built around a test-feedback loop: a Test Designer agent generates unit tests, a Test Executor runs the code on those tests, and the Programmer agent then debugs or refines the code based on failures [@huang2023agentcoder]. This ensures that code isn't accepted as "done" until it passes its tests. Even single-agent approaches like OpenAI's Codex have employed this idea (often called execute-and-fix): run the code, and if an error or failing test is detected, prompt the model to fix it. The orchestrator's job is to automate this loop. In PerfOrch's multi-stage pipeline, after generation they perform a validation: if the code is functionally incorrect, it triggers the Fix Stage, invoking specialized bug-fixing models iteratively until a correct solution is found [@wang2025perforch]. Only once all tests pass does it proceed to the next stage. This demonstrates an automated acceptance gate – test passing is the criterion for success. Orchestrators can also measure other aspects like performance: PerfOrch's final Refine Stage checked if runtime improved and only accepted refinements that preserved correctness and met the performance target.</p>
<p><strong>Code Reviews and LLM Critics:</strong> Besides tests, another verification layer is reviewing the code, either by humans or by a second "reviewer" agent. Having a separate agent critique the generated code can catch issues that tests don't (e.g. poor readability, not following style, potential inefficiencies). A pattern often cited is dual-agent coding: one agent writes code, another agent reviews it. If the reviewer finds problems, the coder agent revises accordingly. This is essentially the Evaluator-Optimizer pattern applied to code [@carpintero2025evaluator] – where the Evaluator (which could be the same LLM or a different one tuned for critique) provides feedback, and the Optimizer (generator) uses it to improve the output in another pass. Academic and industry work supports this approach. In practice, companies like Meta have prototypes where an AI code generation is always followed by an AI code review, flagging issues before any human sees the code. Ellipsis's production system likewise emphasizes accuracy over latency: they run multiple comment-generation agents and then use a filtering pipeline (including an LLM-as-judge that evaluates the proposed comments) to ensure only high-quality, non-false-positive feedback is given [@zenml2024ellipsis]. By using an LLM judge on the outputs of LLM coders, they add a second layer of assurance.</p>
<p><strong>Manifests and Static Verification:</strong> As discussed in prompt engineering, if the orchestrator provided a manifest of expected changes, verification includes checking the agent's output against that manifest. Tools like MAID runner perform static analysis on the diff: did the agent only modify the allowed files and functions? Did it include the required new function? This kind of structural verification can be automated by parsing the output or performing AST (Abstract Syntax Tree) comparison between the original and modified code. If the agent deviated (e.g. changed something unrelated), the orchestrator can reject that output or prompt a correction ("You edited file Y which was out of scope; please revert that."). Additionally, the orchestrator might enforce style guides by running linters or formatters on the code and ensuring no new warnings were introduced.</p>
<p><strong>Acceptance and Quality Gates:</strong> Complex orchestrations often define multi-dimensional acceptance criteria. Functional correctness (tests passing) is usually mandatory. Beyond that, style compliance, performance metrics, security scans, etc., can be gates. For example, after an agent's code passes tests, the orchestrator might also run a static security analyzer; if it flags issues, the agent's output isn't fully accepted and a fix task is spawned (or a human review is requested for that aspect). Hugging Face's guide suggests quality gates such as "code passes all test suites" as a primary stopping condition for an iterative refine loop – essentially, you keep refining until the code meets the quality bar [@pujals2025agentic]. They also note using a maximum iteration limit and similarity checks to avoid infinite loops.</p>
<p><strong>Handling Partial Success:</strong> Sometimes an agent's output is partially correct – e.g. it implements one feature but misses another, or fixes some bugs but not all. In these cases, orchestrators can take a pragmatic approach: accept the improvements made and then create follow-up tasks for the remaining work. This is akin to how a human might commit a partial fix and then address the rest. Orchestrators with tracking systems (like the completed_work_log in the agent farm) can mark certain sub-goals as done and only reassign what failed. If tests are granular, the orchestrator can rerun them and see which specific tests still fail, then prompt the agent specifically about those failures on the next try. This targeted retry approach zeroes in on the unresolved issues rather than redoing everything.</p>
<p><strong>Automated vs Human Acceptance:</strong> Ultimately, the output may either be auto-merged or require human approval. Many real-world setups keep a human-in-the-loop for final approval – for example, an orchestrator might open a pull request with the agent's changes and require a human developer to review and merge. The orchestrator in this case hands off to a human at the final step. Another checkpoint is when an agent is uncertain or stuck. If an agent expresses confusion (some advanced agents might output a special token or message like "I am not sure how to proceed"), the orchestrator should detect that and involve a human to clarify requirements or provide a hint.</p>
<p>In summary, verification in agent orchestration is about establishing feedback loops and safety nets: tests to verify correctness, secondary agents or tools to verify quality, and structured checks (manifests, linters) to enforce constraints. By combining these, orchestrators can confidently determine when an agent "succeeded" and its output is acceptable.</p>
<h2 id="failure-modes-recovery-strategies">Failure Modes &amp; Recovery Strategies<a class="heading-anchor" href="#failure-modes-recovery-strategies" aria-label="Link to heading">#</a></h2>
<p>Even with the best planning, agents will sometimes fail – producing wrong code, no code, or even crashing. The orchestration layer must be resilient to these failures and have recovery strategies.</p>
<p><strong>Graceful Retries:</strong> The simplest recovery pattern is retrying the same prompt. Because LLM agents are stochastic, a second attempt might yield a different (possibly correct) result. Orchestrators often implement a retry-on-failure (with a limit on retries to avoid loops). If an agent times out or returns an error (e.g. if using an API that errors out), the orchestrator can automatically retry after a short delay or with a smaller context. The Claude agent farm tool, for example, monitors each agent's health and will auto-restart an agent if it crashes or gets stuck. This ensures that a transient glitch doesn't halt the entire workflow.</p>
<p><strong>Refining the Prompt or Task Breakdown:</strong> If a direct retry doesn't help, the orchestrator should try a modified approach. One common strategy is progressive prompting: if the agent failed to produce output, perhaps the instructions were too broad – so break the task into smaller sub-tasks and prompt those instead. For instance, if "implement the foobar algorithm" came back empty or incorrect, the orchestrator might break it into "step 1: implement the helper function X" and "step 2: implement main foobar using X". Alternatively, the orchestrator can supply more guidance in the prompt on a retry – e.g. provide a partial solution or additional hints. This resembles a fallback to a more guided prompt when the agent seems to be stuck with a high-level request. In the Prompt-Chaining considerations, it's suggested to re-prompt including details of the failure in the context if an intermediate step fails [@pujals2025agentic]. For example: "The previous attempt resulted in a runtime error at line 20. Here is the error log… Please fix the code to handle this issue." This gives the agent new information to succeed on the next pass.</p>
<p><strong>Alternate Agents or Models:</strong> Another recovery pattern is trying a different agent or model. If one model repeatedly fails to solve a problem, perhaps a more powerful model (or one fine-tuned differently) could succeed. PerfOrch demonstrates this by maintaining a ranking of models for each stage – if the top choice fails, it falls back to the next best model for that task [@wang2025perforch]. In their bug-fixing stage, they iteratively invoked up to n different specialized LLMs until one fixed the bug. Similarly, if an agent like GPT-3.5 is not producing a solution, an orchestrator might escalate to GPT-4 for that task. This is a cost-performance trade-off: use cheaper models for most tasks, but have a path to involve a more capable (or simply different) model on failure.</p>
<p><strong>Preserving Partial Progress:</strong> In complex multi-step tasks, an agent might accomplish some steps successfully before failing at a later step. A robust orchestrator will preserve this partial progress so it's not lost. For example, if an agent wrote 80% of a file correctly and then struggled, the orchestrator can save the file (or keep it in memory) and instruct a subsequent agent to continue from that state. This could involve checking the agent's output for usable sections. Some frameworks treat the codebase like a persistent state that agents incrementally build – even if one agent stops mid-way, the next agent sees the codebase with whatever was added so far.</p>
<p><strong>Escalating to Human or Halting:</strong> Not all failures can or should be handled autonomously. A critical safety mechanism is to escalate to a human when automated retries are exhausted or when the cost of more retries outweighs the benefit. The orchestrator might, for instance, make 3 attempts at a task (each possibly with refined prompts or different models). If none succeed, it can stop and label the task for human attention. In a CI/CD integration, this could mean leaving a comment like "AI agent could not resolve this issue – please review manually."</p>
<p><strong>Logging and Learning from Failures:</strong> Another aspect of handling failures is logging them for continuous improvement. The orchestrator should log error cases, so developers can later analyze if prompt tweaks or training could solve them. In some advanced setups, this log is fed into a "reflexion" agent that tries to learn from failures by adjusting future prompts.</p>
<p>In essence, failure recovery in orchestration is about resilience and adaptability. Try again (maybe differently), switch strategies if needed, and know when to ask for help.</p>
<h2 id="multi-pass-architectures-planner-implementer-reviewer-etc">Multi-Pass Architectures (Planner, Implementer, Reviewer, etc.)<a class="heading-anchor" href="#multi-pass-architectures-planner-implementer-reviewer-etc" aria-label="Link to heading">#</a></h2>
<p>One of the most powerful design patterns emerging in AI code generation is the use of multiple passes or multiple agent roles to tackle a problem in stages. Instead of a single monolithic prompt-to-code step, the task is divided into phases such as planning, implementing, and reviewing.</p>
<p><strong>Planner → Implementer Workflow:</strong> In this pattern, a Planner agent first creates a high-level plan or design, which is then used by an Implementer agent to write the actual code. The planner might outline which functions need to be created, what the approach will be, or even pseudo-code. The implementer then follows this blueprint. The self-planning study exemplifies this: the LLM generates concise, structured planning steps from the intent, then in the implementation phase it "generates code step by step, guided by the preceding planning steps" [@jiang2024selfplanning]. This two-pass approach yielded significantly higher correctness than a single-pass solution, because the model effectively did some problem decomposition and reasoning out loud before coding. MapCoder extends this idea to four stages: it uses one agent to recall relevant examples, another to plan the solution approach, a third to generate code, and a fourth to debug the result [@islam2024mapcoder]. Each agent is specialized for its role, and together they "replicate the full cycle of program synthesis as observed in human developers". The results were state-of-the-art on code benchmarks, underlining the efficacy of a multi-pass pipeline.</p>
<p><strong>Reviewer or Refiner Pass:</strong> Another common multi-pass setup is having a Reviewer agent or second pass that checks and improves the code written by an Implementer (first pass). In a multi-pass architecture, this is built-in: after code generation, the process automatically goes to a review phase. The reviewer might be tasked with ensuring the code meets the spec and then either directly editing the code to fix issues or providing feedback for the implementer to act on. This can even become an iterative loop: implementer → reviewer → implementer (for revisions) → reviewer … until the reviewer is satisfied. This approach aligns with the Evaluator-Optimizer iterative refinement pattern [@carpintero2025evaluator].</p>
<p><strong>When Multi-Pass Helps:</strong> Multi-pass shines especially for complex tasks that benefit from explicit reasoning or quality control. If a coding task requires sophisticated reasoning (algorithm design, complex logic), a planning pass helps. If it requires high assurance (safety-critical code, or just code in a large codebase where consistency matters), a review/testing pass helps. On simpler tasks (e.g. "add a print statement"), multi-pass might be overkill – a single agent can handle it and the overhead of planning/reviewing might outweigh the simplicity of the change.</p>
<p><strong>Feedback Loops:</strong> Multi-pass architectures inherently create a feedback loop, especially with a reviewer/corrector in the mix. The orchestrator should feed the feedback in a form the implementer can act on. For example, if a reviewer says "Function foo doesn't handle negative inputs properly," the orchestrator might prompt the implementer agent with that exact feedback: "The code was reviewed and the following issue was found: it doesn't handle negative inputs. Please update the code to address this."</p>
<h2 id="human-in-the-loop-in-orchestration">Human-in-the-Loop in Orchestration<a class="heading-anchor" href="#human-in-the-loop-in-orchestration" aria-label="Link to heading">#</a></h2>
<p>Even with all the above automation, the human element remains important. Orchestration should consider when to involve human developers or stakeholders to guide or approve the process.</p>
<p><strong>Checkpoints for Human Input:</strong> A well-designed orchestrator will define certain points at which it pauses and seeks human input. This might be after planning – e.g., once a Planner agent produces a design, the orchestrator could show that plan to a human for approval before coding. It could also be after code generation but before acceptance – essentially treating the AI-generated code like a regular code review that a human must OK.</p>
<p><strong>Surviving "I'm stuck" vs "I'm done":</strong> Differentiating between an agent that believes it's finished vs one that gave up is key. If an agent says "Solution complete" but the tests are failing, clearly it was wrongly done. The orchestrator should treat that as a failure (and possibly loop in a reviewer agent or human). On the other hand, if after several tries the agent outputs, "I cannot find an approach for this problem," then the orchestrator has an impasse. This is a prime moment for human-in-the-loop.</p>
<p><strong>Human Oversight and Approval:</strong> Humans may also impose guardrails by setting policies the orchestrator follows. For example, an organization might require that any code touching security-critical files must be reviewed by a human, no matter what. The orchestrator can have a rule: if task involves auth.py (or generally high-risk area), always assign it to a human or at least get human approval after the AI suggests changes.</p>
<p><strong>Balancing Autonomy and Oversight:</strong> The goal of orchestration is often to minimize human effort, but not to eliminate it entirely at the cost of quality. An insightful guideline from Anthropic is to use the simplest solution possible and only add complexity (autonomy) when needed [@anthropic2024agents]. This suggests that if a task can be handled with a single LLM call plus a quick human tweak, that might be preferable to spinning up a whole autonomous multi-agent system.</p>
<h2 id="conclusion">Conclusion<a class="heading-anchor" href="#conclusion" aria-label="Link to heading">#</a></h2>
<p>Orchestrating black-box code agents is an exercise in applying sound software engineering principles to AI-driven automation. The patterns identified – from task assignment and context curation to multi-pass workflows and human oversight – collectively form a toolkit for reliable AI-assisted development.</p>
<p>In designing an orchestration framework, one should treat AI agents as fallible collaborators that need structure and guidance – much like human junior developers. The orchestrator's role is part project manager, part version-control system, and part QA engineer for this AI "team." It decides who does what (task assignment), gives them what they need (context), tells them how to do it (prompt engineering and constraints), lets them work together safely (parallelism control), checks their work (verification), handles when they get it wrong (failure recovery), encourages iterative improvement (multi-pass), and knows when to ask a senior (human-in-loop).</p>
<h2 id="references">References<a class="heading-anchor" href="#references" aria-label="Link to heading">#</a></h2>


      
      <hr>
      <div id="backlinks">
        <h3>Backlinks</h3>
        <ul class="backlinks-list">
          
          <li><a href="/parallel-execution.html">Parallel Execution</a></li>
          
          <li><a href="/manifests.html">File Manifests</a></li>
          
          <li><a href="/multi-pass.html">Multi-Pass Architecture</a></li>
          
          <li><a href="/index.html">noface</a></li>
          
        </ul>
      </div>
      
    </article>

    
    <hr>

    <p>
      <a href="/index.html">← Back to home</a>
    </p>
    
    </article>

  </main>



  <!-- Global Graph Modal -->
  <div class="global-graph-outer" id="global-graph-outer">
    <div class="global-graph-container" id="global-graph-container"></div>
  </div>

  <!-- Search Modal -->
  <div id="search-modal">
    <div class="search-modal-wrapper">
      <div class="search-modal-header">
        <input
          type="text"
          id="search-modal-input"
          class="search-modal-input"
          placeholder="Search documentation..."
          autocomplete="off"
        />
      </div>
      <div class="search-modal-tabs">
        <button class="search-tab active" data-tab="results">Results</button>
        <button class="search-tab" data-tab="graph">Graph</button>
      </div>
      <div class="search-modal-content">
        <div class="search-tab-panel active" id="search-tab-results">
          <div class="search-modal-results" id="search-modal-results"></div>
        </div>
        <div class="search-tab-panel" id="search-tab-graph">
          <div class="search-graph-container" id="search-graph-container"></div>
        </div>
      </div>
      <div class="search-modal-footer">
        <div class="search-hint">
          <span><kbd>↑</kbd><kbd>↓</kbd> Navigate</span>
          <span><kbd>↵</kbd> Select</span>
          <span><kbd>ESC</kbd> Close</span>
        </div>
        <div class="search-count"></div>
      </div>
    </div>
  </div>

  
    <script id="page-source-data" type="application/json">"# Design Patterns for Orchestrating Black-Box Code Agents: A Literature Survey\n\nOrchestrating black-box code-generation agents requires careful planning and robust design patterns to ensure these AI \"developers\" work effectively. Researchers and practitioners have proposed various strategies to assign tasks, manage context, engineer prompts, run agents in parallel safely, verify outputs, handle failures, and integrate multiple passes or human oversight. This survey reviews key patterns from academic literature and real-world case studies, organized by orchestration concern, and provides a pattern catalog of proven approaches.\n\n## Task Assignment and Granularity\n\n**Dynamic Task Decomposition:** A central pattern is to use an orchestrator agent (or component) that breaks down a software project into manageable subtasks and assigns them to specialized agents. Rather than having one AI attempt an entire project, orchestrators analyze the request and determine necessary subtasks (e.g. implement a feature, write tests, update docs). In Microsoft\u0027s taxonomy of agent architectures, an Orchestrator-Workers pattern is defined where a lead agent \"dynamically decomposes tasks, delegates them to worker agents, and synthesizes their outputs\". For example, in a coding context the orchestrator could decide which files or modules need changes and coordinate sub-agents for code generation, testing, and documentation. This dynamic assignment allows tackling large development goals by partitioning work.\n\n**Specialization and Routing:** Task assignment often leverages specialization. The orchestrator may classify each task and route it to an agent best suited for that category (using a Routing Workflow pattern) [@pujals2025agentic]. For instance, code-generation requests might go to a \"Coder\" agent, while bug-fix tasks route to a \"Debugger\" agent, and documentation tasks to a \"Doc Writer\" agent. The input can be analyzed (via rules or an LLM) to decide which agent is most appropriate. This ensures that each agent works on tasks aligned with its expertise, improving effectiveness. Ellipsis\u0027s production code review system is an example: it runs multiple specialized agents in parallel, each looking for a specific type of issue in a pull request (security, style, duplication, etc.), rather than one monolithic agent [@zenml2024ellipsis].\n\n**Granularity – Sizing the Task:** Deciding how large a task to give a single agent call is crucial. Literature suggests that tasks should be fine-grained enough to fit the agent\u0027s context window and capabilities, yet not so granular that the overhead of orchestration dominates. If a task is too large (e.g. \"implement an entire feature spanning many files\"), agents may struggle with context or produce incomplete solutions. Researchers have addressed this by explicit task planning. Self-planning code generation has the LLM first outline a plan of steps for a complex request, then tackle each step sequentially [@jiang2024selfplanning]. This two-phase approach of planning then coding yields up to 25.4% higher success (Pass@1) than one-shot generation for complex problems. Similarly, the Prompt-Chaining workflow breaks a complex job into a sequence of smaller subtasks, each handled by a dedicated prompt/agent call [@pujals2025agentic]. This not only makes each step easier for the model to execute but also improves transparency and debuggability. In practice, a rule of thumb is to treat one well-defined issue or function as a unit of work for an agent, and if the request spans multiple concerns or components, the orchestrator should subdivide it.\n\n**Handling Dependencies and Readiness:** An orchestrator must also respect task dependencies when assigning work. Some tasks can run in parallel (e.g. implement UI and backend separately), while others depend on earlier outputs (e.g. tests should run after code is written). One practical approach is to maintain a shared task board with statuses and dependencies. In a user\u0027s multi-agent Claude setup, tasks were tracked in a Markdown plan file with fields for Assigned Agent, Status, and Dependencies – e.g. \"Write Integration Tests – Assigned to Validator – Pending (waiting for Builder to complete auth module)\". This allows the orchestrator (and agents themselves) to know when a task is ready to start. If a task is too big or not well-defined, a planner agent might refine it further or ask for clarification before assignment. Ultimately, effective task assignment balances priority (which issues are most important), readiness (are prerequisites done?), and size (fits in one agent invocation).\n\n## Context Curation: Providing Helpful Context (and Avoiding Harmful Noise)\n\n**Relevant Context Injections:** The information an agent gets in its prompt (code context, design docs, examples) profoundly impacts its success. A key pattern is Retrieval-Augmented Code Generation (RACG), where the orchestrator fetches relevant snippets from the codebase or documentation to include in the agent\u0027s prompt. Real-world software tasks often require understanding cross-file dependencies and global architecture. Since an LLM has a limited context window, the orchestrator must be selective: for example, retrieving the function signature and related config file when asking the agent to implement a new feature in that function. A recent survey emphasizes that repository-level code generation needs to ensure global semantic consistency across files, and integrating a retrieval mechanism (e.g. searching a knowledge base of code) helps the LLM reason beyond a single file. In practice, tools like vector databases are used to find and feed the most relevant code pieces to the agent. Ellipsis\u0027s code review agents, for instance, use advanced code search backed by embeddings to find pertinent pieces of the codebase when analyzing a PR [@zenml2024ellipsis].\n\n**Curating Helpful Examples and Constraints:** Apart from code context, orchestrators often supply design documents, API usage examples, or style guides as context if they will guide the agent toward the right solution. Providing a few-shot example of a similar function or test can dramatically improve accuracy, as the agent can analogize from the given pattern. Additionally, context can include explicit constraints – e.g. a list of files that are in scope, or performance requirements. By giving the agent a clear problem statement along with these auxiliary materials, the orchestrator sets it up for success. For example, if generating a new module, the orchestrator might include an outline of the module\u0027s design or a simplified spec in the prompt. In PerfOrch (Performance-Oriented Orchestration), contextual knowledge about which LLMs perform best for a given language and problem type is stored and used to pick the right model for the task [@wang2025perforch]. This \"strategic memory\" of model strengths is a form of context that improves outcomes by dynamically selecting the optimal agent for each subtask.\n\n**Avoiding Context Overload:** Too much context can hurt agent performance. It might be tempting to provide every possibly relevant file to be safe, but large context size can confuse the model or dilute its focus. Recent studies have shown that even if all provided context is relevant, performance can degrade substantially (by 13–85%) as input length grows. In other words, beyond a certain point, the sheer volume of text can overwhelm the LLM\u0027s reasoning (\"context dilution\"). Thus, orchestrators should be judicious: include only the most relevant code and information. Irrelevant or contradictory context is even worse – it can lead the agent to draw false inferences or focus on the wrong problem. Best practices include using retrieval algorithms to filter context, and possibly summarizing or abstracting context to fit the window. For instance, rather than inserting an entire 500-line file, an orchestrator might insert a summary or the specific function signature needed. The Context-Augmentation pattern described by Hugging Face highlights that context expansion should be iterative and need-based: the agent identifies what extra info it needs, the orchestrator fetches it, and this cycle repeats, rather than dumping everything at once [@pujals2025agentic]. Techniques like embedding search, docstrings extraction, or code property graphs can help select which context truly matters.\n\n**Detecting and Removing Harmful Context:** Orchestrators may also implement \"context pruning\" – removing any information from the prompt that isn\u0027t consistent or necessary. This can include stale data (e.g. outdated function definitions if a newer version exists) or irrelevant conversation history in iterative settings. The goal is to minimize noise. If an agent will be doing a focused code edit, the orchestrator might strip out any unrelated instructions from the conversation and supply a fresh prompt with just the needed background. In summary, curated context is a double-edged sword: the right snippets and examples can greatly enhance success, but too much or wrong context can mislead the agent. Effective orchestration employs retrieval, filtering, and pruning to optimize context.\n\n## Prompt Engineering at the Orchestration Level\n\n**Structured Instructions vs. Plain Language:** Orchestration involves not only what tasks agents do, but how those tasks are described in prompts. A recurring pattern is to use structured, formal instructions rather than leaving everything to freeform natural language. For example, instead of saying \"Please write some code to do X\", an orchestrator-generated prompt might include a template: a step-by-step checklist, or a specific format like pseudo-code or JSON plan that the agent must follow. This structure can guide the model\u0027s output and reduce ambiguity. A concrete case is the self-planning approach: the orchestrator first asks the LLM to output a formatted plan (e.g. a numbered list of steps), and then feeds that plan into the next prompt for code generation [@jiang2024selfplanning]. By explicitly separating \"thinking\" from \"coding,\" the instructions become more systematic, leading to better results. Similarly, multi-agent frameworks often start with a role acknowledgment and task breakdown. In a four-agent setup using Claude (Architect, Builder, Validator, Scribe), each agent\u0027s system prompt begins with a role declaration and primary tasks in bullet form (e.g. \"You are Agent 1 – The Architect. Primary Tasks: requirements analysis, architecture planning…\"). This kind of structured prompt ensures each agent knows its exact scope and constraints.\n\n**Communicating Constraints Clearly:** The orchestration layer must convey any hard constraints or \"rules of engagement\" to the agent in the prompt. This can include: \"Do not modify code outside of Function A in file X\", \"Follow the project\u0027s style guide for naming\", or \"Make sure to run the provided tests and ensure they pass\". Explicitly listing files or sections not to touch can prevent collateral changes. However, LLMs may not always obey implicitly phrased constraints, so phrasing is important. Some systems adopt a manifest-driven approach, where the orchestrator provides a manifest of intended changes. For instance, the manifest might enumerate which functions to implement or which files to create, and the agent is instructed that any output will be checked against this manifest. A recent methodology called Manifest-Driven AI Development (MAID) takes this further by treating the manifest as an enforceable contract: after generation, a validator checks that only the declared files/functions were altered and no undeclared edits occurred. By front-loading these constraints (and later verifying them), the orchestrator greatly reduces the chance of the agent \"coloring outside the lines.\" Another way to impose structure is using tool-assisted prompting languages (like Microsoft\u0027s Guidance library) that allow templates with placeholders and even regex checks on the LLM\u0027s output, ensuring the agent adheres to formats.\n\n**Success Criteria and Framing:** The prompt should also frame what constitutes success. A good orchestration prompt might end with a clear success condition, e.g.: \"Your solution will be considered correct if all given unit tests pass and it meets the performance constraint of \u003c2s runtime.\" By stating these criteria, the agent can self-evaluate its output against them, leading to better alignment with the goal. This is related to the Evaluator-Optimizer pattern [@carpintero2025evaluator], where the prompt or subsequent agent pass includes evaluation guidelines. For example, an orchestrator might instruct: \"First, implement the function. Then double-check that the function handles edge cases (null input, etc.) and meets the description. Only finalize if these criteria are satisfied.\" Such structured prompts push the agent to reflect on its answer against requirements, effectively building a spec into the prompt. Research by Anthropic also suggests that using systematic prompt patterns often outperforms ad-hoc prompting [@anthropic2024agents]. In their experience, many teams succeeded by \"simple, composable patterns\" in prompts (like stepwise reasoning or tool usage specifications) rather than overly open-ended prompts. In sum, the orchestration layer benefits from acting like a technical writer: providing the right scaffolding in the prompt (roles, steps, examples, constraints, and success checks) so the black-box agent is guided toward the desired outcome.\n\n**Example – Claude Subagents:** As a practical example, Claude\u0027s subagents feature allows configuring specialized system prompts for different sub-tasks. Each subagent has \"a custom system prompt that guides its behavior\" and a defined expertise. The main agent automatically delegates to these subagents when appropriate. In effect, the orchestrator (Claude\u0027s internal logic) appends the specialized prompt and context for that sub-task. This demonstrates prompt engineering at scale: rather than one giant prompt for everything, the orchestration chooses a tailored prompt per task type. Overall, structured and deliberate prompt engineering at the orchestration level — using instructions, role context, examples, and criteria — is key to maximizing agent reliability.\n\n## Parallelism \u0026 Conflict Avoidance\n\n**When to Run Agents in Parallel:** Orchestrating multiple agents in parallel can speed up development, but only if their tasks won\u0027t conflict. The Parallelization Workflow follows a divide-and-conquer approach: identify subtasks that are truly independent and execute them concurrently [@pujals2025agentic]. For example, an orchestrator might split a feature into frontend and backend implementations and assign each to a different agent at the same time. Another parallel pattern is \"voting\" – run multiple agents on the same task in parallel (perhaps with different prompting styles or different model seeds) and then choose the best result. In coding, this could mean generating multiple candidate solutions concurrently and later selecting one that passes tests (\"N-best\" approach). However, if parallel agents operate on shared artifacts (e.g. editing the same file), conflicts can arise. Thus, orchestrators should only parallelize tasks that either target separate components or use a strategy to reconcile outputs.\n\n**Locking and Work Queues:** A common strategy for conflict avoidance is file-level locking or resource locking. Before an agent begins work, the orchestrator can lock the files or modules it will touch, preventing other agents from editing them until completion. A community-built \"Claude code agent farm\" demonstrates this with a coordination protocol: each agent must create a lock file listing the files/features it intends to work on, and consult a shared active work registry to ensure no overlap. If a conflict is detected (e.g. two agents want to modify utils.py), one agent will wait (queue the task) until the file is free. This effectively serializes conflicting sections while still allowing non-conflicting tasks in parallel. The registry and lock files also help avoid duplicate work – agents check the completed task log so they don\u0027t redo something another agent already finished. Such locking is a conservative heuristic but very effective: it \"eliminates merge conflicts\" and \"prevents wasted work\" in concurrent development. The downside is that overly coarse locks (e.g. locking an entire project) nullify the speed benefit, so orchestrators must strike a balance by locking at a sensible granularity (file or feature level).\n\n**Advanced Conflict Resolution (CRDTs):** Recent research explores lock-free coordination using principles from distributed systems. CodeCRDT introduces an observation-driven approach where agents don\u0027t explicitly lock resources, but instead work on a shared state (a codebase) that automatically merges changes in a conflict-free manner [@pugachev2025codecrdt]. It uses Conflict-Free Replicated Data Types to ensure that as agents make edits, their updates converge deterministically without stepping on each other. In trials, this achieved 100% merge convergence with zero low-level conflicts, eliminating the need for manual conflict resolution. Essentially, each agent can edit concurrently and the CRDT mechanism integrates their changes (like Google Docs but for code). While CRDT-based orchestration can avoid the bottleneck of locks (which can become contention points as agent count N grows), it introduces complexity: agents must deal with semantic conflicts (e.g. two agents might implement the same feature in different ways, both syntactically merged but logically redundant). CodeCRDT reported about 5–10% of such higher-level conflicts, meaning an orchestrator or later review still has to reconcile duplicate or inconsistent implementations. Nonetheless, this approach is promising for truly scalable parallel agent teams, as it sidesteps the O(N×L) contention of lock-based systems.\n\n**Heuristics for Parallel Task Selection:** In practice, orchestrators often use simple heuristics to decide parallelism: e.g. no two agents on the same file or closely related module. Some systems use static analysis of code dependency graphs to ensure two parallel tasks don\u0027t overlap in call graphs or data models. Others rely on the developer to specify which tasks are independent. For example, an orchestrator might know that adding a new API endpoint (task A) is largely independent of refactoring the logging library (task B), so it schedules those simultaneously. If there is uncertainty, a conservative orchestrator will serialize tasks to avoid the risk of conflict. In the worst case that a conflict does occur (say two agents inadvertently changed the same file), the orchestrator must have a conflict-resolution step: it could auto-merge the code (if changes are in different regions), prefer one agent\u0027s output over the other, or fall back to human intervention to reconcile differences. Generally, the goal is conflict avoidance through design: split work such that each agent has its own domain of responsibility. As one user\u0027s experience with 4 parallel Claude agents showed, dividing roles by concern (architecture vs. coding vs. testing vs. docs) inherently reduced direct conflicts and even brought positive effects like built-in peer review. Each agent had a clear lane, enabling parallel development with quality checks due to the separation of concerns.\n\nIn summary, orchestrating parallel code agents requires careful coordination: lock or isolate shared resources, or adopt novel merging strategies, so that parallelism yields speed-up without chaos. When done right (e.g. with file locks or CRDTs), teams of agents can work \"like a well-oiled machine\" in parallel.\n\n## Verification \u0026 Acceptance Criteria\n\n**Automated Testing as Ground Truth:** One of the most powerful verification techniques is running tests on the agent\u0027s output. If the task comes with a test suite or at least some example cases, the orchestrator can automatically execute the generated code and check for correctness. This pattern is evident in many frameworks. AgentCoder is explicitly built around a test-feedback loop: a Test Designer agent generates unit tests, a Test Executor runs the code on those tests, and the Programmer agent then debugs or refines the code based on failures [@huang2023agentcoder]. This ensures that code isn\u0027t accepted as \"done\" until it passes its tests. Even single-agent approaches like OpenAI\u0027s Codex have employed this idea (often called execute-and-fix): run the code, and if an error or failing test is detected, prompt the model to fix it. The orchestrator\u0027s job is to automate this loop. In PerfOrch\u0027s multi-stage pipeline, after generation they perform a validation: if the code is functionally incorrect, it triggers the Fix Stage, invoking specialized bug-fixing models iteratively until a correct solution is found [@wang2025perforch]. Only once all tests pass does it proceed to the next stage. This demonstrates an automated acceptance gate – test passing is the criterion for success. Orchestrators can also measure other aspects like performance: PerfOrch\u0027s final Refine Stage checked if runtime improved and only accepted refinements that preserved correctness and met the performance target.\n\n**Code Reviews and LLM Critics:** Besides tests, another verification layer is reviewing the code, either by humans or by a second \"reviewer\" agent. Having a separate agent critique the generated code can catch issues that tests don\u0027t (e.g. poor readability, not following style, potential inefficiencies). A pattern often cited is dual-agent coding: one agent writes code, another agent reviews it. If the reviewer finds problems, the coder agent revises accordingly. This is essentially the Evaluator-Optimizer pattern applied to code [@carpintero2025evaluator] – where the Evaluator (which could be the same LLM or a different one tuned for critique) provides feedback, and the Optimizer (generator) uses it to improve the output in another pass. Academic and industry work supports this approach. In practice, companies like Meta have prototypes where an AI code generation is always followed by an AI code review, flagging issues before any human sees the code. Ellipsis\u0027s production system likewise emphasizes accuracy over latency: they run multiple comment-generation agents and then use a filtering pipeline (including an LLM-as-judge that evaluates the proposed comments) to ensure only high-quality, non-false-positive feedback is given [@zenml2024ellipsis]. By using an LLM judge on the outputs of LLM coders, they add a second layer of assurance.\n\n**Manifests and Static Verification:** As discussed in prompt engineering, if the orchestrator provided a manifest of expected changes, verification includes checking the agent\u0027s output against that manifest. Tools like MAID runner perform static analysis on the diff: did the agent only modify the allowed files and functions? Did it include the required new function? This kind of structural verification can be automated by parsing the output or performing AST (Abstract Syntax Tree) comparison between the original and modified code. If the agent deviated (e.g. changed something unrelated), the orchestrator can reject that output or prompt a correction (\"You edited file Y which was out of scope; please revert that.\"). Additionally, the orchestrator might enforce style guides by running linters or formatters on the code and ensuring no new warnings were introduced.\n\n**Acceptance and Quality Gates:** Complex orchestrations often define multi-dimensional acceptance criteria. Functional correctness (tests passing) is usually mandatory. Beyond that, style compliance, performance metrics, security scans, etc., can be gates. For example, after an agent\u0027s code passes tests, the orchestrator might also run a static security analyzer; if it flags issues, the agent\u0027s output isn\u0027t fully accepted and a fix task is spawned (or a human review is requested for that aspect). Hugging Face\u0027s guide suggests quality gates such as \"code passes all test suites\" as a primary stopping condition for an iterative refine loop – essentially, you keep refining until the code meets the quality bar [@pujals2025agentic]. They also note using a maximum iteration limit and similarity checks to avoid infinite loops.\n\n**Handling Partial Success:** Sometimes an agent\u0027s output is partially correct – e.g. it implements one feature but misses another, or fixes some bugs but not all. In these cases, orchestrators can take a pragmatic approach: accept the improvements made and then create follow-up tasks for the remaining work. This is akin to how a human might commit a partial fix and then address the rest. Orchestrators with tracking systems (like the completed_work_log in the agent farm) can mark certain sub-goals as done and only reassign what failed. If tests are granular, the orchestrator can rerun them and see which specific tests still fail, then prompt the agent specifically about those failures on the next try. This targeted retry approach zeroes in on the unresolved issues rather than redoing everything.\n\n**Automated vs Human Acceptance:** Ultimately, the output may either be auto-merged or require human approval. Many real-world setups keep a human-in-the-loop for final approval – for example, an orchestrator might open a pull request with the agent\u0027s changes and require a human developer to review and merge. The orchestrator in this case hands off to a human at the final step. Another checkpoint is when an agent is uncertain or stuck. If an agent expresses confusion (some advanced agents might output a special token or message like \"I am not sure how to proceed\"), the orchestrator should detect that and involve a human to clarify requirements or provide a hint.\n\nIn summary, verification in agent orchestration is about establishing feedback loops and safety nets: tests to verify correctness, secondary agents or tools to verify quality, and structured checks (manifests, linters) to enforce constraints. By combining these, orchestrators can confidently determine when an agent \"succeeded\" and its output is acceptable.\n\n## Failure Modes \u0026 Recovery Strategies\n\nEven with the best planning, agents will sometimes fail – producing wrong code, no code, or even crashing. The orchestration layer must be resilient to these failures and have recovery strategies.\n\n**Graceful Retries:** The simplest recovery pattern is retrying the same prompt. Because LLM agents are stochastic, a second attempt might yield a different (possibly correct) result. Orchestrators often implement a retry-on-failure (with a limit on retries to avoid loops). If an agent times out or returns an error (e.g. if using an API that errors out), the orchestrator can automatically retry after a short delay or with a smaller context. The Claude agent farm tool, for example, monitors each agent\u0027s health and will auto-restart an agent if it crashes or gets stuck. This ensures that a transient glitch doesn\u0027t halt the entire workflow.\n\n**Refining the Prompt or Task Breakdown:** If a direct retry doesn\u0027t help, the orchestrator should try a modified approach. One common strategy is progressive prompting: if the agent failed to produce output, perhaps the instructions were too broad – so break the task into smaller sub-tasks and prompt those instead. For instance, if \"implement the foobar algorithm\" came back empty or incorrect, the orchestrator might break it into \"step 1: implement the helper function X\" and \"step 2: implement main foobar using X\". Alternatively, the orchestrator can supply more guidance in the prompt on a retry – e.g. provide a partial solution or additional hints. This resembles a fallback to a more guided prompt when the agent seems to be stuck with a high-level request. In the Prompt-Chaining considerations, it\u0027s suggested to re-prompt including details of the failure in the context if an intermediate step fails [@pujals2025agentic]. For example: \"The previous attempt resulted in a runtime error at line 20. Here is the error log… Please fix the code to handle this issue.\" This gives the agent new information to succeed on the next pass.\n\n**Alternate Agents or Models:** Another recovery pattern is trying a different agent or model. If one model repeatedly fails to solve a problem, perhaps a more powerful model (or one fine-tuned differently) could succeed. PerfOrch demonstrates this by maintaining a ranking of models for each stage – if the top choice fails, it falls back to the next best model for that task [@wang2025perforch]. In their bug-fixing stage, they iteratively invoked up to n different specialized LLMs until one fixed the bug. Similarly, if an agent like GPT-3.5 is not producing a solution, an orchestrator might escalate to GPT-4 for that task. This is a cost-performance trade-off: use cheaper models for most tasks, but have a path to involve a more capable (or simply different) model on failure.\n\n**Preserving Partial Progress:** In complex multi-step tasks, an agent might accomplish some steps successfully before failing at a later step. A robust orchestrator will preserve this partial progress so it\u0027s not lost. For example, if an agent wrote 80% of a file correctly and then struggled, the orchestrator can save the file (or keep it in memory) and instruct a subsequent agent to continue from that state. This could involve checking the agent\u0027s output for usable sections. Some frameworks treat the codebase like a persistent state that agents incrementally build – even if one agent stops mid-way, the next agent sees the codebase with whatever was added so far.\n\n**Escalating to Human or Halting:** Not all failures can or should be handled autonomously. A critical safety mechanism is to escalate to a human when automated retries are exhausted or when the cost of more retries outweighs the benefit. The orchestrator might, for instance, make 3 attempts at a task (each possibly with refined prompts or different models). If none succeed, it can stop and label the task for human attention. In a CI/CD integration, this could mean leaving a comment like \"AI agent could not resolve this issue – please review manually.\"\n\n**Logging and Learning from Failures:** Another aspect of handling failures is logging them for continuous improvement. The orchestrator should log error cases, so developers can later analyze if prompt tweaks or training could solve them. In some advanced setups, this log is fed into a \"reflexion\" agent that tries to learn from failures by adjusting future prompts.\n\nIn essence, failure recovery in orchestration is about resilience and adaptability. Try again (maybe differently), switch strategies if needed, and know when to ask for help.\n\n## Multi-Pass Architectures (Planner, Implementer, Reviewer, etc.)\n\nOne of the most powerful design patterns emerging in AI code generation is the use of multiple passes or multiple agent roles to tackle a problem in stages. Instead of a single monolithic prompt-to-code step, the task is divided into phases such as planning, implementing, and reviewing.\n\n**Planner → Implementer Workflow:** In this pattern, a Planner agent first creates a high-level plan or design, which is then used by an Implementer agent to write the actual code. The planner might outline which functions need to be created, what the approach will be, or even pseudo-code. The implementer then follows this blueprint. The self-planning study exemplifies this: the LLM generates concise, structured planning steps from the intent, then in the implementation phase it \"generates code step by step, guided by the preceding planning steps\" [@jiang2024selfplanning]. This two-pass approach yielded significantly higher correctness than a single-pass solution, because the model effectively did some problem decomposition and reasoning out loud before coding. MapCoder extends this idea to four stages: it uses one agent to recall relevant examples, another to plan the solution approach, a third to generate code, and a fourth to debug the result [@islam2024mapcoder]. Each agent is specialized for its role, and together they \"replicate the full cycle of program synthesis as observed in human developers\". The results were state-of-the-art on code benchmarks, underlining the efficacy of a multi-pass pipeline.\n\n**Reviewer or Refiner Pass:** Another common multi-pass setup is having a Reviewer agent or second pass that checks and improves the code written by an Implementer (first pass). In a multi-pass architecture, this is built-in: after code generation, the process automatically goes to a review phase. The reviewer might be tasked with ensuring the code meets the spec and then either directly editing the code to fix issues or providing feedback for the implementer to act on. This can even become an iterative loop: implementer → reviewer → implementer (for revisions) → reviewer … until the reviewer is satisfied. This approach aligns with the Evaluator-Optimizer iterative refinement pattern [@carpintero2025evaluator].\n\n**When Multi-Pass Helps:** Multi-pass shines especially for complex tasks that benefit from explicit reasoning or quality control. If a coding task requires sophisticated reasoning (algorithm design, complex logic), a planning pass helps. If it requires high assurance (safety-critical code, or just code in a large codebase where consistency matters), a review/testing pass helps. On simpler tasks (e.g. \"add a print statement\"), multi-pass might be overkill – a single agent can handle it and the overhead of planning/reviewing might outweigh the simplicity of the change.\n\n**Feedback Loops:** Multi-pass architectures inherently create a feedback loop, especially with a reviewer/corrector in the mix. The orchestrator should feed the feedback in a form the implementer can act on. For example, if a reviewer says \"Function foo doesn\u0027t handle negative inputs properly,\" the orchestrator might prompt the implementer agent with that exact feedback: \"The code was reviewed and the following issue was found: it doesn\u0027t handle negative inputs. Please update the code to address this.\"\n\n## Human-in-the-Loop in Orchestration\n\nEven with all the above automation, the human element remains important. Orchestration should consider when to involve human developers or stakeholders to guide or approve the process.\n\n**Checkpoints for Human Input:** A well-designed orchestrator will define certain points at which it pauses and seeks human input. This might be after planning – e.g., once a Planner agent produces a design, the orchestrator could show that plan to a human for approval before coding. It could also be after code generation but before acceptance – essentially treating the AI-generated code like a regular code review that a human must OK.\n\n**Surviving \"I\u0027m stuck\" vs \"I\u0027m done\":** Differentiating between an agent that believes it\u0027s finished vs one that gave up is key. If an agent says \"Solution complete\" but the tests are failing, clearly it was wrongly done. The orchestrator should treat that as a failure (and possibly loop in a reviewer agent or human). On the other hand, if after several tries the agent outputs, \"I cannot find an approach for this problem,\" then the orchestrator has an impasse. This is a prime moment for human-in-the-loop.\n\n**Human Oversight and Approval:** Humans may also impose guardrails by setting policies the orchestrator follows. For example, an organization might require that any code touching security-critical files must be reviewed by a human, no matter what. The orchestrator can have a rule: if task involves auth.py (or generally high-risk area), always assign it to a human or at least get human approval after the AI suggests changes.\n\n**Balancing Autonomy and Oversight:** The goal of orchestration is often to minimize human effort, but not to eliminate it entirely at the cost of quality. An insightful guideline from Anthropic is to use the simplest solution possible and only add complexity (autonomy) when needed [@anthropic2024agents]. This suggests that if a task can be handled with a single LLM call plus a quick human tweak, that might be preferable to spinning up a whole autonomous multi-agent system.\n\n## Conclusion\n\nOrchestrating black-box code agents is an exercise in applying sound software engineering principles to AI-driven automation. The patterns identified – from task assignment and context curation to multi-pass workflows and human oversight – collectively form a toolkit for reliable AI-assisted development.\n\nIn designing an orchestration framework, one should treat AI agents as fallible collaborators that need structure and guidance – much like human junior developers. The orchestrator\u0027s role is part project manager, part version-control system, and part QA engineer for this AI \"team.\" It decides who does what (task assignment), gives them what they need (context), tells them how to do it (prompt engineering and constraints), lets them work together safely (parallelism control), checks their work (verification), handles when they get it wrong (failure recovery), encourages iterative improvement (multi-pass), and knows when to ask a senior (human-in-loop).\n\n## References\n"</script>
    

</body>
</html>