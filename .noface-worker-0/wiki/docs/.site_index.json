{"version":"1","generated_at":"2025-12-10T19:58:25.791371+00:00","site_index":{"notes":[{"slug":"agent","title":"AGENT","content_html":"<h1 id=\"monowiki-agent-guide\">monowiki Agent Guide<a class=\"heading-anchor\" href=\"#monowiki-agent-guide\" aria-label=\"Link to heading\">#</a></h1>\n<p>This vault ships with a CLI that is designed to be agent-friendly. Key commands:</p>\n<ul>\n<li><code>monowiki build</code>: render the site to <code>docs/</code> and emit <code>index.json</code>, <code>graph.json</code>, <code>previews.json</code>, and a cached site index at <code>docs/.site_index.json</code>.</li>\n<li><code>monowiki dev</code>: serve the site locally with live rebuilds <strong>and</strong> JSON endpoints:\n<ul>\n<li><code>/api/search?q=term&amp;limit=10</code></li>\n<li><code>/api/note/&lt;slug&gt;</code></li>\n<li><code>/api/graph/&lt;slug&gt;?depth=2&amp;direction=both</code></li>\n<li><code>/api/graph/path?from=a&amp;to=b&amp;max_depth=5</code></li>\n</ul>\n</li>\n<li><code>monowiki search \"&lt;query&gt;\" --json --limit 5 --types essay,thought --tags rust,notes --with-links</code> for machine-readable results.</li>\n<li><code>monowiki note &lt;slug&gt; --format json --with-links</code> to fetch a single note (frontmatter, rendered HTML, raw body, links).</li>\n<li><code>monowiki graph neighbors --slug &lt;slug&gt; --depth 2 --direction outgoing --json</code> to fan out.</li>\n<li><code>monowiki graph path --from a --to b --max-depth 4 --json</code> to find shortest paths.</li>\n<li><code>monowiki export sections --format jsonl --with-links</code> to stream embedding-ready chunks.</li>\n<li><code>monowiki watch</code> streams JSON change events from <code>vault/</code> (one line per event).</li>\n</ul>\n<p>JSON schemas</p>\n<ul>\n<li>CLI <code>--json</code> and dev server <code>/api/*</code> responses are wrapped in:</li>\n</ul>\n<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">{\n</span><span style=\"color:#323232;\">  </span><span style=\"font-weight:bold;color:#183691;\">&quot;schema_version&quot;</span><span style=\"color:#323232;\">: &quot;2024-11-llm-v1&quot;,\n</span><span style=\"color:#323232;\">  </span><span style=\"font-weight:bold;color:#183691;\">&quot;kind&quot;</span><span style=\"color:#323232;\">: &quot;search.results | note.full | graph.neighbors | graph.path&quot;,\n</span><span style=\"color:#323232;\">  </span><span style=\"font-weight:bold;color:#183691;\">&quot;data&quot;</span><span style=\"color:#323232;\">: { </span><span style=\"background-color:#f5f5f5;font-weight:bold;color:#b52a1d;\">...</span><span style=\"color:#323232;\"> }\n</span><span style=\"color:#323232;\">}\n</span></pre>\n\n</div>\n<ul>\n<li>Search results include <code>id</code>, <code>slug</code>, <code>url</code>, <code>title</code>, <code>section_title</code>, <code>snippet</code>, <code>tags</code>, <code>type</code>, <code>score</code>, <code>outgoing</code>, <code>backlinks</code>.</li>\n<li>Notes include frontmatter, HTML, raw markdown, toc, outgoing, backlinks, and dates.</li>\n<li>Graph neighbors include nodes with <code>slug/title/url/tags/type</code> and edges; graph path returns the path array.</li>\n</ul>\n<p>Performance tips</p>\n<ul>\n<li><code>monowiki build</code>/<code>dev</code> write <code>docs/.site_index.json</code>. The <code>note</code>, <code>graph</code>, and <code>export</code> commands reuse this cache to avoid rebuilding when only reading data. Delete it if you need a fresh rebuild.</li>\n</ul>\n<p>Conventions:</p>\n<ul>\n<li>Slugs come from frontmatter <code>slug</code>, otherwise the filename slugified.</li>\n<li>Drafts are excluded from exports/search when <code>type: draft</code> or <code>draft: true</code>.</li>\n<li>Backlinks are computed from <code>[[WikiLinks]]</code> in markdown and exposed via <code>graph.json</code> and the CLI/API.</li>\n<li>Section-level search slices HTML headings into chunks; IDs match rendered anchors.</li>\n</ul>\n<p>Tips for agents:</p>\n<ul>\n<li>Use <code>monowiki export sections</code> to build retrieval datasets without scraping.</li>\n<li>Use <code>monowiki note &lt;slug&gt; --format json</code> to fetch full context (toc, html, raw markdown) before editing.</li>\n<li>Prefer the dev server APIs during interactive sessions; they reflect live rebuilds.</li>\n</ul>\n<p>Happy hacking!</p>\n","frontmatter":{"title":"","description":null,"summary":null,"date":null,"type":null,"tags":[],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":[],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":[],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#monowiki-agent-guide\">monowiki Agent Guide</a></li></ul></nav>","raw_body":"# monowiki Agent Guide\n\nThis vault ships with a CLI that is designed to be agent-friendly. Key commands:\n\n- `monowiki build`: render the site to `docs/` and emit `index.json`, `graph.json`, `previews.json`, and a cached site index at `docs/.site_index.json`.\n- `monowiki dev`: serve the site locally with live rebuilds **and** JSON endpoints:\n  - `/api/search?q=term&limit=10`\n  - `/api/note/<slug>`\n  - `/api/graph/<slug>?depth=2&direction=both`\n  - `/api/graph/path?from=a&to=b&max_depth=5`\n- `monowiki search \"<query>\" --json --limit 5 --types essay,thought --tags rust,notes --with-links` for machine-readable results.\n- `monowiki note <slug> --format json --with-links` to fetch a single note (frontmatter, rendered HTML, raw body, links).\n- `monowiki graph neighbors --slug <slug> --depth 2 --direction outgoing --json` to fan out.\n- `monowiki graph path --from a --to b --max-depth 4 --json` to find shortest paths.\n- `monowiki export sections --format jsonl --with-links` to stream embedding-ready chunks.\n- `monowiki watch` streams JSON change events from `vault/` (one line per event).\n\nJSON schemas\n\n- CLI `--json` and dev server `/api/*` responses are wrapped in:\n\n```json\n{\n  \"schema_version\": \"2024-11-llm-v1\",\n  \"kind\": \"search.results | note.full | graph.neighbors | graph.path\",\n  \"data\": { ... }\n}\n```\n\n- Search results include `id`, `slug`, `url`, `title`, `section_title`, `snippet`, `tags`, `type`, `score`, `outgoing`, `backlinks`.\n- Notes include frontmatter, HTML, raw markdown, toc, outgoing, backlinks, and dates.\n- Graph neighbors include nodes with `slug/title/url/tags/type` and edges; graph path returns the path array.\n\nPerformance tips\n\n- `monowiki build`/`dev` write `docs/.site_index.json`. The `note`, `graph`, and `export` commands reuse this cache to avoid rebuilding when only reading data. Delete it if you need a fresh rebuild.\n\nConventions:\n- Slugs come from frontmatter `slug`, otherwise the filename slugified.\n- Drafts are excluded from exports/search when `type: draft` or `draft: true`.\n- Backlinks are computed from `[[WikiLinks]]` in markdown and exposed via `graph.json` and the CLI/API.\n- Section-level search slices HTML headings into chunks; IDs match rendered anchors.\n\nTips for agents:\n- Use `monowiki export sections` to build retrieval datasets without scraping.\n- Use `monowiki note <slug> --format json` to fetch full context (toc, html, raw markdown) before editing.\n- Prefer the dev server APIs during interactive sessions; they reflect live rebuilds.\n\nHappy hacking!\n","source_path":"AGENT.md"},{"slug":"orchestration-survey","title":"Design Patterns for Orchestrating Black-Box Code Agents","content_html":"<h1 id=\"design-patterns-for-orchestrating-black-box-code-agents-a-literature-survey\">Design Patterns for Orchestrating Black-Box Code Agents: A Literature Survey<a class=\"heading-anchor\" href=\"#design-patterns-for-orchestrating-black-box-code-agents-a-literature-survey\" aria-label=\"Link to heading\">#</a></h1>\n<p>Orchestrating black-box code-generation agents requires careful planning and robust design patterns to ensure these AI \"developers\" work effectively. Researchers and practitioners have proposed various strategies to assign tasks, manage context, engineer prompts, run agents in parallel safely, verify outputs, handle failures, and integrate multiple passes or human oversight. This survey reviews key patterns from academic literature and real-world case studies, organized by orchestration concern, and provides a pattern catalog of proven approaches.</p>\n<h2 id=\"task-assignment-and-granularity\">Task Assignment and Granularity<a class=\"heading-anchor\" href=\"#task-assignment-and-granularity\" aria-label=\"Link to heading\">#</a></h2>\n<p><strong>Dynamic Task Decomposition:</strong> A central pattern is to use an orchestrator agent (or component) that breaks down a software project into manageable subtasks and assigns them to specialized agents. Rather than having one AI attempt an entire project, orchestrators analyze the request and determine necessary subtasks (e.g. implement a feature, write tests, update docs). In Microsoft's taxonomy of agent architectures, an Orchestrator-Workers pattern is defined where a lead agent \"dynamically decomposes tasks, delegates them to worker agents, and synthesizes their outputs\". For example, in a coding context the orchestrator could decide which files or modules need changes and coordinate sub-agents for code generation, testing, and documentation. This dynamic assignment allows tackling large development goals by partitioning work.</p>\n<p><strong>Specialization and Routing:</strong> Task assignment often leverages specialization. The orchestrator may classify each task and route it to an agent best suited for that category (using a Routing Workflow pattern) [@pujals2025agentic]. For instance, code-generation requests might go to a \"Coder\" agent, while bug-fix tasks route to a \"Debugger\" agent, and documentation tasks to a \"Doc Writer\" agent. The input can be analyzed (via rules or an LLM) to decide which agent is most appropriate. This ensures that each agent works on tasks aligned with its expertise, improving effectiveness. Ellipsis's production code review system is an example: it runs multiple specialized agents in parallel, each looking for a specific type of issue in a pull request (security, style, duplication, etc.), rather than one monolithic agent [@zenml2024ellipsis].</p>\n<p><strong>Granularity – Sizing the Task:</strong> Deciding how large a task to give a single agent call is crucial. Literature suggests that tasks should be fine-grained enough to fit the agent's context window and capabilities, yet not so granular that the overhead of orchestration dominates. If a task is too large (e.g. \"implement an entire feature spanning many files\"), agents may struggle with context or produce incomplete solutions. Researchers have addressed this by explicit task planning. Self-planning code generation has the LLM first outline a plan of steps for a complex request, then tackle each step sequentially [@jiang2024selfplanning]. This two-phase approach of planning then coding yields up to 25.4% higher success (Pass@1) than one-shot generation for complex problems. Similarly, the Prompt-Chaining workflow breaks a complex job into a sequence of smaller subtasks, each handled by a dedicated prompt/agent call [@pujals2025agentic]. This not only makes each step easier for the model to execute but also improves transparency and debuggability. In practice, a rule of thumb is to treat one well-defined issue or function as a unit of work for an agent, and if the request spans multiple concerns or components, the orchestrator should subdivide it.</p>\n<p><strong>Handling Dependencies and Readiness:</strong> An orchestrator must also respect task dependencies when assigning work. Some tasks can run in parallel (e.g. implement UI and backend separately), while others depend on earlier outputs (e.g. tests should run after code is written). One practical approach is to maintain a shared task board with statuses and dependencies. In a user's multi-agent Claude setup, tasks were tracked in a Markdown plan file with fields for Assigned Agent, Status, and Dependencies – e.g. \"Write Integration Tests – Assigned to Validator – Pending (waiting for Builder to complete auth module)\". This allows the orchestrator (and agents themselves) to know when a task is ready to start. If a task is too big or not well-defined, a planner agent might refine it further or ask for clarification before assignment. Ultimately, effective task assignment balances priority (which issues are most important), readiness (are prerequisites done?), and size (fits in one agent invocation).</p>\n<h2 id=\"context-curation-providing-helpful-context-and-avoiding-harmful-noise\">Context Curation: Providing Helpful Context (and Avoiding Harmful Noise)<a class=\"heading-anchor\" href=\"#context-curation-providing-helpful-context-and-avoiding-harmful-noise\" aria-label=\"Link to heading\">#</a></h2>\n<p><strong>Relevant Context Injections:</strong> The information an agent gets in its prompt (code context, design docs, examples) profoundly impacts its success. A key pattern is Retrieval-Augmented Code Generation (RACG), where the orchestrator fetches relevant snippets from the codebase or documentation to include in the agent's prompt. Real-world software tasks often require understanding cross-file dependencies and global architecture. Since an LLM has a limited context window, the orchestrator must be selective: for example, retrieving the function signature and related config file when asking the agent to implement a new feature in that function. A recent survey emphasizes that repository-level code generation needs to ensure global semantic consistency across files, and integrating a retrieval mechanism (e.g. searching a knowledge base of code) helps the LLM reason beyond a single file. In practice, tools like vector databases are used to find and feed the most relevant code pieces to the agent. Ellipsis's code review agents, for instance, use advanced code search backed by embeddings to find pertinent pieces of the codebase when analyzing a PR [@zenml2024ellipsis].</p>\n<p><strong>Curating Helpful Examples and Constraints:</strong> Apart from code context, orchestrators often supply design documents, API usage examples, or style guides as context if they will guide the agent toward the right solution. Providing a few-shot example of a similar function or test can dramatically improve accuracy, as the agent can analogize from the given pattern. Additionally, context can include explicit constraints – e.g. a list of files that are in scope, or performance requirements. By giving the agent a clear problem statement along with these auxiliary materials, the orchestrator sets it up for success. For example, if generating a new module, the orchestrator might include an outline of the module's design or a simplified spec in the prompt. In PerfOrch (Performance-Oriented Orchestration), contextual knowledge about which LLMs perform best for a given language and problem type is stored and used to pick the right model for the task [@wang2025perforch]. This \"strategic memory\" of model strengths is a form of context that improves outcomes by dynamically selecting the optimal agent for each subtask.</p>\n<p><strong>Avoiding Context Overload:</strong> Too much context can hurt agent performance. It might be tempting to provide every possibly relevant file to be safe, but large context size can confuse the model or dilute its focus. Recent studies have shown that even if all provided context is relevant, performance can degrade substantially (by 13–85%) as input length grows. In other words, beyond a certain point, the sheer volume of text can overwhelm the LLM's reasoning (\"context dilution\"). Thus, orchestrators should be judicious: include only the most relevant code and information. Irrelevant or contradictory context is even worse – it can lead the agent to draw false inferences or focus on the wrong problem. Best practices include using retrieval algorithms to filter context, and possibly summarizing or abstracting context to fit the window. For instance, rather than inserting an entire 500-line file, an orchestrator might insert a summary or the specific function signature needed. The Context-Augmentation pattern described by Hugging Face highlights that context expansion should be iterative and need-based: the agent identifies what extra info it needs, the orchestrator fetches it, and this cycle repeats, rather than dumping everything at once [@pujals2025agentic]. Techniques like embedding search, docstrings extraction, or code property graphs can help select which context truly matters.</p>\n<p><strong>Detecting and Removing Harmful Context:</strong> Orchestrators may also implement \"context pruning\" – removing any information from the prompt that isn't consistent or necessary. This can include stale data (e.g. outdated function definitions if a newer version exists) or irrelevant conversation history in iterative settings. The goal is to minimize noise. If an agent will be doing a focused code edit, the orchestrator might strip out any unrelated instructions from the conversation and supply a fresh prompt with just the needed background. In summary, curated context is a double-edged sword: the right snippets and examples can greatly enhance success, but too much or wrong context can mislead the agent. Effective orchestration employs retrieval, filtering, and pruning to optimize context.</p>\n<h2 id=\"prompt-engineering-at-the-orchestration-level\">Prompt Engineering at the Orchestration Level<a class=\"heading-anchor\" href=\"#prompt-engineering-at-the-orchestration-level\" aria-label=\"Link to heading\">#</a></h2>\n<p><strong>Structured Instructions vs. Plain Language:</strong> Orchestration involves not only what tasks agents do, but how those tasks are described in prompts. A recurring pattern is to use structured, formal instructions rather than leaving everything to freeform natural language. For example, instead of saying \"Please write some code to do X\", an orchestrator-generated prompt might include a template: a step-by-step checklist, or a specific format like pseudo-code or JSON plan that the agent must follow. This structure can guide the model's output and reduce ambiguity. A concrete case is the self-planning approach: the orchestrator first asks the LLM to output a formatted plan (e.g. a numbered list of steps), and then feeds that plan into the next prompt for code generation [@jiang2024selfplanning]. By explicitly separating \"thinking\" from \"coding,\" the instructions become more systematic, leading to better results. Similarly, multi-agent frameworks often start with a role acknowledgment and task breakdown. In a four-agent setup using Claude (Architect, Builder, Validator, Scribe), each agent's system prompt begins with a role declaration and primary tasks in bullet form (e.g. \"You are Agent 1 – The Architect. Primary Tasks: requirements analysis, architecture planning…\"). This kind of structured prompt ensures each agent knows its exact scope and constraints.</p>\n<p><strong>Communicating Constraints Clearly:</strong> The orchestration layer must convey any hard constraints or \"rules of engagement\" to the agent in the prompt. This can include: \"Do not modify code outside of Function A in file X\", \"Follow the project's style guide for naming\", or \"Make sure to run the provided tests and ensure they pass\". Explicitly listing files or sections not to touch can prevent collateral changes. However, LLMs may not always obey implicitly phrased constraints, so phrasing is important. Some systems adopt a manifest-driven approach, where the orchestrator provides a manifest of intended changes. For instance, the manifest might enumerate which functions to implement or which files to create, and the agent is instructed that any output will be checked against this manifest. A recent methodology called Manifest-Driven AI Development (MAID) takes this further by treating the manifest as an enforceable contract: after generation, a validator checks that only the declared files/functions were altered and no undeclared edits occurred. By front-loading these constraints (and later verifying them), the orchestrator greatly reduces the chance of the agent \"coloring outside the lines.\" Another way to impose structure is using tool-assisted prompting languages (like Microsoft's Guidance library) that allow templates with placeholders and even regex checks on the LLM's output, ensuring the agent adheres to formats.</p>\n<p><strong>Success Criteria and Framing:</strong> The prompt should also frame what constitutes success. A good orchestration prompt might end with a clear success condition, e.g.: \"Your solution will be considered correct if all given unit tests pass and it meets the performance constraint of &lt;2s runtime.\" By stating these criteria, the agent can self-evaluate its output against them, leading to better alignment with the goal. This is related to the Evaluator-Optimizer pattern [@carpintero2025evaluator], where the prompt or subsequent agent pass includes evaluation guidelines. For example, an orchestrator might instruct: \"First, implement the function. Then double-check that the function handles edge cases (null input, etc.) and meets the description. Only finalize if these criteria are satisfied.\" Such structured prompts push the agent to reflect on its answer against requirements, effectively building a spec into the prompt. Research by Anthropic also suggests that using systematic prompt patterns often outperforms ad-hoc prompting [@anthropic2024agents]. In their experience, many teams succeeded by \"simple, composable patterns\" in prompts (like stepwise reasoning or tool usage specifications) rather than overly open-ended prompts. In sum, the orchestration layer benefits from acting like a technical writer: providing the right scaffolding in the prompt (roles, steps, examples, constraints, and success checks) so the black-box agent is guided toward the desired outcome.</p>\n<p><strong>Example – Claude Subagents:</strong> As a practical example, Claude's subagents feature allows configuring specialized system prompts for different sub-tasks. Each subagent has \"a custom system prompt that guides its behavior\" and a defined expertise. The main agent automatically delegates to these subagents when appropriate. In effect, the orchestrator (Claude's internal logic) appends the specialized prompt and context for that sub-task. This demonstrates prompt engineering at scale: rather than one giant prompt for everything, the orchestration chooses a tailored prompt per task type. Overall, structured and deliberate prompt engineering at the orchestration level — using instructions, role context, examples, and criteria — is key to maximizing agent reliability.</p>\n<h2 id=\"parallelism-conflict-avoidance\">Parallelism &amp; Conflict Avoidance<a class=\"heading-anchor\" href=\"#parallelism-conflict-avoidance\" aria-label=\"Link to heading\">#</a></h2>\n<p><strong>When to Run Agents in Parallel:</strong> Orchestrating multiple agents in parallel can speed up development, but only if their tasks won't conflict. The Parallelization Workflow follows a divide-and-conquer approach: identify subtasks that are truly independent and execute them concurrently [@pujals2025agentic]. For example, an orchestrator might split a feature into frontend and backend implementations and assign each to a different agent at the same time. Another parallel pattern is \"voting\" – run multiple agents on the same task in parallel (perhaps with different prompting styles or different model seeds) and then choose the best result. In coding, this could mean generating multiple candidate solutions concurrently and later selecting one that passes tests (\"N-best\" approach). However, if parallel agents operate on shared artifacts (e.g. editing the same file), conflicts can arise. Thus, orchestrators should only parallelize tasks that either target separate components or use a strategy to reconcile outputs.</p>\n<p><strong>Locking and Work Queues:</strong> A common strategy for conflict avoidance is file-level locking or resource locking. Before an agent begins work, the orchestrator can lock the files or modules it will touch, preventing other agents from editing them until completion. A community-built \"Claude code agent farm\" demonstrates this with a coordination protocol: each agent must create a lock file listing the files/features it intends to work on, and consult a shared active work registry to ensure no overlap. If a conflict is detected (e.g. two agents want to modify utils.py), one agent will wait (queue the task) until the file is free. This effectively serializes conflicting sections while still allowing non-conflicting tasks in parallel. The registry and lock files also help avoid duplicate work – agents check the completed task log so they don't redo something another agent already finished. Such locking is a conservative heuristic but very effective: it \"eliminates merge conflicts\" and \"prevents wasted work\" in concurrent development. The downside is that overly coarse locks (e.g. locking an entire project) nullify the speed benefit, so orchestrators must strike a balance by locking at a sensible granularity (file or feature level).</p>\n<p><strong>Advanced Conflict Resolution (CRDTs):</strong> Recent research explores lock-free coordination using principles from distributed systems. CodeCRDT introduces an observation-driven approach where agents don't explicitly lock resources, but instead work on a shared state (a codebase) that automatically merges changes in a conflict-free manner [@pugachev2025codecrdt]. It uses Conflict-Free Replicated Data Types to ensure that as agents make edits, their updates converge deterministically without stepping on each other. In trials, this achieved 100% merge convergence with zero low-level conflicts, eliminating the need for manual conflict resolution. Essentially, each agent can edit concurrently and the CRDT mechanism integrates their changes (like Google Docs but for code). While CRDT-based orchestration can avoid the bottleneck of locks (which can become contention points as agent count N grows), it introduces complexity: agents must deal with semantic conflicts (e.g. two agents might implement the same feature in different ways, both syntactically merged but logically redundant). CodeCRDT reported about 5–10% of such higher-level conflicts, meaning an orchestrator or later review still has to reconcile duplicate or inconsistent implementations. Nonetheless, this approach is promising for truly scalable parallel agent teams, as it sidesteps the O(N×L) contention of lock-based systems.</p>\n<p><strong>Heuristics for Parallel Task Selection:</strong> In practice, orchestrators often use simple heuristics to decide parallelism: e.g. no two agents on the same file or closely related module. Some systems use static analysis of code dependency graphs to ensure two parallel tasks don't overlap in call graphs or data models. Others rely on the developer to specify which tasks are independent. For example, an orchestrator might know that adding a new API endpoint (task A) is largely independent of refactoring the logging library (task B), so it schedules those simultaneously. If there is uncertainty, a conservative orchestrator will serialize tasks to avoid the risk of conflict. In the worst case that a conflict does occur (say two agents inadvertently changed the same file), the orchestrator must have a conflict-resolution step: it could auto-merge the code (if changes are in different regions), prefer one agent's output over the other, or fall back to human intervention to reconcile differences. Generally, the goal is conflict avoidance through design: split work such that each agent has its own domain of responsibility. As one user's experience with 4 parallel Claude agents showed, dividing roles by concern (architecture vs. coding vs. testing vs. docs) inherently reduced direct conflicts and even brought positive effects like built-in peer review. Each agent had a clear lane, enabling parallel development with quality checks due to the separation of concerns.</p>\n<p>In summary, orchestrating parallel code agents requires careful coordination: lock or isolate shared resources, or adopt novel merging strategies, so that parallelism yields speed-up without chaos. When done right (e.g. with file locks or CRDTs), teams of agents can work \"like a well-oiled machine\" in parallel.</p>\n<h2 id=\"verification-acceptance-criteria\">Verification &amp; Acceptance Criteria<a class=\"heading-anchor\" href=\"#verification-acceptance-criteria\" aria-label=\"Link to heading\">#</a></h2>\n<p><strong>Automated Testing as Ground Truth:</strong> One of the most powerful verification techniques is running tests on the agent's output. If the task comes with a test suite or at least some example cases, the orchestrator can automatically execute the generated code and check for correctness. This pattern is evident in many frameworks. AgentCoder is explicitly built around a test-feedback loop: a Test Designer agent generates unit tests, a Test Executor runs the code on those tests, and the Programmer agent then debugs or refines the code based on failures [@huang2023agentcoder]. This ensures that code isn't accepted as \"done\" until it passes its tests. Even single-agent approaches like OpenAI's Codex have employed this idea (often called execute-and-fix): run the code, and if an error or failing test is detected, prompt the model to fix it. The orchestrator's job is to automate this loop. In PerfOrch's multi-stage pipeline, after generation they perform a validation: if the code is functionally incorrect, it triggers the Fix Stage, invoking specialized bug-fixing models iteratively until a correct solution is found [@wang2025perforch]. Only once all tests pass does it proceed to the next stage. This demonstrates an automated acceptance gate – test passing is the criterion for success. Orchestrators can also measure other aspects like performance: PerfOrch's final Refine Stage checked if runtime improved and only accepted refinements that preserved correctness and met the performance target.</p>\n<p><strong>Code Reviews and LLM Critics:</strong> Besides tests, another verification layer is reviewing the code, either by humans or by a second \"reviewer\" agent. Having a separate agent critique the generated code can catch issues that tests don't (e.g. poor readability, not following style, potential inefficiencies). A pattern often cited is dual-agent coding: one agent writes code, another agent reviews it. If the reviewer finds problems, the coder agent revises accordingly. This is essentially the Evaluator-Optimizer pattern applied to code [@carpintero2025evaluator] – where the Evaluator (which could be the same LLM or a different one tuned for critique) provides feedback, and the Optimizer (generator) uses it to improve the output in another pass. Academic and industry work supports this approach. In practice, companies like Meta have prototypes where an AI code generation is always followed by an AI code review, flagging issues before any human sees the code. Ellipsis's production system likewise emphasizes accuracy over latency: they run multiple comment-generation agents and then use a filtering pipeline (including an LLM-as-judge that evaluates the proposed comments) to ensure only high-quality, non-false-positive feedback is given [@zenml2024ellipsis]. By using an LLM judge on the outputs of LLM coders, they add a second layer of assurance.</p>\n<p><strong>Manifests and Static Verification:</strong> As discussed in prompt engineering, if the orchestrator provided a manifest of expected changes, verification includes checking the agent's output against that manifest. Tools like MAID runner perform static analysis on the diff: did the agent only modify the allowed files and functions? Did it include the required new function? This kind of structural verification can be automated by parsing the output or performing AST (Abstract Syntax Tree) comparison between the original and modified code. If the agent deviated (e.g. changed something unrelated), the orchestrator can reject that output or prompt a correction (\"You edited file Y which was out of scope; please revert that.\"). Additionally, the orchestrator might enforce style guides by running linters or formatters on the code and ensuring no new warnings were introduced.</p>\n<p><strong>Acceptance and Quality Gates:</strong> Complex orchestrations often define multi-dimensional acceptance criteria. Functional correctness (tests passing) is usually mandatory. Beyond that, style compliance, performance metrics, security scans, etc., can be gates. For example, after an agent's code passes tests, the orchestrator might also run a static security analyzer; if it flags issues, the agent's output isn't fully accepted and a fix task is spawned (or a human review is requested for that aspect). Hugging Face's guide suggests quality gates such as \"code passes all test suites\" as a primary stopping condition for an iterative refine loop – essentially, you keep refining until the code meets the quality bar [@pujals2025agentic]. They also note using a maximum iteration limit and similarity checks to avoid infinite loops.</p>\n<p><strong>Handling Partial Success:</strong> Sometimes an agent's output is partially correct – e.g. it implements one feature but misses another, or fixes some bugs but not all. In these cases, orchestrators can take a pragmatic approach: accept the improvements made and then create follow-up tasks for the remaining work. This is akin to how a human might commit a partial fix and then address the rest. Orchestrators with tracking systems (like the completed_work_log in the agent farm) can mark certain sub-goals as done and only reassign what failed. If tests are granular, the orchestrator can rerun them and see which specific tests still fail, then prompt the agent specifically about those failures on the next try. This targeted retry approach zeroes in on the unresolved issues rather than redoing everything.</p>\n<p><strong>Automated vs Human Acceptance:</strong> Ultimately, the output may either be auto-merged or require human approval. Many real-world setups keep a human-in-the-loop for final approval – for example, an orchestrator might open a pull request with the agent's changes and require a human developer to review and merge. The orchestrator in this case hands off to a human at the final step. Another checkpoint is when an agent is uncertain or stuck. If an agent expresses confusion (some advanced agents might output a special token or message like \"I am not sure how to proceed\"), the orchestrator should detect that and involve a human to clarify requirements or provide a hint.</p>\n<p>In summary, verification in agent orchestration is about establishing feedback loops and safety nets: tests to verify correctness, secondary agents or tools to verify quality, and structured checks (manifests, linters) to enforce constraints. By combining these, orchestrators can confidently determine when an agent \"succeeded\" and its output is acceptable.</p>\n<h2 id=\"failure-modes-recovery-strategies\">Failure Modes &amp; Recovery Strategies<a class=\"heading-anchor\" href=\"#failure-modes-recovery-strategies\" aria-label=\"Link to heading\">#</a></h2>\n<p>Even with the best planning, agents will sometimes fail – producing wrong code, no code, or even crashing. The orchestration layer must be resilient to these failures and have recovery strategies.</p>\n<p><strong>Graceful Retries:</strong> The simplest recovery pattern is retrying the same prompt. Because LLM agents are stochastic, a second attempt might yield a different (possibly correct) result. Orchestrators often implement a retry-on-failure (with a limit on retries to avoid loops). If an agent times out or returns an error (e.g. if using an API that errors out), the orchestrator can automatically retry after a short delay or with a smaller context. The Claude agent farm tool, for example, monitors each agent's health and will auto-restart an agent if it crashes or gets stuck. This ensures that a transient glitch doesn't halt the entire workflow.</p>\n<p><strong>Refining the Prompt or Task Breakdown:</strong> If a direct retry doesn't help, the orchestrator should try a modified approach. One common strategy is progressive prompting: if the agent failed to produce output, perhaps the instructions were too broad – so break the task into smaller sub-tasks and prompt those instead. For instance, if \"implement the foobar algorithm\" came back empty or incorrect, the orchestrator might break it into \"step 1: implement the helper function X\" and \"step 2: implement main foobar using X\". Alternatively, the orchestrator can supply more guidance in the prompt on a retry – e.g. provide a partial solution or additional hints. This resembles a fallback to a more guided prompt when the agent seems to be stuck with a high-level request. In the Prompt-Chaining considerations, it's suggested to re-prompt including details of the failure in the context if an intermediate step fails [@pujals2025agentic]. For example: \"The previous attempt resulted in a runtime error at line 20. Here is the error log… Please fix the code to handle this issue.\" This gives the agent new information to succeed on the next pass.</p>\n<p><strong>Alternate Agents or Models:</strong> Another recovery pattern is trying a different agent or model. If one model repeatedly fails to solve a problem, perhaps a more powerful model (or one fine-tuned differently) could succeed. PerfOrch demonstrates this by maintaining a ranking of models for each stage – if the top choice fails, it falls back to the next best model for that task [@wang2025perforch]. In their bug-fixing stage, they iteratively invoked up to n different specialized LLMs until one fixed the bug. Similarly, if an agent like GPT-3.5 is not producing a solution, an orchestrator might escalate to GPT-4 for that task. This is a cost-performance trade-off: use cheaper models for most tasks, but have a path to involve a more capable (or simply different) model on failure.</p>\n<p><strong>Preserving Partial Progress:</strong> In complex multi-step tasks, an agent might accomplish some steps successfully before failing at a later step. A robust orchestrator will preserve this partial progress so it's not lost. For example, if an agent wrote 80% of a file correctly and then struggled, the orchestrator can save the file (or keep it in memory) and instruct a subsequent agent to continue from that state. This could involve checking the agent's output for usable sections. Some frameworks treat the codebase like a persistent state that agents incrementally build – even if one agent stops mid-way, the next agent sees the codebase with whatever was added so far.</p>\n<p><strong>Escalating to Human or Halting:</strong> Not all failures can or should be handled autonomously. A critical safety mechanism is to escalate to a human when automated retries are exhausted or when the cost of more retries outweighs the benefit. The orchestrator might, for instance, make 3 attempts at a task (each possibly with refined prompts or different models). If none succeed, it can stop and label the task for human attention. In a CI/CD integration, this could mean leaving a comment like \"AI agent could not resolve this issue – please review manually.\"</p>\n<p><strong>Logging and Learning from Failures:</strong> Another aspect of handling failures is logging them for continuous improvement. The orchestrator should log error cases, so developers can later analyze if prompt tweaks or training could solve them. In some advanced setups, this log is fed into a \"reflexion\" agent that tries to learn from failures by adjusting future prompts.</p>\n<p>In essence, failure recovery in orchestration is about resilience and adaptability. Try again (maybe differently), switch strategies if needed, and know when to ask for help.</p>\n<h2 id=\"multi-pass-architectures-planner-implementer-reviewer-etc\">Multi-Pass Architectures (Planner, Implementer, Reviewer, etc.)<a class=\"heading-anchor\" href=\"#multi-pass-architectures-planner-implementer-reviewer-etc\" aria-label=\"Link to heading\">#</a></h2>\n<p>One of the most powerful design patterns emerging in AI code generation is the use of multiple passes or multiple agent roles to tackle a problem in stages. Instead of a single monolithic prompt-to-code step, the task is divided into phases such as planning, implementing, and reviewing.</p>\n<p><strong>Planner → Implementer Workflow:</strong> In this pattern, a Planner agent first creates a high-level plan or design, which is then used by an Implementer agent to write the actual code. The planner might outline which functions need to be created, what the approach will be, or even pseudo-code. The implementer then follows this blueprint. The self-planning study exemplifies this: the LLM generates concise, structured planning steps from the intent, then in the implementation phase it \"generates code step by step, guided by the preceding planning steps\" [@jiang2024selfplanning]. This two-pass approach yielded significantly higher correctness than a single-pass solution, because the model effectively did some problem decomposition and reasoning out loud before coding. MapCoder extends this idea to four stages: it uses one agent to recall relevant examples, another to plan the solution approach, a third to generate code, and a fourth to debug the result [@islam2024mapcoder]. Each agent is specialized for its role, and together they \"replicate the full cycle of program synthesis as observed in human developers\". The results were state-of-the-art on code benchmarks, underlining the efficacy of a multi-pass pipeline.</p>\n<p><strong>Reviewer or Refiner Pass:</strong> Another common multi-pass setup is having a Reviewer agent or second pass that checks and improves the code written by an Implementer (first pass). In a multi-pass architecture, this is built-in: after code generation, the process automatically goes to a review phase. The reviewer might be tasked with ensuring the code meets the spec and then either directly editing the code to fix issues or providing feedback for the implementer to act on. This can even become an iterative loop: implementer → reviewer → implementer (for revisions) → reviewer … until the reviewer is satisfied. This approach aligns with the Evaluator-Optimizer iterative refinement pattern [@carpintero2025evaluator].</p>\n<p><strong>When Multi-Pass Helps:</strong> Multi-pass shines especially for complex tasks that benefit from explicit reasoning or quality control. If a coding task requires sophisticated reasoning (algorithm design, complex logic), a planning pass helps. If it requires high assurance (safety-critical code, or just code in a large codebase where consistency matters), a review/testing pass helps. On simpler tasks (e.g. \"add a print statement\"), multi-pass might be overkill – a single agent can handle it and the overhead of planning/reviewing might outweigh the simplicity of the change.</p>\n<p><strong>Feedback Loops:</strong> Multi-pass architectures inherently create a feedback loop, especially with a reviewer/corrector in the mix. The orchestrator should feed the feedback in a form the implementer can act on. For example, if a reviewer says \"Function foo doesn't handle negative inputs properly,\" the orchestrator might prompt the implementer agent with that exact feedback: \"The code was reviewed and the following issue was found: it doesn't handle negative inputs. Please update the code to address this.\"</p>\n<h2 id=\"human-in-the-loop-in-orchestration\">Human-in-the-Loop in Orchestration<a class=\"heading-anchor\" href=\"#human-in-the-loop-in-orchestration\" aria-label=\"Link to heading\">#</a></h2>\n<p>Even with all the above automation, the human element remains important. Orchestration should consider when to involve human developers or stakeholders to guide or approve the process.</p>\n<p><strong>Checkpoints for Human Input:</strong> A well-designed orchestrator will define certain points at which it pauses and seeks human input. This might be after planning – e.g., once a Planner agent produces a design, the orchestrator could show that plan to a human for approval before coding. It could also be after code generation but before acceptance – essentially treating the AI-generated code like a regular code review that a human must OK.</p>\n<p><strong>Surviving \"I'm stuck\" vs \"I'm done\":</strong> Differentiating between an agent that believes it's finished vs one that gave up is key. If an agent says \"Solution complete\" but the tests are failing, clearly it was wrongly done. The orchestrator should treat that as a failure (and possibly loop in a reviewer agent or human). On the other hand, if after several tries the agent outputs, \"I cannot find an approach for this problem,\" then the orchestrator has an impasse. This is a prime moment for human-in-the-loop.</p>\n<p><strong>Human Oversight and Approval:</strong> Humans may also impose guardrails by setting policies the orchestrator follows. For example, an organization might require that any code touching security-critical files must be reviewed by a human, no matter what. The orchestrator can have a rule: if task involves auth.py (or generally high-risk area), always assign it to a human or at least get human approval after the AI suggests changes.</p>\n<p><strong>Balancing Autonomy and Oversight:</strong> The goal of orchestration is often to minimize human effort, but not to eliminate it entirely at the cost of quality. An insightful guideline from Anthropic is to use the simplest solution possible and only add complexity (autonomy) when needed [@anthropic2024agents]. This suggests that if a task can be handled with a single LLM call plus a quick human tweak, that might be preferable to spinning up a whole autonomous multi-agent system.</p>\n<h2 id=\"conclusion\">Conclusion<a class=\"heading-anchor\" href=\"#conclusion\" aria-label=\"Link to heading\">#</a></h2>\n<p>Orchestrating black-box code agents is an exercise in applying sound software engineering principles to AI-driven automation. The patterns identified – from task assignment and context curation to multi-pass workflows and human oversight – collectively form a toolkit for reliable AI-assisted development.</p>\n<p>In designing an orchestration framework, one should treat AI agents as fallible collaborators that need structure and guidance – much like human junior developers. The orchestrator's role is part project manager, part version-control system, and part QA engineer for this AI \"team.\" It decides who does what (task assignment), gives them what they need (context), tells them how to do it (prompt engineering and constraints), lets them work together safely (parallelism control), checks their work (verification), handles when they get it wrong (failure recovery), encourages iterative improvement (multi-pass), and knows when to ask a senior (human-in-loop).</p>\n<h2 id=\"references\">References<a class=\"heading-anchor\" href=\"#references\" aria-label=\"Link to heading\">#</a></h2>\n","frontmatter":{"title":"Design Patterns for Orchestrating Black-Box Code Agents","description":null,"summary":null,"date":null,"type":"essay","tags":["research","agents","orchestration","survey"],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":["research","agents","orchestration","survey"],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":[],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#design-patterns-for-orchestrating-black-box-code-agents-a-literature-survey\">Design Patterns for Orchestrating Black-Box Code Agents: A Literature Survey</a></li><li class=\"toc-level-2\"><a href=\"#task-assignment-and-granularity\">Task Assignment and Granularity</a></li><li class=\"toc-level-2\"><a href=\"#context-curation-providing-helpful-context-and-avoiding-harmful-noise\">Context Curation: Providing Helpful Context (and Avoiding Harmful Noise)</a></li><li class=\"toc-level-2\"><a href=\"#prompt-engineering-at-the-orchestration-level\">Prompt Engineering at the Orchestration Level</a></li><li class=\"toc-level-2\"><a href=\"#parallelism-conflict-avoidance\">Parallelism &amp; Conflict Avoidance</a></li><li class=\"toc-level-2\"><a href=\"#verification-acceptance-criteria\">Verification &amp; Acceptance Criteria</a></li><li class=\"toc-level-2\"><a href=\"#failure-modes-recovery-strategies\">Failure Modes &amp; Recovery Strategies</a></li><li class=\"toc-level-2\"><a href=\"#multi-pass-architectures-planner-implementer-reviewer-etc\">Multi-Pass Architectures (Planner, Implementer, Reviewer, etc.)</a></li><li class=\"toc-level-2\"><a href=\"#human-in-the-loop-in-orchestration\">Human-in-the-Loop in Orchestration</a></li><li class=\"toc-level-2\"><a href=\"#conclusion\">Conclusion</a></li><li class=\"toc-level-2\"><a href=\"#references\">References</a></li></ul></nav>","raw_body":"# Design Patterns for Orchestrating Black-Box Code Agents: A Literature Survey\n\nOrchestrating black-box code-generation agents requires careful planning and robust design patterns to ensure these AI \"developers\" work effectively. Researchers and practitioners have proposed various strategies to assign tasks, manage context, engineer prompts, run agents in parallel safely, verify outputs, handle failures, and integrate multiple passes or human oversight. This survey reviews key patterns from academic literature and real-world case studies, organized by orchestration concern, and provides a pattern catalog of proven approaches.\n\n## Task Assignment and Granularity\n\n**Dynamic Task Decomposition:** A central pattern is to use an orchestrator agent (or component) that breaks down a software project into manageable subtasks and assigns them to specialized agents. Rather than having one AI attempt an entire project, orchestrators analyze the request and determine necessary subtasks (e.g. implement a feature, write tests, update docs). In Microsoft's taxonomy of agent architectures, an Orchestrator-Workers pattern is defined where a lead agent \"dynamically decomposes tasks, delegates them to worker agents, and synthesizes their outputs\". For example, in a coding context the orchestrator could decide which files or modules need changes and coordinate sub-agents for code generation, testing, and documentation. This dynamic assignment allows tackling large development goals by partitioning work.\n\n**Specialization and Routing:** Task assignment often leverages specialization. The orchestrator may classify each task and route it to an agent best suited for that category (using a Routing Workflow pattern) [@pujals2025agentic]. For instance, code-generation requests might go to a \"Coder\" agent, while bug-fix tasks route to a \"Debugger\" agent, and documentation tasks to a \"Doc Writer\" agent. The input can be analyzed (via rules or an LLM) to decide which agent is most appropriate. This ensures that each agent works on tasks aligned with its expertise, improving effectiveness. Ellipsis's production code review system is an example: it runs multiple specialized agents in parallel, each looking for a specific type of issue in a pull request (security, style, duplication, etc.), rather than one monolithic agent [@zenml2024ellipsis].\n\n**Granularity – Sizing the Task:** Deciding how large a task to give a single agent call is crucial. Literature suggests that tasks should be fine-grained enough to fit the agent's context window and capabilities, yet not so granular that the overhead of orchestration dominates. If a task is too large (e.g. \"implement an entire feature spanning many files\"), agents may struggle with context or produce incomplete solutions. Researchers have addressed this by explicit task planning. Self-planning code generation has the LLM first outline a plan of steps for a complex request, then tackle each step sequentially [@jiang2024selfplanning]. This two-phase approach of planning then coding yields up to 25.4% higher success (Pass@1) than one-shot generation for complex problems. Similarly, the Prompt-Chaining workflow breaks a complex job into a sequence of smaller subtasks, each handled by a dedicated prompt/agent call [@pujals2025agentic]. This not only makes each step easier for the model to execute but also improves transparency and debuggability. In practice, a rule of thumb is to treat one well-defined issue or function as a unit of work for an agent, and if the request spans multiple concerns or components, the orchestrator should subdivide it.\n\n**Handling Dependencies and Readiness:** An orchestrator must also respect task dependencies when assigning work. Some tasks can run in parallel (e.g. implement UI and backend separately), while others depend on earlier outputs (e.g. tests should run after code is written). One practical approach is to maintain a shared task board with statuses and dependencies. In a user's multi-agent Claude setup, tasks were tracked in a Markdown plan file with fields for Assigned Agent, Status, and Dependencies – e.g. \"Write Integration Tests – Assigned to Validator – Pending (waiting for Builder to complete auth module)\". This allows the orchestrator (and agents themselves) to know when a task is ready to start. If a task is too big or not well-defined, a planner agent might refine it further or ask for clarification before assignment. Ultimately, effective task assignment balances priority (which issues are most important), readiness (are prerequisites done?), and size (fits in one agent invocation).\n\n## Context Curation: Providing Helpful Context (and Avoiding Harmful Noise)\n\n**Relevant Context Injections:** The information an agent gets in its prompt (code context, design docs, examples) profoundly impacts its success. A key pattern is Retrieval-Augmented Code Generation (RACG), where the orchestrator fetches relevant snippets from the codebase or documentation to include in the agent's prompt. Real-world software tasks often require understanding cross-file dependencies and global architecture. Since an LLM has a limited context window, the orchestrator must be selective: for example, retrieving the function signature and related config file when asking the agent to implement a new feature in that function. A recent survey emphasizes that repository-level code generation needs to ensure global semantic consistency across files, and integrating a retrieval mechanism (e.g. searching a knowledge base of code) helps the LLM reason beyond a single file. In practice, tools like vector databases are used to find and feed the most relevant code pieces to the agent. Ellipsis's code review agents, for instance, use advanced code search backed by embeddings to find pertinent pieces of the codebase when analyzing a PR [@zenml2024ellipsis].\n\n**Curating Helpful Examples and Constraints:** Apart from code context, orchestrators often supply design documents, API usage examples, or style guides as context if they will guide the agent toward the right solution. Providing a few-shot example of a similar function or test can dramatically improve accuracy, as the agent can analogize from the given pattern. Additionally, context can include explicit constraints – e.g. a list of files that are in scope, or performance requirements. By giving the agent a clear problem statement along with these auxiliary materials, the orchestrator sets it up for success. For example, if generating a new module, the orchestrator might include an outline of the module's design or a simplified spec in the prompt. In PerfOrch (Performance-Oriented Orchestration), contextual knowledge about which LLMs perform best for a given language and problem type is stored and used to pick the right model for the task [@wang2025perforch]. This \"strategic memory\" of model strengths is a form of context that improves outcomes by dynamically selecting the optimal agent for each subtask.\n\n**Avoiding Context Overload:** Too much context can hurt agent performance. It might be tempting to provide every possibly relevant file to be safe, but large context size can confuse the model or dilute its focus. Recent studies have shown that even if all provided context is relevant, performance can degrade substantially (by 13–85%) as input length grows. In other words, beyond a certain point, the sheer volume of text can overwhelm the LLM's reasoning (\"context dilution\"). Thus, orchestrators should be judicious: include only the most relevant code and information. Irrelevant or contradictory context is even worse – it can lead the agent to draw false inferences or focus on the wrong problem. Best practices include using retrieval algorithms to filter context, and possibly summarizing or abstracting context to fit the window. For instance, rather than inserting an entire 500-line file, an orchestrator might insert a summary or the specific function signature needed. The Context-Augmentation pattern described by Hugging Face highlights that context expansion should be iterative and need-based: the agent identifies what extra info it needs, the orchestrator fetches it, and this cycle repeats, rather than dumping everything at once [@pujals2025agentic]. Techniques like embedding search, docstrings extraction, or code property graphs can help select which context truly matters.\n\n**Detecting and Removing Harmful Context:** Orchestrators may also implement \"context pruning\" – removing any information from the prompt that isn't consistent or necessary. This can include stale data (e.g. outdated function definitions if a newer version exists) or irrelevant conversation history in iterative settings. The goal is to minimize noise. If an agent will be doing a focused code edit, the orchestrator might strip out any unrelated instructions from the conversation and supply a fresh prompt with just the needed background. In summary, curated context is a double-edged sword: the right snippets and examples can greatly enhance success, but too much or wrong context can mislead the agent. Effective orchestration employs retrieval, filtering, and pruning to optimize context.\n\n## Prompt Engineering at the Orchestration Level\n\n**Structured Instructions vs. Plain Language:** Orchestration involves not only what tasks agents do, but how those tasks are described in prompts. A recurring pattern is to use structured, formal instructions rather than leaving everything to freeform natural language. For example, instead of saying \"Please write some code to do X\", an orchestrator-generated prompt might include a template: a step-by-step checklist, or a specific format like pseudo-code or JSON plan that the agent must follow. This structure can guide the model's output and reduce ambiguity. A concrete case is the self-planning approach: the orchestrator first asks the LLM to output a formatted plan (e.g. a numbered list of steps), and then feeds that plan into the next prompt for code generation [@jiang2024selfplanning]. By explicitly separating \"thinking\" from \"coding,\" the instructions become more systematic, leading to better results. Similarly, multi-agent frameworks often start with a role acknowledgment and task breakdown. In a four-agent setup using Claude (Architect, Builder, Validator, Scribe), each agent's system prompt begins with a role declaration and primary tasks in bullet form (e.g. \"You are Agent 1 – The Architect. Primary Tasks: requirements analysis, architecture planning…\"). This kind of structured prompt ensures each agent knows its exact scope and constraints.\n\n**Communicating Constraints Clearly:** The orchestration layer must convey any hard constraints or \"rules of engagement\" to the agent in the prompt. This can include: \"Do not modify code outside of Function A in file X\", \"Follow the project's style guide for naming\", or \"Make sure to run the provided tests and ensure they pass\". Explicitly listing files or sections not to touch can prevent collateral changes. However, LLMs may not always obey implicitly phrased constraints, so phrasing is important. Some systems adopt a manifest-driven approach, where the orchestrator provides a manifest of intended changes. For instance, the manifest might enumerate which functions to implement or which files to create, and the agent is instructed that any output will be checked against this manifest. A recent methodology called Manifest-Driven AI Development (MAID) takes this further by treating the manifest as an enforceable contract: after generation, a validator checks that only the declared files/functions were altered and no undeclared edits occurred. By front-loading these constraints (and later verifying them), the orchestrator greatly reduces the chance of the agent \"coloring outside the lines.\" Another way to impose structure is using tool-assisted prompting languages (like Microsoft's Guidance library) that allow templates with placeholders and even regex checks on the LLM's output, ensuring the agent adheres to formats.\n\n**Success Criteria and Framing:** The prompt should also frame what constitutes success. A good orchestration prompt might end with a clear success condition, e.g.: \"Your solution will be considered correct if all given unit tests pass and it meets the performance constraint of <2s runtime.\" By stating these criteria, the agent can self-evaluate its output against them, leading to better alignment with the goal. This is related to the Evaluator-Optimizer pattern [@carpintero2025evaluator], where the prompt or subsequent agent pass includes evaluation guidelines. For example, an orchestrator might instruct: \"First, implement the function. Then double-check that the function handles edge cases (null input, etc.) and meets the description. Only finalize if these criteria are satisfied.\" Such structured prompts push the agent to reflect on its answer against requirements, effectively building a spec into the prompt. Research by Anthropic also suggests that using systematic prompt patterns often outperforms ad-hoc prompting [@anthropic2024agents]. In their experience, many teams succeeded by \"simple, composable patterns\" in prompts (like stepwise reasoning or tool usage specifications) rather than overly open-ended prompts. In sum, the orchestration layer benefits from acting like a technical writer: providing the right scaffolding in the prompt (roles, steps, examples, constraints, and success checks) so the black-box agent is guided toward the desired outcome.\n\n**Example – Claude Subagents:** As a practical example, Claude's subagents feature allows configuring specialized system prompts for different sub-tasks. Each subagent has \"a custom system prompt that guides its behavior\" and a defined expertise. The main agent automatically delegates to these subagents when appropriate. In effect, the orchestrator (Claude's internal logic) appends the specialized prompt and context for that sub-task. This demonstrates prompt engineering at scale: rather than one giant prompt for everything, the orchestration chooses a tailored prompt per task type. Overall, structured and deliberate prompt engineering at the orchestration level — using instructions, role context, examples, and criteria — is key to maximizing agent reliability.\n\n## Parallelism & Conflict Avoidance\n\n**When to Run Agents in Parallel:** Orchestrating multiple agents in parallel can speed up development, but only if their tasks won't conflict. The Parallelization Workflow follows a divide-and-conquer approach: identify subtasks that are truly independent and execute them concurrently [@pujals2025agentic]. For example, an orchestrator might split a feature into frontend and backend implementations and assign each to a different agent at the same time. Another parallel pattern is \"voting\" – run multiple agents on the same task in parallel (perhaps with different prompting styles or different model seeds) and then choose the best result. In coding, this could mean generating multiple candidate solutions concurrently and later selecting one that passes tests (\"N-best\" approach). However, if parallel agents operate on shared artifacts (e.g. editing the same file), conflicts can arise. Thus, orchestrators should only parallelize tasks that either target separate components or use a strategy to reconcile outputs.\n\n**Locking and Work Queues:** A common strategy for conflict avoidance is file-level locking or resource locking. Before an agent begins work, the orchestrator can lock the files or modules it will touch, preventing other agents from editing them until completion. A community-built \"Claude code agent farm\" demonstrates this with a coordination protocol: each agent must create a lock file listing the files/features it intends to work on, and consult a shared active work registry to ensure no overlap. If a conflict is detected (e.g. two agents want to modify utils.py), one agent will wait (queue the task) until the file is free. This effectively serializes conflicting sections while still allowing non-conflicting tasks in parallel. The registry and lock files also help avoid duplicate work – agents check the completed task log so they don't redo something another agent already finished. Such locking is a conservative heuristic but very effective: it \"eliminates merge conflicts\" and \"prevents wasted work\" in concurrent development. The downside is that overly coarse locks (e.g. locking an entire project) nullify the speed benefit, so orchestrators must strike a balance by locking at a sensible granularity (file or feature level).\n\n**Advanced Conflict Resolution (CRDTs):** Recent research explores lock-free coordination using principles from distributed systems. CodeCRDT introduces an observation-driven approach where agents don't explicitly lock resources, but instead work on a shared state (a codebase) that automatically merges changes in a conflict-free manner [@pugachev2025codecrdt]. It uses Conflict-Free Replicated Data Types to ensure that as agents make edits, their updates converge deterministically without stepping on each other. In trials, this achieved 100% merge convergence with zero low-level conflicts, eliminating the need for manual conflict resolution. Essentially, each agent can edit concurrently and the CRDT mechanism integrates their changes (like Google Docs but for code). While CRDT-based orchestration can avoid the bottleneck of locks (which can become contention points as agent count N grows), it introduces complexity: agents must deal with semantic conflicts (e.g. two agents might implement the same feature in different ways, both syntactically merged but logically redundant). CodeCRDT reported about 5–10% of such higher-level conflicts, meaning an orchestrator or later review still has to reconcile duplicate or inconsistent implementations. Nonetheless, this approach is promising for truly scalable parallel agent teams, as it sidesteps the O(N×L) contention of lock-based systems.\n\n**Heuristics for Parallel Task Selection:** In practice, orchestrators often use simple heuristics to decide parallelism: e.g. no two agents on the same file or closely related module. Some systems use static analysis of code dependency graphs to ensure two parallel tasks don't overlap in call graphs or data models. Others rely on the developer to specify which tasks are independent. For example, an orchestrator might know that adding a new API endpoint (task A) is largely independent of refactoring the logging library (task B), so it schedules those simultaneously. If there is uncertainty, a conservative orchestrator will serialize tasks to avoid the risk of conflict. In the worst case that a conflict does occur (say two agents inadvertently changed the same file), the orchestrator must have a conflict-resolution step: it could auto-merge the code (if changes are in different regions), prefer one agent's output over the other, or fall back to human intervention to reconcile differences. Generally, the goal is conflict avoidance through design: split work such that each agent has its own domain of responsibility. As one user's experience with 4 parallel Claude agents showed, dividing roles by concern (architecture vs. coding vs. testing vs. docs) inherently reduced direct conflicts and even brought positive effects like built-in peer review. Each agent had a clear lane, enabling parallel development with quality checks due to the separation of concerns.\n\nIn summary, orchestrating parallel code agents requires careful coordination: lock or isolate shared resources, or adopt novel merging strategies, so that parallelism yields speed-up without chaos. When done right (e.g. with file locks or CRDTs), teams of agents can work \"like a well-oiled machine\" in parallel.\n\n## Verification & Acceptance Criteria\n\n**Automated Testing as Ground Truth:** One of the most powerful verification techniques is running tests on the agent's output. If the task comes with a test suite or at least some example cases, the orchestrator can automatically execute the generated code and check for correctness. This pattern is evident in many frameworks. AgentCoder is explicitly built around a test-feedback loop: a Test Designer agent generates unit tests, a Test Executor runs the code on those tests, and the Programmer agent then debugs or refines the code based on failures [@huang2023agentcoder]. This ensures that code isn't accepted as \"done\" until it passes its tests. Even single-agent approaches like OpenAI's Codex have employed this idea (often called execute-and-fix): run the code, and if an error or failing test is detected, prompt the model to fix it. The orchestrator's job is to automate this loop. In PerfOrch's multi-stage pipeline, after generation they perform a validation: if the code is functionally incorrect, it triggers the Fix Stage, invoking specialized bug-fixing models iteratively until a correct solution is found [@wang2025perforch]. Only once all tests pass does it proceed to the next stage. This demonstrates an automated acceptance gate – test passing is the criterion for success. Orchestrators can also measure other aspects like performance: PerfOrch's final Refine Stage checked if runtime improved and only accepted refinements that preserved correctness and met the performance target.\n\n**Code Reviews and LLM Critics:** Besides tests, another verification layer is reviewing the code, either by humans or by a second \"reviewer\" agent. Having a separate agent critique the generated code can catch issues that tests don't (e.g. poor readability, not following style, potential inefficiencies). A pattern often cited is dual-agent coding: one agent writes code, another agent reviews it. If the reviewer finds problems, the coder agent revises accordingly. This is essentially the Evaluator-Optimizer pattern applied to code [@carpintero2025evaluator] – where the Evaluator (which could be the same LLM or a different one tuned for critique) provides feedback, and the Optimizer (generator) uses it to improve the output in another pass. Academic and industry work supports this approach. In practice, companies like Meta have prototypes where an AI code generation is always followed by an AI code review, flagging issues before any human sees the code. Ellipsis's production system likewise emphasizes accuracy over latency: they run multiple comment-generation agents and then use a filtering pipeline (including an LLM-as-judge that evaluates the proposed comments) to ensure only high-quality, non-false-positive feedback is given [@zenml2024ellipsis]. By using an LLM judge on the outputs of LLM coders, they add a second layer of assurance.\n\n**Manifests and Static Verification:** As discussed in prompt engineering, if the orchestrator provided a manifest of expected changes, verification includes checking the agent's output against that manifest. Tools like MAID runner perform static analysis on the diff: did the agent only modify the allowed files and functions? Did it include the required new function? This kind of structural verification can be automated by parsing the output or performing AST (Abstract Syntax Tree) comparison between the original and modified code. If the agent deviated (e.g. changed something unrelated), the orchestrator can reject that output or prompt a correction (\"You edited file Y which was out of scope; please revert that.\"). Additionally, the orchestrator might enforce style guides by running linters or formatters on the code and ensuring no new warnings were introduced.\n\n**Acceptance and Quality Gates:** Complex orchestrations often define multi-dimensional acceptance criteria. Functional correctness (tests passing) is usually mandatory. Beyond that, style compliance, performance metrics, security scans, etc., can be gates. For example, after an agent's code passes tests, the orchestrator might also run a static security analyzer; if it flags issues, the agent's output isn't fully accepted and a fix task is spawned (or a human review is requested for that aspect). Hugging Face's guide suggests quality gates such as \"code passes all test suites\" as a primary stopping condition for an iterative refine loop – essentially, you keep refining until the code meets the quality bar [@pujals2025agentic]. They also note using a maximum iteration limit and similarity checks to avoid infinite loops.\n\n**Handling Partial Success:** Sometimes an agent's output is partially correct – e.g. it implements one feature but misses another, or fixes some bugs but not all. In these cases, orchestrators can take a pragmatic approach: accept the improvements made and then create follow-up tasks for the remaining work. This is akin to how a human might commit a partial fix and then address the rest. Orchestrators with tracking systems (like the completed_work_log in the agent farm) can mark certain sub-goals as done and only reassign what failed. If tests are granular, the orchestrator can rerun them and see which specific tests still fail, then prompt the agent specifically about those failures on the next try. This targeted retry approach zeroes in on the unresolved issues rather than redoing everything.\n\n**Automated vs Human Acceptance:** Ultimately, the output may either be auto-merged or require human approval. Many real-world setups keep a human-in-the-loop for final approval – for example, an orchestrator might open a pull request with the agent's changes and require a human developer to review and merge. The orchestrator in this case hands off to a human at the final step. Another checkpoint is when an agent is uncertain or stuck. If an agent expresses confusion (some advanced agents might output a special token or message like \"I am not sure how to proceed\"), the orchestrator should detect that and involve a human to clarify requirements or provide a hint.\n\nIn summary, verification in agent orchestration is about establishing feedback loops and safety nets: tests to verify correctness, secondary agents or tools to verify quality, and structured checks (manifests, linters) to enforce constraints. By combining these, orchestrators can confidently determine when an agent \"succeeded\" and its output is acceptable.\n\n## Failure Modes & Recovery Strategies\n\nEven with the best planning, agents will sometimes fail – producing wrong code, no code, or even crashing. The orchestration layer must be resilient to these failures and have recovery strategies.\n\n**Graceful Retries:** The simplest recovery pattern is retrying the same prompt. Because LLM agents are stochastic, a second attempt might yield a different (possibly correct) result. Orchestrators often implement a retry-on-failure (with a limit on retries to avoid loops). If an agent times out or returns an error (e.g. if using an API that errors out), the orchestrator can automatically retry after a short delay or with a smaller context. The Claude agent farm tool, for example, monitors each agent's health and will auto-restart an agent if it crashes or gets stuck. This ensures that a transient glitch doesn't halt the entire workflow.\n\n**Refining the Prompt or Task Breakdown:** If a direct retry doesn't help, the orchestrator should try a modified approach. One common strategy is progressive prompting: if the agent failed to produce output, perhaps the instructions were too broad – so break the task into smaller sub-tasks and prompt those instead. For instance, if \"implement the foobar algorithm\" came back empty or incorrect, the orchestrator might break it into \"step 1: implement the helper function X\" and \"step 2: implement main foobar using X\". Alternatively, the orchestrator can supply more guidance in the prompt on a retry – e.g. provide a partial solution or additional hints. This resembles a fallback to a more guided prompt when the agent seems to be stuck with a high-level request. In the Prompt-Chaining considerations, it's suggested to re-prompt including details of the failure in the context if an intermediate step fails [@pujals2025agentic]. For example: \"The previous attempt resulted in a runtime error at line 20. Here is the error log… Please fix the code to handle this issue.\" This gives the agent new information to succeed on the next pass.\n\n**Alternate Agents or Models:** Another recovery pattern is trying a different agent or model. If one model repeatedly fails to solve a problem, perhaps a more powerful model (or one fine-tuned differently) could succeed. PerfOrch demonstrates this by maintaining a ranking of models for each stage – if the top choice fails, it falls back to the next best model for that task [@wang2025perforch]. In their bug-fixing stage, they iteratively invoked up to n different specialized LLMs until one fixed the bug. Similarly, if an agent like GPT-3.5 is not producing a solution, an orchestrator might escalate to GPT-4 for that task. This is a cost-performance trade-off: use cheaper models for most tasks, but have a path to involve a more capable (or simply different) model on failure.\n\n**Preserving Partial Progress:** In complex multi-step tasks, an agent might accomplish some steps successfully before failing at a later step. A robust orchestrator will preserve this partial progress so it's not lost. For example, if an agent wrote 80% of a file correctly and then struggled, the orchestrator can save the file (or keep it in memory) and instruct a subsequent agent to continue from that state. This could involve checking the agent's output for usable sections. Some frameworks treat the codebase like a persistent state that agents incrementally build – even if one agent stops mid-way, the next agent sees the codebase with whatever was added so far.\n\n**Escalating to Human or Halting:** Not all failures can or should be handled autonomously. A critical safety mechanism is to escalate to a human when automated retries are exhausted or when the cost of more retries outweighs the benefit. The orchestrator might, for instance, make 3 attempts at a task (each possibly with refined prompts or different models). If none succeed, it can stop and label the task for human attention. In a CI/CD integration, this could mean leaving a comment like \"AI agent could not resolve this issue – please review manually.\"\n\n**Logging and Learning from Failures:** Another aspect of handling failures is logging them for continuous improvement. The orchestrator should log error cases, so developers can later analyze if prompt tweaks or training could solve them. In some advanced setups, this log is fed into a \"reflexion\" agent that tries to learn from failures by adjusting future prompts.\n\nIn essence, failure recovery in orchestration is about resilience and adaptability. Try again (maybe differently), switch strategies if needed, and know when to ask for help.\n\n## Multi-Pass Architectures (Planner, Implementer, Reviewer, etc.)\n\nOne of the most powerful design patterns emerging in AI code generation is the use of multiple passes or multiple agent roles to tackle a problem in stages. Instead of a single monolithic prompt-to-code step, the task is divided into phases such as planning, implementing, and reviewing.\n\n**Planner → Implementer Workflow:** In this pattern, a Planner agent first creates a high-level plan or design, which is then used by an Implementer agent to write the actual code. The planner might outline which functions need to be created, what the approach will be, or even pseudo-code. The implementer then follows this blueprint. The self-planning study exemplifies this: the LLM generates concise, structured planning steps from the intent, then in the implementation phase it \"generates code step by step, guided by the preceding planning steps\" [@jiang2024selfplanning]. This two-pass approach yielded significantly higher correctness than a single-pass solution, because the model effectively did some problem decomposition and reasoning out loud before coding. MapCoder extends this idea to four stages: it uses one agent to recall relevant examples, another to plan the solution approach, a third to generate code, and a fourth to debug the result [@islam2024mapcoder]. Each agent is specialized for its role, and together they \"replicate the full cycle of program synthesis as observed in human developers\". The results were state-of-the-art on code benchmarks, underlining the efficacy of a multi-pass pipeline.\n\n**Reviewer or Refiner Pass:** Another common multi-pass setup is having a Reviewer agent or second pass that checks and improves the code written by an Implementer (first pass). In a multi-pass architecture, this is built-in: after code generation, the process automatically goes to a review phase. The reviewer might be tasked with ensuring the code meets the spec and then either directly editing the code to fix issues or providing feedback for the implementer to act on. This can even become an iterative loop: implementer → reviewer → implementer (for revisions) → reviewer … until the reviewer is satisfied. This approach aligns with the Evaluator-Optimizer iterative refinement pattern [@carpintero2025evaluator].\n\n**When Multi-Pass Helps:** Multi-pass shines especially for complex tasks that benefit from explicit reasoning or quality control. If a coding task requires sophisticated reasoning (algorithm design, complex logic), a planning pass helps. If it requires high assurance (safety-critical code, or just code in a large codebase where consistency matters), a review/testing pass helps. On simpler tasks (e.g. \"add a print statement\"), multi-pass might be overkill – a single agent can handle it and the overhead of planning/reviewing might outweigh the simplicity of the change.\n\n**Feedback Loops:** Multi-pass architectures inherently create a feedback loop, especially with a reviewer/corrector in the mix. The orchestrator should feed the feedback in a form the implementer can act on. For example, if a reviewer says \"Function foo doesn't handle negative inputs properly,\" the orchestrator might prompt the implementer agent with that exact feedback: \"The code was reviewed and the following issue was found: it doesn't handle negative inputs. Please update the code to address this.\"\n\n## Human-in-the-Loop in Orchestration\n\nEven with all the above automation, the human element remains important. Orchestration should consider when to involve human developers or stakeholders to guide or approve the process.\n\n**Checkpoints for Human Input:** A well-designed orchestrator will define certain points at which it pauses and seeks human input. This might be after planning – e.g., once a Planner agent produces a design, the orchestrator could show that plan to a human for approval before coding. It could also be after code generation but before acceptance – essentially treating the AI-generated code like a regular code review that a human must OK.\n\n**Surviving \"I'm stuck\" vs \"I'm done\":** Differentiating between an agent that believes it's finished vs one that gave up is key. If an agent says \"Solution complete\" but the tests are failing, clearly it was wrongly done. The orchestrator should treat that as a failure (and possibly loop in a reviewer agent or human). On the other hand, if after several tries the agent outputs, \"I cannot find an approach for this problem,\" then the orchestrator has an impasse. This is a prime moment for human-in-the-loop.\n\n**Human Oversight and Approval:** Humans may also impose guardrails by setting policies the orchestrator follows. For example, an organization might require that any code touching security-critical files must be reviewed by a human, no matter what. The orchestrator can have a rule: if task involves auth.py (or generally high-risk area), always assign it to a human or at least get human approval after the AI suggests changes.\n\n**Balancing Autonomy and Oversight:** The goal of orchestration is often to minimize human effort, but not to eliminate it entirely at the cost of quality. An insightful guideline from Anthropic is to use the simplest solution possible and only add complexity (autonomy) when needed [@anthropic2024agents]. This suggests that if a task can be handled with a single LLM call plus a quick human tweak, that might be preferable to spinning up a whole autonomous multi-agent system.\n\n## Conclusion\n\nOrchestrating black-box code agents is an exercise in applying sound software engineering principles to AI-driven automation. The patterns identified – from task assignment and context curation to multi-pass workflows and human oversight – collectively form a toolkit for reliable AI-assisted development.\n\nIn designing an orchestration framework, one should treat AI agents as fallible collaborators that need structure and guidance – much like human junior developers. The orchestrator's role is part project manager, part version-control system, and part QA engineer for this AI \"team.\" It decides who does what (task assignment), gives them what they need (context), tells them how to do it (prompt engineering and constraints), lets them work together safely (parallelism control), checks their work (verification), handles when they get it wrong (failure recovery), encourages iterative improvement (multi-pass), and knows when to ask a senior (human-in-loop).\n\n## References\n","source_path":"research/orchestration-survey.md"},{"slug":"context-injection","title":"Context Injection","content_html":"<h1 id=\"context-injection\">Context Injection<a class=\"heading-anchor\" href=\"#context-injection\" aria-label=\"Link to heading\">#</a></h1>\n<p>What information noface provides to agents and how.</p>\n<h2 id=\"current-design\">Current Design<a class=\"heading-anchor\" href=\"#current-design\" aria-label=\"Link to heading\">#</a></h2>\n<p>The implementation prompt includes:</p>\n<ol>\n<li><strong>Issue description</strong> — from beads</li>\n<li><strong>Manifest</strong> — which files the agent can touch</li>\n<li><strong>Build/test commands</strong> — from <code>.noface.toml</code></li>\n<li><strong>Design docs</strong> — fetched from monowiki if configured</li>\n<li><strong>Workflow instructions</strong> — step-by-step process (implement → test → commit)</li>\n</ol>\n<p>Context is capped by <code>max_context_docs</code> setting (default 5).</p>\n<h2 id=\"relation-to-survey\">Relation to Survey<a class=\"heading-anchor\" href=\"#relation-to-survey\" aria-label=\"Link to heading\">#</a></h2>\n<p>The survey warns about <strong>context dilution</strong>:</p>\n<blockquote>\n<p>\"Recent studies have shown that even if all provided context is relevant, performance can degrade substantially (by 13–85%) as input length grows.\"</p>\n</blockquote>\n<p>And recommends <strong>iterative context expansion</strong>:</p>\n<blockquote>\n<p>\"The Context-Augmentation pattern described by Hugging Face highlights that context expansion should be iterative and need-based: the agent identifies what extra info it needs, the orchestrator fetches it.\"</p>\n</blockquote>\n<h2 id=\"design-decisions\">Design Decisions<a class=\"heading-anchor\" href=\"#design-decisions\" aria-label=\"Link to heading\">#</a></h2>\n<h3 id=\"1-context-budget-dynamic-based-on-token-budget-and-complexity\">1. Context Budget: Dynamic based on token budget and complexity<a class=\"heading-anchor\" href=\"#1-context-budget-dynamic-based-on-token-budget-and-complexity\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Make context budget dynamic, not fixed.</p>\n<p><strong>Approach:</strong></p>\n<ul>\n<li>Define a target fraction of prompt tokens for retrieved docs (e.g., 30–40%)</li>\n<li>Fill that slot with as many top-ranked docs as fit</li>\n<li>Complexity heuristics:\n<ul>\n<li>More files / cross-cutting change → allow more docs</li>\n<li>Tiny, local change → maybe 0–2 docs only</li>\n</ul>\n</li>\n</ul>\n<p>Keep <code>max_context_docs = 5</code> as a hard cap, but choose K ∈ [0, 5] per issue based on:</p>\n<ul>\n<li>Issue complexity score</li>\n<li>Available token budget</li>\n<li>Relevance scores of candidate docs</li>\n</ul>\n<h3 id=\"2-relevance-ranking-hybrid-bm25-embeddings\">2. Relevance Ranking: Hybrid BM25 + embeddings<a class=\"heading-anchor\" href=\"#2-relevance-ranking-hybrid-bm25-embeddings\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Use hybrid ranking instead of simple wikilink fetching.</p>\n<p><strong>Pipeline:</strong></p>\n<ol>\n<li><strong>Candidate generation:</strong>\n<ul>\n<li>Wikilinks / explicit references from the issue</li>\n<li>BM25 search (already have <code>src/bm25.zig</code>)</li>\n</ul>\n</li>\n<li><strong>Re-ranking:</strong>\n<ul>\n<li>Embedding similarity (doc ↔ issue description)</li>\n</ul>\n</li>\n<li><strong>Final score:</strong><div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">score = α * BM25 + β * embedding_score + boost_if_explicitly_linked\n</span></pre>\n\n</div></li>\n</ol>\n<p>This will beat naive \"first N by wikilink\" in most repos.</p>\n<h3 id=\"3-code-context-manifest-files-relevant-snippets\">3. Code Context: Manifest files + relevant snippets<a class=\"heading-anchor\" href=\"#3-code-context-manifest-files-relevant-snippets\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Include manifest files plus targeted snippets from related code.</p>\n<p><strong>Always include:</strong></p>\n<ul>\n<li>The files in the manifest that the agent is allowed to edit</li>\n</ul>\n<p><strong>Optionally include:</strong></p>\n<ul>\n<li>Small snippets from related files:\n<ul>\n<li>Direct callers/callees (if xref info available)</li>\n<li>BM25/embedding matches for key identifiers in the issue</li>\n</ul>\n</li>\n</ul>\n<p><strong>Guardrails:</strong></p>\n<ul>\n<li>Truncate to relevant functions rather than whole files when possible</li>\n<li>If file is huge, include:\n<ul>\n<li>Signature + docstring + 1–2 nearby functions</li>\n</ul>\n</li>\n<li>Use BM25 over code to pick relevant ranges, not entire files</li>\n</ul>\n<h3 id=\"4-context-freshness-downrank-stale-docs-dont-hard-exclude\">4. Context Freshness: Downrank stale docs, don't hard exclude<a class=\"heading-anchor\" href=\"#4-context-freshness-downrank-stale-docs-dont-hard-exclude\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Add freshness metadata + downranking, not hard exclusion.</p>\n<p><strong>For each doc, track:</strong></p>\n<ul>\n<li><code>last_updated</code> timestamp</li>\n<li>Optionally \"tied-to-commit\" info</li>\n</ul>\n<p><strong>Heuristics:</strong></p>\n<ul>\n<li>If doc references symbols that no longer exist in code → heavily downrank or mark stale</li>\n<li>If <code>now - last_updated &gt; threshold</code> and code around it changed a lot → downrank</li>\n</ul>\n<p><strong>In the prompt, for borderline docs:</strong></p>\n<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">Note: this doc may be outdated; prefer the current code as source of truth.\n</span></pre>\n\n</div>\n<p>Relevant but slightly stale docs can still help, but the model is warned.</p>\n<h3 id=\"5-negative-context-explicit-exclusion-list\">5. Negative Context: Explicit exclusion list<a class=\"heading-anchor\" href=\"#5-negative-context-explicit-exclusion-list\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Define explicit exclusions + heuristics.</p>\n<p><strong>Path-based exclusions:</strong></p>\n<ul>\n<li><code>vendor/</code>, <code>node_modules/</code>, <code>dist/</code>, <code>build/</code>, <code>.cache/</code></li>\n<li><code>*.min.js</code>, large generated protobufs</li>\n</ul>\n<p><strong>Size-based:</strong></p>\n<ul>\n<li>Files &gt; X KB not included unless explicitly requested</li>\n</ul>\n<p><strong>Type-based:</strong></p>\n<ul>\n<li>Binary blobs, lockfiles, big JSON dumps</li>\n</ul>\n<p>Implement in retrieval pipeline so excluded files never show up as candidates.</p>\n<h3 id=\"6-agent-requested-context-thin-orchestrator-layer-harness-delegation\">6. Agent-Requested Context: Thin orchestrator layer + harness delegation<a class=\"heading-anchor\" href=\"#6-agent-requested-context-thin-orchestrator-layer-harness-delegation\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Lean on Claude Code's native file-opening capabilities, add thin orchestrator layer for explicit requests.</p>\n<p><strong>If using Claude Code / similar harness:</strong></p>\n<ul>\n<li>Let the harness handle intra-session context (open file, run search, etc.)</li>\n</ul>\n<p><strong>At orchestration level:</strong></p>\n<ul>\n<li>Support structured \"needs more context\" signals:<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">NEED_CODE: auth/login.zig\n</span><span style=\"color:#323232;\">NEED_DOC: docs/auth.md\n</span></pre>\n\n</div></li>\n<li>When signal detected:\n<ul>\n<li>Fetch &amp; add context on next call, or</li>\n<li>Delegate to harness's file-opening APIs</li>\n</ul>\n</li>\n</ul>\n<p><strong>Don't</strong> build a whole second interactive context loop on top of Claude.\n<strong>Do</strong> add a thin layer to observe/log/respond when agent clearly asks for more info.</p>\n<h2 id=\"implementation-notes\">Implementation Notes<a class=\"heading-anchor\" href=\"#implementation-notes\" aria-label=\"Link to heading\">#</a></h2>\n<p>See <code>src/loop.zig:buildImplementationPrompt</code> and <code>src/monowiki.zig</code>.</p>\n<h3 id=\"todo\">TODO<a class=\"heading-anchor\" href=\"#todo\" aria-label=\"Link to heading\">#</a></h3>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"/>\nImplement dynamic context budget based on issue complexity</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd hybrid ranking (BM25 + embeddings)</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd freshness tracking to monowiki docs</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nImplement exclusion list in retrieval pipeline</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nParse <code>NEED_CODE</code> / <code>NEED_DOC</code> signals from agent output</li>\n</ul>\n","frontmatter":{"title":"Context Injection","description":null,"summary":null,"date":null,"type":"essay","tags":["design","context","prompts","monowiki"],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":["design","context","prompts","monowiki"],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":[],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#context-injection\">Context Injection</a></li><li class=\"toc-level-2\"><a href=\"#current-design\">Current Design</a></li><li class=\"toc-level-2\"><a href=\"#relation-to-survey\">Relation to Survey</a></li><li class=\"toc-level-2\"><a href=\"#design-decisions\">Design Decisions</a></li><li class=\"toc-level-3\"><a href=\"#1-context-budget-dynamic-based-on-token-budget-and-complexity\">1. Context Budget: Dynamic based on token budget and complexity</a></li><li class=\"toc-level-3\"><a href=\"#2-relevance-ranking-hybrid-bm25-embeddings\">2. Relevance Ranking: Hybrid BM25 + embeddings</a></li><li class=\"toc-level-3\"><a href=\"#3-code-context-manifest-files-relevant-snippets\">3. Code Context: Manifest files + relevant snippets</a></li><li class=\"toc-level-3\"><a href=\"#4-context-freshness-downrank-stale-docs-dont-hard-exclude\">4. Context Freshness: Downrank stale docs, don&#39;t hard exclude</a></li><li class=\"toc-level-3\"><a href=\"#5-negative-context-explicit-exclusion-list\">5. Negative Context: Explicit exclusion list</a></li><li class=\"toc-level-3\"><a href=\"#6-agent-requested-context-thin-orchestrator-layer-harness-delegation\">6. Agent-Requested Context: Thin orchestrator layer + harness delegation</a></li><li class=\"toc-level-2\"><a href=\"#implementation-notes\">Implementation Notes</a></li><li class=\"toc-level-3\"><a href=\"#todo\">TODO</a></li></ul></nav>","raw_body":"# Context Injection\n\nWhat information noface provides to agents and how.\n\n## Current Design\n\nThe implementation prompt includes:\n\n1. **Issue description** — from beads\n2. **Manifest** — which files the agent can touch\n3. **Build/test commands** — from `.noface.toml`\n4. **Design docs** — fetched from monowiki if configured\n5. **Workflow instructions** — step-by-step process (implement → test → commit)\n\nContext is capped by `max_context_docs` setting (default 5).\n\n## Relation to Survey\n\nThe survey warns about **context dilution**:\n\n> \"Recent studies have shown that even if all provided context is relevant, performance can degrade substantially (by 13–85%) as input length grows.\"\n\nAnd recommends **iterative context expansion**:\n\n> \"The Context-Augmentation pattern described by Hugging Face highlights that context expansion should be iterative and need-based: the agent identifies what extra info it needs, the orchestrator fetches it.\"\n\n## Design Decisions\n\n### 1. Context Budget: Dynamic based on token budget and complexity\n\n**Decision:** Make context budget dynamic, not fixed.\n\n**Approach:**\n- Define a target fraction of prompt tokens for retrieved docs (e.g., 30–40%)\n- Fill that slot with as many top-ranked docs as fit\n- Complexity heuristics:\n  - More files / cross-cutting change → allow more docs\n  - Tiny, local change → maybe 0–2 docs only\n\nKeep `max_context_docs = 5` as a hard cap, but choose K ∈ [0, 5] per issue based on:\n- Issue complexity score\n- Available token budget\n- Relevance scores of candidate docs\n\n### 2. Relevance Ranking: Hybrid BM25 + embeddings\n\n**Decision:** Use hybrid ranking instead of simple wikilink fetching.\n\n**Pipeline:**\n1. **Candidate generation:**\n   - Wikilinks / explicit references from the issue\n   - BM25 search (already have `src/bm25.zig`)\n2. **Re-ranking:**\n   - Embedding similarity (doc ↔ issue description)\n3. **Final score:**\n   ```\n   score = α * BM25 + β * embedding_score + boost_if_explicitly_linked\n   ```\n\nThis will beat naive \"first N by wikilink\" in most repos.\n\n### 3. Code Context: Manifest files + relevant snippets\n\n**Decision:** Include manifest files plus targeted snippets from related code.\n\n**Always include:**\n- The files in the manifest that the agent is allowed to edit\n\n**Optionally include:**\n- Small snippets from related files:\n  - Direct callers/callees (if xref info available)\n  - BM25/embedding matches for key identifiers in the issue\n\n**Guardrails:**\n- Truncate to relevant functions rather than whole files when possible\n- If file is huge, include:\n  - Signature + docstring + 1–2 nearby functions\n- Use BM25 over code to pick relevant ranges, not entire files\n\n### 4. Context Freshness: Downrank stale docs, don't hard exclude\n\n**Decision:** Add freshness metadata + downranking, not hard exclusion.\n\n**For each doc, track:**\n- `last_updated` timestamp\n- Optionally \"tied-to-commit\" info\n\n**Heuristics:**\n- If doc references symbols that no longer exist in code → heavily downrank or mark stale\n- If `now - last_updated > threshold` and code around it changed a lot → downrank\n\n**In the prompt, for borderline docs:**\n```\nNote: this doc may be outdated; prefer the current code as source of truth.\n```\n\nRelevant but slightly stale docs can still help, but the model is warned.\n\n### 5. Negative Context: Explicit exclusion list\n\n**Decision:** Define explicit exclusions + heuristics.\n\n**Path-based exclusions:**\n- `vendor/`, `node_modules/`, `dist/`, `build/`, `.cache/`\n- `*.min.js`, large generated protobufs\n\n**Size-based:**\n- Files > X KB not included unless explicitly requested\n\n**Type-based:**\n- Binary blobs, lockfiles, big JSON dumps\n\nImplement in retrieval pipeline so excluded files never show up as candidates.\n\n### 6. Agent-Requested Context: Thin orchestrator layer + harness delegation\n\n**Decision:** Lean on Claude Code's native file-opening capabilities, add thin orchestrator layer for explicit requests.\n\n**If using Claude Code / similar harness:**\n- Let the harness handle intra-session context (open file, run search, etc.)\n\n**At orchestration level:**\n- Support structured \"needs more context\" signals:\n  ```\n  NEED_CODE: auth/login.zig\n  NEED_DOC: docs/auth.md\n  ```\n- When signal detected:\n  - Fetch & add context on next call, or\n  - Delegate to harness's file-opening APIs\n\n**Don't** build a whole second interactive context loop on top of Claude.\n**Do** add a thin layer to observe/log/respond when agent clearly asks for more info.\n\n## Implementation Notes\n\nSee `src/loop.zig:buildImplementationPrompt` and `src/monowiki.zig`.\n\n### TODO\n- [ ] Implement dynamic context budget based on issue complexity\n- [ ] Add hybrid ranking (BM25 + embeddings)\n- [ ] Add freshness tracking to monowiki docs\n- [ ] Implement exclusion list in retrieval pipeline\n- [ ] Parse `NEED_CODE` / `NEED_DOC` signals from agent output\n","source_path":"design/context-injection.md"},{"slug":"parallel-execution","title":"Parallel Execution","content_html":"<h1 id=\"parallel-execution\">Parallel Execution<a class=\"heading-anchor\" href=\"#parallel-execution\" aria-label=\"Link to heading\">#</a></h1>\n<p>How noface runs multiple agents concurrently without conflicts.</p>\n<h2 id=\"current-design\">Current Design<a class=\"heading-anchor\" href=\"#current-design\" aria-label=\"Link to heading\">#</a></h2>\n<ol>\n<li><strong>Planner pass</strong> groups issues into batches based on manifest analysis</li>\n<li>Issues in the same batch have <strong>disjoint primary_files</strong> (no overlap)</li>\n<li><strong>Worker pool</strong> spawns up to N parallel processes (configurable, default 8)</li>\n<li><strong>Lock entries</strong> track which files are held by which worker</li>\n<li>Batches execute sequentially; issues within a batch execute in parallel</li>\n</ol>\n<h2 id=\"relation-to-survey\">Relation to Survey<a class=\"heading-anchor\" href=\"#relation-to-survey\" aria-label=\"Link to heading\">#</a></h2>\n<p>This implements the <strong>file-level locking</strong> pattern from <a href=\"/orchestration-survey.html\">orchestration-survey</a>:</p>\n<blockquote>\n<p>\"A community-built 'Claude code agent farm' demonstrates this with a coordination protocol: each agent must create a lock file listing the files/features it intends to work on, and consult a shared active work registry to ensure no overlap.\"</p>\n</blockquote>\n<p>The survey also discusses <strong>CRDTs</strong> for lock-free coordination (CodeCRDT achieved 100% merge convergence). This is more scalable but more complex.</p>\n<h2 id=\"design-decisions\">Design Decisions<a class=\"heading-anchor\" href=\"#design-decisions\" aria-label=\"Link to heading\">#</a></h2>\n<h3 id=\"1-lock-granularity-file-level-only-for-now\">1. Lock Granularity: File-level only (for now)<a class=\"heading-anchor\" href=\"#1-lock-granularity-file-level-only-for-now\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Start with file-level locking only. Add function-level as a future optimization for specific languages.</p>\n<p><strong>File-level:</strong></p>\n<ul>\n<li>Easy to reason about</li>\n<li>Guarantees no diff overlap</li>\n</ul>\n<p><strong>Function-level (deferred):</strong></p>\n<ul>\n<li>Needs parser, symbol table, stable function IDs</li>\n<li>Harder for languages with macros, generated code, ad-hoc patterns</li>\n</ul>\n<p><strong>Instead, get most of the win by:</strong></p>\n<ul>\n<li>Task design: aim one issue per file or small set of files</li>\n<li>Using manifest to ensure each issue's scope is small and minimally overlapping</li>\n</ul>\n<p>If later we need function-level:</p>\n<ul>\n<li>Limit to languages where we already have a good AST + symbol index</li>\n<li>Keep file-level as fallback</li>\n</ul>\n<h3 id=\"2-batch-sizing-adaptive-concurrency\">2. Batch Sizing: Adaptive concurrency<a class=\"heading-anchor\" href=\"#2-batch-sizing-adaptive-concurrency\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Use adaptive concurrency based on runtime signals.</p>\n<p><strong>Config:</strong></p>\n<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">[parallelism]\n</span><span style=\"color:#323232;\">max_concurrent_global = 8      # hard cap\n</span><span style=\"color:#323232;\">max_concurrent_per_repo = 4    # per-repo cap\n</span><span style=\"color:#323232;\">initial_concurrency = 2        # starting point\n</span></pre>\n\n</div>\n<p><strong>Runtime signals:</strong></p>\n<ul>\n<li>Average agent latency</li>\n<li>Queue length</li>\n<li>Error rate</li>\n</ul>\n<p><strong>Policy:</strong></p>\n<ol>\n<li>Start with N = 2</li>\n<li>If queue backlog grows and success rate is high → increase toward max</li>\n<li>If failures/timeouts increase or host is resource constrained → back off</li>\n</ol>\n<p>Also: maintain a \"serial bucket\" for issues touching very hot files (see below).</p>\n<h3 id=\"3-lock-contention-hot-file-detection-mitigation\">3. Lock Contention: Hot file detection + mitigation<a class=\"heading-anchor\" href=\"#3-lock-contention-hot-file-detection-mitigation\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Track per-file lock statistics and treat \"hot files\" specially.</p>\n<p><strong>Metrics to track:</strong></p>\n<ul>\n<li>Lock wait time per file</li>\n<li>Number of pending issues referencing each file</li>\n</ul>\n<p><strong>If a file (e.g., <code>main.zig</code>) is often a bottleneck:</strong></p>\n<ul>\n<li>Force tasks that touch that file into a single-threaded lane (queue)</li>\n<li>Encourage planner to batch related issues touching that file into one larger issue</li>\n<li>Suggest refactor tasks: spawn meta-issue \"split main.zig into modules\"</li>\n</ul>\n<p><strong>Implementation:</strong></p>\n<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">// In-memory or persisted\n</span><span style=\"color:#323232;\">const HotFileStats = struct {\n</span><span style=\"color:#323232;\">    lock_count: u32,\n</span><span style=\"color:#323232;\">    avg_wait_ms: u64,\n</span><span style=\"color:#323232;\">    queue_depth: u32,\n</span><span style=\"color:#323232;\">};\n</span><span style=\"color:#323232;\">// Map: file_path -&gt; HotFileStats\n</span></pre>\n\n</div>\n<p>Use this when scheduling new issues.</p>\n<h3 id=\"4-conflict-recovery-three-step-policy\">4. Conflict Recovery: Three-step policy<a class=\"heading-anchor\" href=\"#4-conflict-recovery-three-step-policy\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Define a concrete conflict resolution pipeline.</p>\n<p><strong>Step 1: Detect</strong></p>\n<ul>\n<li>Two diffs touch overlapping hunks, or</li>\n<li><code>git apply</code> fails</li>\n</ul>\n<p><strong>Step 2: Auto-merge</strong></p>\n<ul>\n<li>Try a 3-way merge (base, A, B)</li>\n<li>If succeeds cleanly, run tests</li>\n<li>If tests pass, accept merged result</li>\n</ul>\n<p><strong>Step 3: LLM-assisted merge</strong></p>\n<ul>\n<li>Spawn a \"merge agent\" with both diffs + conflicts</li>\n<li>Ask it to produce a unified patch</li>\n<li>Run tests again</li>\n</ul>\n<p><strong>Step 4: Escalate</strong></p>\n<ul>\n<li>If merge agent fails or tests still fail</li>\n<li>Escalate to human</li>\n<li>Leave a merge PR with both original diffs + explanation</li>\n</ul>\n<h3 id=\"5-crdt-exploration-ast-aware-patching-as-middle-ground\">5. CRDT Exploration: AST-aware patching as middle ground<a class=\"heading-anchor\" href=\"#5-crdt-exploration-ast-aware-patching-as-middle-ground\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Use AST-aware patching + 3-way merge instead of full CRDTs.</p>\n<p><strong>Approach:</strong></p>\n<ul>\n<li>Ask agents to emit structured edits per file:\n<ul>\n<li>\"Replace function foo body with …\"</li>\n<li>\"Add new function bar…\"</li>\n</ul>\n</li>\n<li>Internally apply these as AST transformations, not raw text splice</li>\n</ul>\n<p><strong>Merge behavior:</strong></p>\n<ul>\n<li>Two agents modifying different functions in the same file → trivially merge</li>\n<li>Same function → conflict; go through the conflict policy above</li>\n</ul>\n<p>This gets most of the benefits of CRDT-style structural merges without full-blown replicated state machinery.</p>\n<h2 id=\"implementation-notes\">Implementation Notes<a class=\"heading-anchor\" href=\"#implementation-notes\" aria-label=\"Link to heading\">#</a></h2>\n<p>See <code>src/worker_pool.zig:WorkerPool</code> and <code>src/state.zig:LockEntry</code>.</p>\n<h3 id=\"todo\">TODO<a class=\"heading-anchor\" href=\"#todo\" aria-label=\"Link to heading\">#</a></h3>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd <code>HotFileStats</code> tracking</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nImplement adaptive concurrency (start low, ramp up)</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd serial lane for hot files</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nImplement 3-way merge fallback</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nDesign structured edit format for AST-aware patching</li>\n</ul>\n","frontmatter":{"title":"Parallel Execution","description":null,"summary":null,"date":null,"type":"essay","tags":["design","parallelism","batching","locking"],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":["design","parallelism","batching","locking"],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":["orchestration-survey"],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#parallel-execution\">Parallel Execution</a></li><li class=\"toc-level-2\"><a href=\"#current-design\">Current Design</a></li><li class=\"toc-level-2\"><a href=\"#relation-to-survey\">Relation to Survey</a></li><li class=\"toc-level-2\"><a href=\"#design-decisions\">Design Decisions</a></li><li class=\"toc-level-3\"><a href=\"#1-lock-granularity-file-level-only-for-now\">1. Lock Granularity: File-level only (for now)</a></li><li class=\"toc-level-3\"><a href=\"#2-batch-sizing-adaptive-concurrency\">2. Batch Sizing: Adaptive concurrency</a></li><li class=\"toc-level-3\"><a href=\"#3-lock-contention-hot-file-detection-mitigation\">3. Lock Contention: Hot file detection + mitigation</a></li><li class=\"toc-level-3\"><a href=\"#4-conflict-recovery-three-step-policy\">4. Conflict Recovery: Three-step policy</a></li><li class=\"toc-level-3\"><a href=\"#5-crdt-exploration-ast-aware-patching-as-middle-ground\">5. CRDT Exploration: AST-aware patching as middle ground</a></li><li class=\"toc-level-2\"><a href=\"#implementation-notes\">Implementation Notes</a></li><li class=\"toc-level-3\"><a href=\"#todo\">TODO</a></li></ul></nav>","raw_body":"# Parallel Execution\n\nHow noface runs multiple agents concurrently without conflicts.\n\n## Current Design\n\n1. **Planner pass** groups issues into batches based on manifest analysis\n2. Issues in the same batch have **disjoint primary_files** (no overlap)\n3. **Worker pool** spawns up to N parallel processes (configurable, default 8)\n4. **Lock entries** track which files are held by which worker\n5. Batches execute sequentially; issues within a batch execute in parallel\n\n## Relation to Survey\n\nThis implements the **file-level locking** pattern from [[orchestration-survey]]:\n\n> \"A community-built 'Claude code agent farm' demonstrates this with a coordination protocol: each agent must create a lock file listing the files/features it intends to work on, and consult a shared active work registry to ensure no overlap.\"\n\nThe survey also discusses **CRDTs** for lock-free coordination (CodeCRDT achieved 100% merge convergence). This is more scalable but more complex.\n\n## Design Decisions\n\n### 1. Lock Granularity: File-level only (for now)\n\n**Decision:** Start with file-level locking only. Add function-level as a future optimization for specific languages.\n\n**File-level:**\n- Easy to reason about\n- Guarantees no diff overlap\n\n**Function-level (deferred):**\n- Needs parser, symbol table, stable function IDs\n- Harder for languages with macros, generated code, ad-hoc patterns\n\n**Instead, get most of the win by:**\n- Task design: aim one issue per file or small set of files\n- Using manifest to ensure each issue's scope is small and minimally overlapping\n\nIf later we need function-level:\n- Limit to languages where we already have a good AST + symbol index\n- Keep file-level as fallback\n\n### 2. Batch Sizing: Adaptive concurrency\n\n**Decision:** Use adaptive concurrency based on runtime signals.\n\n**Config:**\n```toml\n[parallelism]\nmax_concurrent_global = 8      # hard cap\nmax_concurrent_per_repo = 4    # per-repo cap\ninitial_concurrency = 2        # starting point\n```\n\n**Runtime signals:**\n- Average agent latency\n- Queue length\n- Error rate\n\n**Policy:**\n1. Start with N = 2\n2. If queue backlog grows and success rate is high → increase toward max\n3. If failures/timeouts increase or host is resource constrained → back off\n\nAlso: maintain a \"serial bucket\" for issues touching very hot files (see below).\n\n### 3. Lock Contention: Hot file detection + mitigation\n\n**Decision:** Track per-file lock statistics and treat \"hot files\" specially.\n\n**Metrics to track:**\n- Lock wait time per file\n- Number of pending issues referencing each file\n\n**If a file (e.g., `main.zig`) is often a bottleneck:**\n- Force tasks that touch that file into a single-threaded lane (queue)\n- Encourage planner to batch related issues touching that file into one larger issue\n- Suggest refactor tasks: spawn meta-issue \"split main.zig into modules\"\n\n**Implementation:**\n```zig\n// In-memory or persisted\nconst HotFileStats = struct {\n    lock_count: u32,\n    avg_wait_ms: u64,\n    queue_depth: u32,\n};\n// Map: file_path -> HotFileStats\n```\n\nUse this when scheduling new issues.\n\n### 4. Conflict Recovery: Three-step policy\n\n**Decision:** Define a concrete conflict resolution pipeline.\n\n**Step 1: Detect**\n- Two diffs touch overlapping hunks, or\n- `git apply` fails\n\n**Step 2: Auto-merge**\n- Try a 3-way merge (base, A, B)\n- If succeeds cleanly, run tests\n- If tests pass, accept merged result\n\n**Step 3: LLM-assisted merge**\n- Spawn a \"merge agent\" with both diffs + conflicts\n- Ask it to produce a unified patch\n- Run tests again\n\n**Step 4: Escalate**\n- If merge agent fails or tests still fail\n- Escalate to human\n- Leave a merge PR with both original diffs + explanation\n\n### 5. CRDT Exploration: AST-aware patching as middle ground\n\n**Decision:** Use AST-aware patching + 3-way merge instead of full CRDTs.\n\n**Approach:**\n- Ask agents to emit structured edits per file:\n  - \"Replace function foo body with …\"\n  - \"Add new function bar…\"\n- Internally apply these as AST transformations, not raw text splice\n\n**Merge behavior:**\n- Two agents modifying different functions in the same file → trivially merge\n- Same function → conflict; go through the conflict policy above\n\nThis gets most of the benefits of CRDT-style structural merges without full-blown replicated state machinery.\n\n## Implementation Notes\n\nSee `src/worker_pool.zig:WorkerPool` and `src/state.zig:LockEntry`.\n\n### TODO\n- [ ] Add `HotFileStats` tracking\n- [ ] Implement adaptive concurrency (start low, ramp up)\n- [ ] Add serial lane for hot files\n- [ ] Implement 3-way merge fallback\n- [ ] Design structured edit format for AST-aware patching\n","source_path":"design/parallel-execution.md"},{"slug":"failure-recovery","title":"Failure Recovery","content_html":"<h1 id=\"failure-recovery\">Failure Recovery<a class=\"heading-anchor\" href=\"#failure-recovery\" aria-label=\"Link to heading\">#</a></h1>\n<p>How noface handles agent failures.</p>\n<h2 id=\"current-design\">Current Design<a class=\"heading-anchor\" href=\"#current-design\" aria-label=\"Link to heading\">#</a></h2>\n<p>noface handles several failure modes:</p>\n<ol>\n<li><strong>Transient failures</strong> (429, 5xx, network) — retry up to 3x with exponential backoff</li>\n<li><strong>Manifest violations</strong> — rollback offending files, retry with stricter prompt</li>\n<li><strong>Timeouts</strong> (no output for N seconds) — break down issue into smaller tasks</li>\n<li><strong>Crash recovery</strong> — on startup, detect in-progress work from previous run, reset stale locks, restore state</li>\n</ol>\n<p>Each attempt is recorded in state with outcome (success/failed/timeout/violation).</p>\n<h2 id=\"relation-to-survey\">Relation to Survey<a class=\"heading-anchor\" href=\"#relation-to-survey\" aria-label=\"Link to heading\">#</a></h2>\n<p>The survey describes several recovery patterns:</p>\n<p><strong>Graceful retries:</strong></p>\n<blockquote>\n<p>\"The simplest recovery pattern is retrying the same prompt. Because LLM agents are stochastic, a second attempt might yield a different (possibly correct) result.\"</p>\n</blockquote>\n<p><strong>Progressive prompting:</strong></p>\n<blockquote>\n<p>\"If a direct retry doesn't help, the orchestrator should try a modified approach... break the task into smaller sub-tasks and prompt those instead.\"</p>\n</blockquote>\n<p><strong>Preserving partial progress:</strong></p>\n<blockquote>\n<p>\"A robust orchestrator will preserve this partial progress so it's not lost.\"</p>\n</blockquote>\n<h2 id=\"design-decisions\">Design Decisions<a class=\"heading-anchor\" href=\"#design-decisions\" aria-label=\"Link to heading\">#</a></h2>\n<h3 id=\"1-retry-strategies-informed-retries-not-blind\">1. Retry Strategies: Informed retries, not blind<a class=\"heading-anchor\" href=\"#1-retry-strategies-informed-retries-not-blind\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Retries should include failure context and self-reflection.</p>\n<p><strong>On retry, include:</strong></p>\n<ul>\n<li>Previous attempt's diff</li>\n<li>Summarized test output / error</li>\n<li>Instructions:<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">Your previous attempt failed with: [error]. Fix that specific problem\n</span><span style=\"color:#323232;\">while preserving working parts.\n</span></pre>\n\n</div></li>\n</ul>\n<p><strong>Optional self-reflection step:</strong></p>\n<ul>\n<li>Ask model: \"Explain why your last attempt failed and how you will fix it\"</li>\n<li>Feed that back into the actual implementation prompt</li>\n</ul>\n<h3 id=\"2-task-breakdown-minimal-concrete-decomposition\">2. Task Breakdown: Minimal, concrete decomposition<a class=\"heading-anchor\" href=\"#2-task-breakdown-minimal-concrete-decomposition\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Implement a simple breakdown strategy when issues are too large.</p>\n<p><strong>Trigger breakdown when:</strong></p>\n<ul>\n<li>Issue times out repeatedly, or</li>\n<li>Fails after N attempts with \"too big\" signature (many files, large diff)</li>\n</ul>\n<p><strong>Breakdown agent:</strong></p>\n<ul>\n<li>Run planner with a different prompt to propose sub-issues:\n<ul>\n<li>e.g., \"Update schema in A\", \"Update API in B\", \"Update tests in C\"</li>\n</ul>\n</li>\n<li>Turn these into child issues, mark parent as an \"epic\"</li>\n</ul>\n<p><strong>First version (simple):</strong></p>\n<ul>\n<li>Split by file: if manifest has 5 files, create 5 issues, each scoped to one file</li>\n</ul>\n<p>Iterate toward more semantic breakdowns later.</p>\n<h3 id=\"3-partial-progress-keep-on-branch-all-or-nothing-to-main\">3. Partial Progress: Keep on branch, all-or-nothing to main<a class=\"heading-anchor\" href=\"#3-partial-progress-keep-on-branch-all-or-nothing-to-main\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Preserve partial progress in scratch branches, but merge atomically.</p>\n<p><strong>Design:</strong></p>\n<ul>\n<li>Each issue has a scratch branch or temp workspace</li>\n<li>Each attempt commits to that branch</li>\n<li>If one file fails, you still keep the other 3 as commits in the branch</li>\n<li>Only when tests + gates pass do you merge that branch to main</li>\n</ul>\n<p>This preserves partial progress for future attempts without exposing half-baked changes to main.</p>\n<h3 id=\"4-model-escalation-simple-escalation-policy\">4. Model Escalation: Simple escalation policy<a class=\"heading-anchor\" href=\"#4-model-escalation-simple-escalation-policy\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Define a simple model escalation policy.</p>\n<p><strong>For each issue:</strong></p>\n<ul>\n<li>First 1–2 attempts: <code>default_model</code> (cheaper)</li>\n<li>If still failing with \"correctable\" errors (tests, syntax): escalate to <code>strong_model</code></li>\n<li>Cap total attempts across all models</li>\n</ul>\n<p>Keep configurable so users can opt-out if they only have access to one model.</p>\n<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">[retry]\n</span><span style=\"color:#323232;\">default_model = &quot;claude-sonnet&quot;\n</span><span style=\"color:#323232;\">escalation_model = &quot;claude-opus&quot;\n</span><span style=\"color:#323232;\">escalate_after_attempts = 2\n</span><span style=\"color:#323232;\">max_total_attempts = 5\n</span></pre>\n\n</div>\n<h3 id=\"5-failure-classification-taxonomy-with-strategy-mapping\">5. Failure Classification: Taxonomy with strategy mapping<a class=\"heading-anchor\" href=\"#5-failure-classification-taxonomy-with-strategy-mapping\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Introduce a failure taxonomy and map each to a strategy.</p>\n<p><strong>Taxonomy:</strong></p>\n<table><thead><tr><th>Failure Type</th><th>Strategy</th></tr></thead><tbody>\n<tr><td><code>SYNTAX_ERROR</code></td><td>Re-prompt with same context + \"fix syntax error\"</td></tr>\n<tr><td><code>RUNTIME_ERROR</code></td><td>Include stack trace, ask to fix</td></tr>\n<tr><td><code>TEST_FAILURE</code></td><td>Include test output, ask to fix</td></tr>\n<tr><td><code>NO_DIFF</code></td><td>\"You must change code; your previous attempt changed nothing\"</td></tr>\n<tr><td><code>TIMEOUT</code></td><td>Break down task / reduce scope</td></tr>\n<tr><td><code>MANIFEST_VIOLATION</code></td><td>Replan + retry with expanded manifest</td></tr>\n<tr><td><code>AGENT_CONFUSION</code></td><td>Involve planner or human for clarification</td></tr>\n</tbody></table>\n<h3 id=\"6-human-escalation-clear-threshold-summary\">6. Human Escalation: Clear threshold + summary<a class=\"heading-anchor\" href=\"#6-human-escalation-clear-threshold-summary\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Define a clear escalation threshold with actionable summary.</p>\n<p><strong>After N consecutive failed attempts (e.g., 3) or severe failure type:</strong></p>\n<ol>\n<li>Mark issue as <code>BLOCKED</code></li>\n<li>Post a summary:\n<ul>\n<li>What was tried</li>\n<li>Errors encountered</li>\n<li>What the agent thinks is unclear (if any)</li>\n</ul>\n</li>\n<li>Pause automation until human:\n<ul>\n<li>Adds context</li>\n<li>Edits the issue, or</li>\n<li>Manually unblocks / retries</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"7-learning-from-failures-lightweight-rule-based-lessons\">7. Learning from Failures: Lightweight rule-based lessons<a class=\"heading-anchor\" href=\"#7-learning-from-failures-lightweight-rule-based-lessons\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Start with manual rule-based \"lessons\", not ML.</p>\n<p><strong>Maintain a small config/ruleset:</strong></p>\n<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">[[failure_hints]]\n</span><span style=\"color:#323232;\">label = &quot;migration&quot;\n</span><span style=\"color:#323232;\">hint = &quot;Migration issues often need schema changes first. Check schema files.&quot;\n</span><span style=\"color:#323232;\">\n</span><span style=\"color:#323232;\">[[failure_hints]]\n</span><span style=\"color:#323232;\">language = &quot;zig&quot;\n</span><span style=\"color:#323232;\">error_pattern = &quot;error.OutOfMemory&quot;\n</span><span style=\"color:#323232;\">hint = &quot;Consider using an arena allocator for temporary allocations.&quot;\n</span></pre>\n\n</div>\n<p>Periodically review failure logs, add new rules where patterns are obvious.</p>\n<p>Later: mine logs to auto-discover patterns, but manual rules get 80% of value quickly.</p>\n<h2 id=\"implementation-notes\">Implementation Notes<a class=\"heading-anchor\" href=\"#implementation-notes\" aria-label=\"Link to heading\">#</a></h2>\n<p>See <code>src/loop.zig:runIteration</code> retry logic and <code>src/state.zig:IssueState</code>.</p>\n<h3 id=\"todo\">TODO<a class=\"heading-anchor\" href=\"#todo\" aria-label=\"Link to heading\">#</a></h3>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd failure context to retry prompts</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nImplement self-reflection step</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd breakdown agent with file-split strategy</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nImplement scratch branch model for partial progress</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd model escalation policy</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nImplement failure taxonomy enum</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd <code>BLOCKED</code> status with human escalation summary</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd <code>failure_hints</code> config section</li>\n</ul>\n","frontmatter":{"title":"Failure Recovery","description":null,"summary":null,"date":null,"type":"essay","tags":["design","failure","retry","recovery"],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":["design","failure","retry","recovery"],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":[],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#failure-recovery\">Failure Recovery</a></li><li class=\"toc-level-2\"><a href=\"#current-design\">Current Design</a></li><li class=\"toc-level-2\"><a href=\"#relation-to-survey\">Relation to Survey</a></li><li class=\"toc-level-2\"><a href=\"#design-decisions\">Design Decisions</a></li><li class=\"toc-level-3\"><a href=\"#1-retry-strategies-informed-retries-not-blind\">1. Retry Strategies: Informed retries, not blind</a></li><li class=\"toc-level-3\"><a href=\"#2-task-breakdown-minimal-concrete-decomposition\">2. Task Breakdown: Minimal, concrete decomposition</a></li><li class=\"toc-level-3\"><a href=\"#3-partial-progress-keep-on-branch-all-or-nothing-to-main\">3. Partial Progress: Keep on branch, all-or-nothing to main</a></li><li class=\"toc-level-3\"><a href=\"#4-model-escalation-simple-escalation-policy\">4. Model Escalation: Simple escalation policy</a></li><li class=\"toc-level-3\"><a href=\"#5-failure-classification-taxonomy-with-strategy-mapping\">5. Failure Classification: Taxonomy with strategy mapping</a></li><li class=\"toc-level-3\"><a href=\"#6-human-escalation-clear-threshold-summary\">6. Human Escalation: Clear threshold + summary</a></li><li class=\"toc-level-3\"><a href=\"#7-learning-from-failures-lightweight-rule-based-lessons\">7. Learning from Failures: Lightweight rule-based lessons</a></li><li class=\"toc-level-2\"><a href=\"#implementation-notes\">Implementation Notes</a></li><li class=\"toc-level-3\"><a href=\"#todo\">TODO</a></li></ul></nav>","raw_body":"# Failure Recovery\n\nHow noface handles agent failures.\n\n## Current Design\n\nnoface handles several failure modes:\n\n1. **Transient failures** (429, 5xx, network) — retry up to 3x with exponential backoff\n2. **Manifest violations** — rollback offending files, retry with stricter prompt\n3. **Timeouts** (no output for N seconds) — break down issue into smaller tasks\n4. **Crash recovery** — on startup, detect in-progress work from previous run, reset stale locks, restore state\n\nEach attempt is recorded in state with outcome (success/failed/timeout/violation).\n\n## Relation to Survey\n\nThe survey describes several recovery patterns:\n\n**Graceful retries:**\n> \"The simplest recovery pattern is retrying the same prompt. Because LLM agents are stochastic, a second attempt might yield a different (possibly correct) result.\"\n\n**Progressive prompting:**\n> \"If a direct retry doesn't help, the orchestrator should try a modified approach... break the task into smaller sub-tasks and prompt those instead.\"\n\n**Preserving partial progress:**\n> \"A robust orchestrator will preserve this partial progress so it's not lost.\"\n\n## Design Decisions\n\n### 1. Retry Strategies: Informed retries, not blind\n\n**Decision:** Retries should include failure context and self-reflection.\n\n**On retry, include:**\n- Previous attempt's diff\n- Summarized test output / error\n- Instructions:\n  ```\n  Your previous attempt failed with: [error]. Fix that specific problem\n  while preserving working parts.\n  ```\n\n**Optional self-reflection step:**\n- Ask model: \"Explain why your last attempt failed and how you will fix it\"\n- Feed that back into the actual implementation prompt\n\n### 2. Task Breakdown: Minimal, concrete decomposition\n\n**Decision:** Implement a simple breakdown strategy when issues are too large.\n\n**Trigger breakdown when:**\n- Issue times out repeatedly, or\n- Fails after N attempts with \"too big\" signature (many files, large diff)\n\n**Breakdown agent:**\n- Run planner with a different prompt to propose sub-issues:\n  - e.g., \"Update schema in A\", \"Update API in B\", \"Update tests in C\"\n- Turn these into child issues, mark parent as an \"epic\"\n\n**First version (simple):**\n- Split by file: if manifest has 5 files, create 5 issues, each scoped to one file\n\nIterate toward more semantic breakdowns later.\n\n### 3. Partial Progress: Keep on branch, all-or-nothing to main\n\n**Decision:** Preserve partial progress in scratch branches, but merge atomically.\n\n**Design:**\n- Each issue has a scratch branch or temp workspace\n- Each attempt commits to that branch\n- If one file fails, you still keep the other 3 as commits in the branch\n- Only when tests + gates pass do you merge that branch to main\n\nThis preserves partial progress for future attempts without exposing half-baked changes to main.\n\n### 4. Model Escalation: Simple escalation policy\n\n**Decision:** Define a simple model escalation policy.\n\n**For each issue:**\n- First 1–2 attempts: `default_model` (cheaper)\n- If still failing with \"correctable\" errors (tests, syntax): escalate to `strong_model`\n- Cap total attempts across all models\n\nKeep configurable so users can opt-out if they only have access to one model.\n\n```toml\n[retry]\ndefault_model = \"claude-sonnet\"\nescalation_model = \"claude-opus\"\nescalate_after_attempts = 2\nmax_total_attempts = 5\n```\n\n### 5. Failure Classification: Taxonomy with strategy mapping\n\n**Decision:** Introduce a failure taxonomy and map each to a strategy.\n\n**Taxonomy:**\n| Failure Type | Strategy |\n|--------------|----------|\n| `SYNTAX_ERROR` | Re-prompt with same context + \"fix syntax error\" |\n| `RUNTIME_ERROR` | Include stack trace, ask to fix |\n| `TEST_FAILURE` | Include test output, ask to fix |\n| `NO_DIFF` | \"You must change code; your previous attempt changed nothing\" |\n| `TIMEOUT` | Break down task / reduce scope |\n| `MANIFEST_VIOLATION` | Replan + retry with expanded manifest |\n| `AGENT_CONFUSION` | Involve planner or human for clarification |\n\n### 6. Human Escalation: Clear threshold + summary\n\n**Decision:** Define a clear escalation threshold with actionable summary.\n\n**After N consecutive failed attempts (e.g., 3) or severe failure type:**\n1. Mark issue as `BLOCKED`\n2. Post a summary:\n   - What was tried\n   - Errors encountered\n   - What the agent thinks is unclear (if any)\n3. Pause automation until human:\n   - Adds context\n   - Edits the issue, or\n   - Manually unblocks / retries\n\n### 7. Learning from Failures: Lightweight rule-based lessons\n\n**Decision:** Start with manual rule-based \"lessons\", not ML.\n\n**Maintain a small config/ruleset:**\n```toml\n[[failure_hints]]\nlabel = \"migration\"\nhint = \"Migration issues often need schema changes first. Check schema files.\"\n\n[[failure_hints]]\nlanguage = \"zig\"\nerror_pattern = \"error.OutOfMemory\"\nhint = \"Consider using an arena allocator for temporary allocations.\"\n```\n\nPeriodically review failure logs, add new rules where patterns are obvious.\n\nLater: mine logs to auto-discover patterns, but manual rules get 80% of value quickly.\n\n## Implementation Notes\n\nSee `src/loop.zig:runIteration` retry logic and `src/state.zig:IssueState`.\n\n### TODO\n- [ ] Add failure context to retry prompts\n- [ ] Implement self-reflection step\n- [ ] Add breakdown agent with file-split strategy\n- [ ] Implement scratch branch model for partial progress\n- [ ] Add model escalation policy\n- [ ] Implement failure taxonomy enum\n- [ ] Add `BLOCKED` status with human escalation summary\n- [ ] Add `failure_hints` config section\n","source_path":"design/failure-recovery.md"},{"slug":"manifests","title":"File Manifests","content_html":"<h1 id=\"file-manifests\">File Manifests<a class=\"heading-anchor\" href=\"#file-manifests\" aria-label=\"Link to heading\">#</a></h1>\n<p>How noface controls what files each agent can touch.</p>\n<h2 id=\"current-design\">Current Design<a class=\"heading-anchor\" href=\"#current-design\" aria-label=\"Link to heading\">#</a></h2>\n<p>Manifests declare three access levels per issue:</p>\n<ul>\n<li><strong>PRIMARY_FILES</strong> — exclusive write access (locked during execution)</li>\n<li><strong>READ_FILES</strong> — shared read-only access</li>\n<li><strong>FORBIDDEN_FILES</strong> — must never be touched</li>\n</ul>\n<p>The planner generates manifests by analyzing each issue. After an agent completes, noface runs <code>git diff</code> and verifies compliance. Violations trigger rollback of offending files and retry with a stricter prompt.</p>\n<h2 id=\"relation-to-survey\">Relation to Survey<a class=\"heading-anchor\" href=\"#relation-to-survey\" aria-label=\"Link to heading\">#</a></h2>\n<p>This implements the <strong>Manifest-Driven AI Development (MAID)</strong> pattern from <a href=\"/orchestration-survey.html\">orchestration-survey</a>:</p>\n<blockquote>\n<p>\"A recent methodology called Manifest-Driven AI Development (MAID) takes this further by treating the manifest as an enforceable contract: after generation, a validator checks that only the declared files/functions were altered and no undeclared edits occurred.\"</p>\n</blockquote>\n<h2 id=\"design-decisions\">Design Decisions<a class=\"heading-anchor\" href=\"#design-decisions\" aria-label=\"Link to heading\">#</a></h2>\n<h3 id=\"1-granularity-file-level-with-soft-function-hints\">1. Granularity: File-level with soft function hints<a class=\"heading-anchor\" href=\"#1-granularity-file-level-with-soft-function-hints\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Stick to file-level manifests as the hard safety boundary. Add soft function-level hints inside the prompt.</p>\n<ul>\n<li>Use the manifest as: \"these are the only files you're allowed to change\"</li>\n<li>Inside each file, include function-level \"edit targets\" in the prompt:<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">You may edit only these functions in foo.zig: [foo, bar].\n</span></pre>\n\n</div></li>\n<li>Line-ranges are brittle (they drift with edits and refactors)</li>\n<li>Function-level locking requires language-aware parsing and a robust symbol table — nice-to-have for later, not v1 safety</li>\n</ul>\n<h3 id=\"2-manifest-generation-instrumentation-replan-on-miss\">2. Manifest Generation: Instrumentation + replan on miss<a class=\"heading-anchor\" href=\"#2-manifest-generation-instrumentation-replan-on-miss\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Treat false negatives as an instrumentation problem first.</p>\n<p>Log for each issue:</p>\n<ul>\n<li><code>manifest_files_predicted</code></li>\n<li><code>files_actually_touched</code> (from diff)</li>\n</ul>\n<p>Compute:</p>\n<ul>\n<li><strong>False positives:</strong> predicted but unused (acceptable — widens safety sandbox)</li>\n<li><strong>False negatives:</strong> needed but not declared (problematic)</li>\n</ul>\n<p><strong>Short-term behavior:</strong> If the agent attempts to touch a non-manifest file:</p>\n<ol>\n<li>Reject that attempt</li>\n<li>Spawn a replan: run planner again with the new file explicitly mentioned\n<ul>\n<li>e.g. \"You also needed X.zig; update your manifest\"</li>\n</ul>\n</li>\n</ol>\n<p><strong>Goal:</strong> Tune planner + retrieval so false negatives are rare.</p>\n<h3 id=\"3-manifest-violations-never-accept-as-is\">3. Manifest Violations: Never accept as-is<a class=\"heading-anchor\" href=\"#3-manifest-violations-never-accept-as-is\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Never accept a violation directly. Use violations as hints for replanning.</p>\n<p>If the change is clearly correct:</p>\n<ol>\n<li>Record: \"Agent wanted to touch foo.zig too\"</li>\n<li>Re-run planner with that fact, generate a new manifest including foo.zig</li>\n<li>Re-run the implementation under the new manifest</li>\n<li>Only then accept</li>\n</ol>\n<p>For interactive mode, offer:</p>\n<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">Agent touched out-of-manifest file X, the change looks good.\n</span><span style=\"color:#323232;\">➜ [Accept &amp; expand manifest] / [Re-run] / [Discard]\n</span></pre>\n\n</div>\n<p>The automation path should always have a manifest-consistent attempt before merging.</p>\n<h3 id=\"4-dynamic-expansion-yes-via-orchestrator-mediated-manifest-v2\">4. Dynamic Expansion: Yes, via orchestrator-mediated \"manifest v2\"<a class=\"heading-anchor\" href=\"#4-dynamic-expansion-yes-via-orchestrator-mediated-manifest-v2\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Allow agents to request additional files mid-run, but only through explicit orchestrator coordination with locking rules applied.</p>\n<p><strong>Flow:</strong></p>\n<ol>\n<li>Agent hits missing symbol/doc and outputs:<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">NEED_FILE: src/auth.zig\n</span></pre>\n\n</div>or<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">NEED_DOC: design/auth.md\n</span></pre>\n\n</div></li>\n<li>Orchestrator checks locks:\n<ul>\n<li>If <code>auth.zig</code> is free:\n<ul>\n<li>Acquire lock</li>\n<li>Produce manifest v2 including <code>auth.zig</code></li>\n<li>Re-prompt the agent with expanded manifest and new file content</li>\n</ul>\n</li>\n<li>If locked by another task:\n<ul>\n<li>Tell agent: \"You cannot access auth.zig because it's locked; proceed without or wait\"</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p>This preserves the isolation model:</p>\n<ul>\n<li>No silent expansion</li>\n<li>All expansions are explicit, logged, and coordinated with concurrency control</li>\n</ul>\n<h2 id=\"implementation-notes\">Implementation Notes<a class=\"heading-anchor\" href=\"#implementation-notes\" aria-label=\"Link to heading\">#</a></h2>\n<p>See <code>src/state.zig:Manifest</code> and <code>src/loop.zig:verifyManifestCompliance</code>.</p>\n<h3 id=\"todo\">TODO<a class=\"heading-anchor\" href=\"#todo\" aria-label=\"Link to heading\">#</a></h3>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd manifest instrumentation (predicted vs actual files)</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nImplement replan-on-violation flow</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd <code>NEED_FILE</code> / <code>NEED_DOC</code> signal parsing</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd function-level hints to prompt builder</li>\n</ul>\n","frontmatter":{"title":"File Manifests","description":null,"summary":null,"date":null,"type":"essay","tags":["design","manifests","access-control"],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":["design","manifests","access-control"],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":["orchestration-survey"],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#file-manifests\">File Manifests</a></li><li class=\"toc-level-2\"><a href=\"#current-design\">Current Design</a></li><li class=\"toc-level-2\"><a href=\"#relation-to-survey\">Relation to Survey</a></li><li class=\"toc-level-2\"><a href=\"#design-decisions\">Design Decisions</a></li><li class=\"toc-level-3\"><a href=\"#1-granularity-file-level-with-soft-function-hints\">1. Granularity: File-level with soft function hints</a></li><li class=\"toc-level-3\"><a href=\"#2-manifest-generation-instrumentation-replan-on-miss\">2. Manifest Generation: Instrumentation + replan on miss</a></li><li class=\"toc-level-3\"><a href=\"#3-manifest-violations-never-accept-as-is\">3. Manifest Violations: Never accept as-is</a></li><li class=\"toc-level-3\"><a href=\"#4-dynamic-expansion-yes-via-orchestrator-mediated-manifest-v2\">4. Dynamic Expansion: Yes, via orchestrator-mediated &quot;manifest v2&quot;</a></li><li class=\"toc-level-2\"><a href=\"#implementation-notes\">Implementation Notes</a></li><li class=\"toc-level-3\"><a href=\"#todo\">TODO</a></li></ul></nav>","raw_body":"# File Manifests\n\nHow noface controls what files each agent can touch.\n\n## Current Design\n\nManifests declare three access levels per issue:\n\n- **PRIMARY_FILES** — exclusive write access (locked during execution)\n- **READ_FILES** — shared read-only access\n- **FORBIDDEN_FILES** — must never be touched\n\nThe planner generates manifests by analyzing each issue. After an agent completes, noface runs `git diff` and verifies compliance. Violations trigger rollback of offending files and retry with a stricter prompt.\n\n## Relation to Survey\n\nThis implements the **Manifest-Driven AI Development (MAID)** pattern from [[orchestration-survey]]:\n\n> \"A recent methodology called Manifest-Driven AI Development (MAID) takes this further by treating the manifest as an enforceable contract: after generation, a validator checks that only the declared files/functions were altered and no undeclared edits occurred.\"\n\n## Design Decisions\n\n### 1. Granularity: File-level with soft function hints\n\n**Decision:** Stick to file-level manifests as the hard safety boundary. Add soft function-level hints inside the prompt.\n\n- Use the manifest as: \"these are the only files you're allowed to change\"\n- Inside each file, include function-level \"edit targets\" in the prompt:\n  ```\n  You may edit only these functions in foo.zig: [foo, bar].\n  ```\n- Line-ranges are brittle (they drift with edits and refactors)\n- Function-level locking requires language-aware parsing and a robust symbol table — nice-to-have for later, not v1 safety\n\n### 2. Manifest Generation: Instrumentation + replan on miss\n\n**Decision:** Treat false negatives as an instrumentation problem first.\n\nLog for each issue:\n- `manifest_files_predicted`\n- `files_actually_touched` (from diff)\n\nCompute:\n- **False positives:** predicted but unused (acceptable — widens safety sandbox)\n- **False negatives:** needed but not declared (problematic)\n\n**Short-term behavior:** If the agent attempts to touch a non-manifest file:\n1. Reject that attempt\n2. Spawn a replan: run planner again with the new file explicitly mentioned\n   - e.g. \"You also needed X.zig; update your manifest\"\n\n**Goal:** Tune planner + retrieval so false negatives are rare.\n\n### 3. Manifest Violations: Never accept as-is\n\n**Decision:** Never accept a violation directly. Use violations as hints for replanning.\n\nIf the change is clearly correct:\n1. Record: \"Agent wanted to touch foo.zig too\"\n2. Re-run planner with that fact, generate a new manifest including foo.zig\n3. Re-run the implementation under the new manifest\n4. Only then accept\n\nFor interactive mode, offer:\n```\nAgent touched out-of-manifest file X, the change looks good.\n➜ [Accept & expand manifest] / [Re-run] / [Discard]\n```\n\nThe automation path should always have a manifest-consistent attempt before merging.\n\n### 4. Dynamic Expansion: Yes, via orchestrator-mediated \"manifest v2\"\n\n**Decision:** Allow agents to request additional files mid-run, but only through explicit orchestrator coordination with locking rules applied.\n\n**Flow:**\n1. Agent hits missing symbol/doc and outputs:\n   ```\n   NEED_FILE: src/auth.zig\n   ```\n   or\n   ```\n   NEED_DOC: design/auth.md\n   ```\n2. Orchestrator checks locks:\n   - If `auth.zig` is free:\n     - Acquire lock\n     - Produce manifest v2 including `auth.zig`\n     - Re-prompt the agent with expanded manifest and new file content\n   - If locked by another task:\n     - Tell agent: \"You cannot access auth.zig because it's locked; proceed without or wait\"\n\nThis preserves the isolation model:\n- No silent expansion\n- All expansions are explicit, logged, and coordinated with concurrency control\n\n## Implementation Notes\n\nSee `src/state.zig:Manifest` and `src/loop.zig:verifyManifestCompliance`.\n\n### TODO\n- [ ] Add manifest instrumentation (predicted vs actual files)\n- [ ] Implement replan-on-violation flow\n- [ ] Add `NEED_FILE` / `NEED_DOC` signal parsing\n- [ ] Add function-level hints to prompt builder\n","source_path":"design/manifests.md"},{"slug":"verification","title":"Verification","content_html":"<h1 id=\"verification\">Verification<a class=\"heading-anchor\" href=\"#verification\" aria-label=\"Link to heading\">#</a></h1>\n<p>How noface determines whether an agent succeeded.</p>\n<h2 id=\"current-design\">Current Design<a class=\"heading-anchor\" href=\"#current-design\" aria-label=\"Link to heading\">#</a></h2>\n<p>noface uses multiple verification layers:</p>\n<ol>\n<li><strong>Test execution</strong> — runs the configured test command; failure = not done</li>\n<li><strong>Manifest compliance</strong> — <code>git diff</code> checked against declared files; violations = rollback</li>\n<li><strong>Build check</strong> — runs the configured build command (implicit in agent workflow)</li>\n</ol>\n<p>The agent is instructed to self-verify (run tests, check output) before committing.</p>\n<h2 id=\"relation-to-survey\">Relation to Survey<a class=\"heading-anchor\" href=\"#relation-to-survey\" aria-label=\"Link to heading\">#</a></h2>\n<p>The survey emphasizes <strong>automated testing as ground truth</strong>:</p>\n<blockquote>\n<p>\"This ensures that code isn't accepted as 'done' until it passes its tests. Even single-agent approaches like OpenAI's Codex have employed this idea (often called execute-and-fix): run the code, and if an error or failing test is detected, prompt the model to fix it.\"</p>\n</blockquote>\n<p>And <strong>manifest verification</strong>:</p>\n<blockquote>\n<p>\"Tools like MAID runner perform static analysis on the diff: did the agent only modify the allowed files and functions?\"</p>\n</blockquote>\n<p>The survey also discusses <strong>LLM critics</strong> as an additional layer — a second agent that reviews the code.</p>\n<h2 id=\"design-decisions\">Design Decisions<a class=\"heading-anchor\" href=\"#design-decisions\" aria-label=\"Link to heading\">#</a></h2>\n<h3 id=\"1-test-coverage-require-tests-for-new-behavior\">1. Test Coverage: Require tests for new behavior<a class=\"heading-anchor\" href=\"#1-test-coverage-require-tests-for-new-behavior\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Add test-centric enhancements for changes that add new behavior.</p>\n<p><strong>For changes that add new behavior:</strong></p>\n<ul>\n<li>Ask agent to write or update tests as part of the task</li>\n<li>Optionally run coverage diff if coverage tool exists:\n<ul>\n<li>If new/changed lines have zero coverage → soft or hard gate</li>\n</ul>\n</li>\n</ul>\n<p><strong>Where tooling is limited:</strong></p>\n<ul>\n<li>At least ensure: \"If tests exist in this module, check that they were updated\"</li>\n<li>Warn if tests not updated for behavioral changes</li>\n</ul>\n<h3 id=\"2-review-pass-optional-but-recommended-for-non-trivial-changes\">2. Review Pass: Optional but recommended for non-trivial changes<a class=\"heading-anchor\" href=\"#2-review-pass-optional-but-recommended-for-non-trivial-changes\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Add dedicated reviewer pass as an optional gate for non-trivial changes.</p>\n<p><strong>Reviewer inputs:</strong></p>\n<ul>\n<li>Issue description</li>\n<li>Old code vs new code diff</li>\n<li>Test results</li>\n</ul>\n<p><strong>Reviewer outputs:</strong></p>\n<ul>\n<li>Verdict: <code>OK</code> / <code>NOT_OK</code> / <code>NEEDS_HUMAN</code></li>\n<li>Specific comments</li>\n</ul>\n<p><strong>Trigger heuristics:</strong></p>\n<table><thead><tr><th>Condition</th><th>Action</th></tr></thead><tbody>\n<tr><td>Large diffs</td><td>Always review</td></tr>\n<tr><td>High-risk directories</td><td>Always review</td></tr>\n<tr><td>Changes without tests</td><td>Always review</td></tr>\n<tr><td>Small, well-tested changes</td><td>Skip or downgrade</td></tr>\n</tbody></table>\n<h3 id=\"3-static-analysis-integrate-repos-existing-tools\">3. Static Analysis: Integrate repo's existing tools<a class=\"heading-anchor\" href=\"#3-static-analysis-integrate-repos-existing-tools\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Integrate whatever static analysis the repo already has.</p>\n<p><strong>Hard gates (block on failure):</strong></p>\n<ul>\n<li>Type errors (<code>zig build</code>, <code>tsc</code>, <code>mypy</code>)</li>\n<li>Formatter failures (<code>zig fmt</code>, <code>prettier</code>)</li>\n</ul>\n<p><strong>Soft gates (log, maybe create follow-up):</strong></p>\n<ul>\n<li>New lint warnings</li>\n<li>Security scanner findings (unless critical)</li>\n</ul>\n<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">[verification.static_analysis]\n</span><span style=\"color:#323232;\">hard_gates = [&quot;zig build&quot;, &quot;zig fmt --check&quot;]\n</span><span style=\"color:#323232;\">soft_gates = [&quot;clippy&quot;, &quot;eslint&quot;]\n</span></pre>\n\n</div>\n<h3 id=\"4-semantic-verification-judge-agent-for-complex-issues\">4. Semantic Verification: Judge agent for complex issues<a class=\"heading-anchor\" href=\"#4-semantic-verification-judge-agent-for-complex-issues\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Add semantic check step for complex issues.</p>\n<p><strong>Option 1: Explicit reasoning</strong></p>\n<ul>\n<li>Have implementer/reviewer write: \"Here is how the change addresses the issue…\"</li>\n<li>Check coherence between explanation and diff</li>\n</ul>\n<p><strong>Option 2: Judge agent</strong></p>\n<ul>\n<li>Input: issue description + old code + new code</li>\n<li>Question: \"Does this change resolve the described behavior? Is anything missing or unrelated?\"</li>\n</ul>\n<p>Doesn't need to be perfect; even catching obvious mismatches is a big win.</p>\n<h3 id=\"5-confidence-signals-explicit-confidence-risk-metadata\">5. Confidence Signals: Explicit confidence + risk metadata<a class=\"heading-anchor\" href=\"#5-confidence-signals-explicit-confidence-risk-metadata\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Ask agent to output explicit confidence and risks.</p>\n<p><strong>Prompt addition:</strong></p>\n<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">After implementing, output:\n</span><span style=\"color:#323232;\">CONFIDENCE: X/5\n</span><span style=\"color:#323232;\">RISKS:\n</span><span style=\"color:#323232;\">- [list any edge cases or uncertainties]\n</span></pre>\n\n</div>\n<p><strong>Policy:</strong></p>\n<ul>\n<li>If confidence ≤ 2/5 → require reviewer + maybe human</li>\n<li>Also watch for heuristics:\n<ul>\n<li>Lots of TODOs in output</li>\n<li>\"I think / maybe\" in comments</li>\n<li>Weirdly small or huge diffs</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"6-partial-acceptance-hard-vs-soft-gates\">6. Partial Acceptance: Hard vs soft gates<a class=\"heading-anchor\" href=\"#6-partial-acceptance-hard-vs-soft-gates\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Define hard vs soft gates; never partially merge.</p>\n<p><strong>Hard gates (must pass for merge):</strong></p>\n<ul>\n<li>Tests passing</li>\n<li>No syntax/build errors</li>\n<li>Manifest compliance</li>\n</ul>\n<p><strong>Soft gates (can proceed with warnings):</strong></p>\n<ul>\n<li>Lint warnings</li>\n<li>Coverage thresholds</li>\n<li>Reviewer \"nit\" comments</li>\n</ul>\n<p><strong>Policy:</strong></p>\n<ul>\n<li>Automated merge requires all hard gates</li>\n<li>Soft gate failures:\n<ul>\n<li>Either block and open follow-up issue, or</li>\n<li>Allow merge but log warnings and create cleanup issues</li>\n</ul>\n</li>\n</ul>\n<p>Avoid partial merges of file subsets; use the branch model from <a href=\"/failure-recovery.html\">failure-recovery</a> instead.</p>\n<h3 id=\"7-human-review-gate-risk-classification-for-high-risk-changes\">7. Human Review Gate: Risk classification for high-risk changes<a class=\"heading-anchor\" href=\"#7-human-review-gate-risk-classification-for-high-risk-changes\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Build risk classification with hard human gate for sensitive areas.</p>\n<p><strong>High-risk triggers:</strong></p>\n<ul>\n<li>Files/directories: <code>auth/</code>, <code>payments/</code>, <code>secrets/</code>, <code>infra/</code>, <code>prod-config/</code></li>\n<li>Labels: <code>security</code>, <code>compliance</code>, <code>breaking-change</code></li>\n</ul>\n<p><strong>For high-risk changes:</strong></p>\n<ul>\n<li>noface never auto-merges</li>\n<li>Opens PR or surfaces diff with \"requires human approval\" flag</li>\n<li>Optionally pre-annotated with AI reviewer's comments</li>\n</ul>\n<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">[verification.human_required]\n</span><span style=\"color:#323232;\">paths = [&quot;src/auth/&quot;, &quot;src/payments/&quot;, &quot;config/prod/&quot;]\n</span><span style=\"color:#323232;\">labels = [&quot;security&quot;, &quot;compliance&quot;]\n</span></pre>\n\n</div>\n<h2 id=\"implementation-notes\">Implementation Notes<a class=\"heading-anchor\" href=\"#implementation-notes\" aria-label=\"Link to heading\">#</a></h2>\n<p>See <code>src/loop.zig:verifyManifestCompliance</code> and prompt instructions for self-testing.</p>\n<h3 id=\"todo\">TODO<a class=\"heading-anchor\" href=\"#todo\" aria-label=\"Link to heading\">#</a></h3>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd test coverage checking (if coverage tool available)</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nImplement reviewer pass with risk-based triggering</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd static analysis integration (hard/soft gates)</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd judge agent for semantic verification</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nParse confidence/risk metadata from agent output</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd human review gate for high-risk paths</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nImplement soft gate → follow-up issue creation</li>\n</ul>\n","frontmatter":{"title":"Verification","description":null,"summary":null,"date":null,"type":"essay","tags":["design","verification","testing","acceptance"],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":["design","verification","testing","acceptance"],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":["failure-recovery"],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#verification\">Verification</a></li><li class=\"toc-level-2\"><a href=\"#current-design\">Current Design</a></li><li class=\"toc-level-2\"><a href=\"#relation-to-survey\">Relation to Survey</a></li><li class=\"toc-level-2\"><a href=\"#design-decisions\">Design Decisions</a></li><li class=\"toc-level-3\"><a href=\"#1-test-coverage-require-tests-for-new-behavior\">1. Test Coverage: Require tests for new behavior</a></li><li class=\"toc-level-3\"><a href=\"#2-review-pass-optional-but-recommended-for-non-trivial-changes\">2. Review Pass: Optional but recommended for non-trivial changes</a></li><li class=\"toc-level-3\"><a href=\"#3-static-analysis-integrate-repos-existing-tools\">3. Static Analysis: Integrate repo&#39;s existing tools</a></li><li class=\"toc-level-3\"><a href=\"#4-semantic-verification-judge-agent-for-complex-issues\">4. Semantic Verification: Judge agent for complex issues</a></li><li class=\"toc-level-3\"><a href=\"#5-confidence-signals-explicit-confidence-risk-metadata\">5. Confidence Signals: Explicit confidence + risk metadata</a></li><li class=\"toc-level-3\"><a href=\"#6-partial-acceptance-hard-vs-soft-gates\">6. Partial Acceptance: Hard vs soft gates</a></li><li class=\"toc-level-3\"><a href=\"#7-human-review-gate-risk-classification-for-high-risk-changes\">7. Human Review Gate: Risk classification for high-risk changes</a></li><li class=\"toc-level-2\"><a href=\"#implementation-notes\">Implementation Notes</a></li><li class=\"toc-level-3\"><a href=\"#todo\">TODO</a></li></ul></nav>","raw_body":"# Verification\n\nHow noface determines whether an agent succeeded.\n\n## Current Design\n\nnoface uses multiple verification layers:\n\n1. **Test execution** — runs the configured test command; failure = not done\n2. **Manifest compliance** — `git diff` checked against declared files; violations = rollback\n3. **Build check** — runs the configured build command (implicit in agent workflow)\n\nThe agent is instructed to self-verify (run tests, check output) before committing.\n\n## Relation to Survey\n\nThe survey emphasizes **automated testing as ground truth**:\n\n> \"This ensures that code isn't accepted as 'done' until it passes its tests. Even single-agent approaches like OpenAI's Codex have employed this idea (often called execute-and-fix): run the code, and if an error or failing test is detected, prompt the model to fix it.\"\n\nAnd **manifest verification**:\n\n> \"Tools like MAID runner perform static analysis on the diff: did the agent only modify the allowed files and functions?\"\n\nThe survey also discusses **LLM critics** as an additional layer — a second agent that reviews the code.\n\n## Design Decisions\n\n### 1. Test Coverage: Require tests for new behavior\n\n**Decision:** Add test-centric enhancements for changes that add new behavior.\n\n**For changes that add new behavior:**\n- Ask agent to write or update tests as part of the task\n- Optionally run coverage diff if coverage tool exists:\n  - If new/changed lines have zero coverage → soft or hard gate\n\n**Where tooling is limited:**\n- At least ensure: \"If tests exist in this module, check that they were updated\"\n- Warn if tests not updated for behavioral changes\n\n### 2. Review Pass: Optional but recommended for non-trivial changes\n\n**Decision:** Add dedicated reviewer pass as an optional gate for non-trivial changes.\n\n**Reviewer inputs:**\n- Issue description\n- Old code vs new code diff\n- Test results\n\n**Reviewer outputs:**\n- Verdict: `OK` / `NOT_OK` / `NEEDS_HUMAN`\n- Specific comments\n\n**Trigger heuristics:**\n| Condition | Action |\n|-----------|--------|\n| Large diffs | Always review |\n| High-risk directories | Always review |\n| Changes without tests | Always review |\n| Small, well-tested changes | Skip or downgrade |\n\n### 3. Static Analysis: Integrate repo's existing tools\n\n**Decision:** Integrate whatever static analysis the repo already has.\n\n**Hard gates (block on failure):**\n- Type errors (`zig build`, `tsc`, `mypy`)\n- Formatter failures (`zig fmt`, `prettier`)\n\n**Soft gates (log, maybe create follow-up):**\n- New lint warnings\n- Security scanner findings (unless critical)\n\n```toml\n[verification.static_analysis]\nhard_gates = [\"zig build\", \"zig fmt --check\"]\nsoft_gates = [\"clippy\", \"eslint\"]\n```\n\n### 4. Semantic Verification: Judge agent for complex issues\n\n**Decision:** Add semantic check step for complex issues.\n\n**Option 1: Explicit reasoning**\n- Have implementer/reviewer write: \"Here is how the change addresses the issue…\"\n- Check coherence between explanation and diff\n\n**Option 2: Judge agent**\n- Input: issue description + old code + new code\n- Question: \"Does this change resolve the described behavior? Is anything missing or unrelated?\"\n\nDoesn't need to be perfect; even catching obvious mismatches is a big win.\n\n### 5. Confidence Signals: Explicit confidence + risk metadata\n\n**Decision:** Ask agent to output explicit confidence and risks.\n\n**Prompt addition:**\n```\nAfter implementing, output:\nCONFIDENCE: X/5\nRISKS:\n- [list any edge cases or uncertainties]\n```\n\n**Policy:**\n- If confidence ≤ 2/5 → require reviewer + maybe human\n- Also watch for heuristics:\n  - Lots of TODOs in output\n  - \"I think / maybe\" in comments\n  - Weirdly small or huge diffs\n\n### 6. Partial Acceptance: Hard vs soft gates\n\n**Decision:** Define hard vs soft gates; never partially merge.\n\n**Hard gates (must pass for merge):**\n- Tests passing\n- No syntax/build errors\n- Manifest compliance\n\n**Soft gates (can proceed with warnings):**\n- Lint warnings\n- Coverage thresholds\n- Reviewer \"nit\" comments\n\n**Policy:**\n- Automated merge requires all hard gates\n- Soft gate failures:\n  - Either block and open follow-up issue, or\n  - Allow merge but log warnings and create cleanup issues\n\nAvoid partial merges of file subsets; use the branch model from [[failure-recovery]] instead.\n\n### 7. Human Review Gate: Risk classification for high-risk changes\n\n**Decision:** Build risk classification with hard human gate for sensitive areas.\n\n**High-risk triggers:**\n- Files/directories: `auth/`, `payments/`, `secrets/`, `infra/`, `prod-config/`\n- Labels: `security`, `compliance`, `breaking-change`\n\n**For high-risk changes:**\n- noface never auto-merges\n- Opens PR or surfaces diff with \"requires human approval\" flag\n- Optionally pre-annotated with AI reviewer's comments\n\n```toml\n[verification.human_required]\npaths = [\"src/auth/\", \"src/payments/\", \"config/prod/\"]\nlabels = [\"security\", \"compliance\"]\n```\n\n## Implementation Notes\n\nSee `src/loop.zig:verifyManifestCompliance` and prompt instructions for self-testing.\n\n### TODO\n- [ ] Add test coverage checking (if coverage tool available)\n- [ ] Implement reviewer pass with risk-based triggering\n- [ ] Add static analysis integration (hard/soft gates)\n- [ ] Add judge agent for semantic verification\n- [ ] Parse confidence/risk metadata from agent output\n- [ ] Add human review gate for high-risk paths\n- [ ] Implement soft gate → follow-up issue creation\n","source_path":"design/verification.md"},{"slug":"multi-pass","title":"Multi-Pass Architecture","content_html":"<h1 id=\"multi-pass-architecture\">Multi-Pass Architecture<a class=\"heading-anchor\" href=\"#multi-pass-architecture\" aria-label=\"Link to heading\">#</a></h1>\n<p>How noface uses multiple agent passes to improve quality.</p>\n<h2 id=\"current-design\">Current Design<a class=\"heading-anchor\" href=\"#current-design\" aria-label=\"Link to heading\">#</a></h2>\n<p>noface runs three types of passes:</p>\n<ol>\n<li>\n<p><strong>Planner pass</strong> (Codex) — every N iterations</p>\n<ul>\n<li>Analyzes issue backlog</li>\n<li>Generates file manifests for each issue</li>\n<li>Groups non-conflicting issues into parallel batches</li>\n</ul>\n</li>\n<li>\n<p><strong>Implementation pass</strong> (Claude) — per issue</p>\n<ul>\n<li>Receives issue + manifest + context</li>\n<li>Implements the change, runs tests, commits</li>\n</ul>\n</li>\n<li>\n<p><strong>Quality pass</strong> (Codex) — every M iterations</p>\n<ul>\n<li>Scans codebase for tech debt</li>\n<li>Creates new issues for findings</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"relation-to-survey\">Relation to Survey<a class=\"heading-anchor\" href=\"#relation-to-survey\" aria-label=\"Link to heading\">#</a></h2>\n<p>This follows the <strong>Planner → Implementer</strong> pattern from <a href=\"/orchestration-survey.html\">orchestration-survey</a>:</p>\n<blockquote>\n<p>\"The self-planning study by Jiang et al. (2025) exemplifies this: the LLM generates concise, structured planning steps from the intent, then in the implementation phase it 'generates code step by step, guided by the preceding planning steps'. This two-pass approach yielded significantly higher correctness than a single-pass solution.\"</p>\n</blockquote>\n<p>The survey also discusses <strong>Reviewer passes</strong> for catching mistakes. noface's quality pass is similar but focuses on proactive debt detection rather than reviewing specific changes.</p>\n<h2 id=\"design-decisions\">Design Decisions<a class=\"heading-anchor\" href=\"#design-decisions\" aria-label=\"Link to heading\">#</a></h2>\n<h3 id=\"1-pass-intervals-event-driven-adaptive-not-fixed\">1. Pass Intervals: Event-driven + adaptive (not fixed)<a class=\"heading-anchor\" href=\"#1-pass-intervals-event-driven-adaptive-not-fixed\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Move from fixed intervals to event-driven + adaptive.</p>\n<p><strong>Planner:</strong></p>\n<ul>\n<li>Run on-demand for new issues (always)</li>\n<li>Run periodically when:\n<ul>\n<li>A manifest violation occurs (planner \"missed\" a file)</li>\n<li>Large codebase change merged (e.g., big refactor)</li>\n</ul>\n</li>\n</ul>\n<p><strong>Quality:</strong></p>\n<ul>\n<li>Run when:\n<ul>\n<li>A batch of N issues is completed</li>\n<li>A spike in failures or bug reports is observed</li>\n</ul>\n</li>\n</ul>\n<p>Keep <code>planner_interval</code> and <code>quality_interval</code> as fallback periodic jobs, but teach orchestrator to invoke them in response to signals.</p>\n<h3 id=\"2-reviewer-pass-yes-for-high-risk-changes\">2. Reviewer Pass: Yes, for high-risk changes<a class=\"heading-anchor\" href=\"#2-reviewer-pass-yes-for-high-risk-changes\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Add a dedicated reviewer pass, but only for certain classes of changes.</p>\n<p><strong>Trigger reviewer when:</strong></p>\n<ul>\n<li>Diff size &gt; threshold</li>\n<li>Changed files in <code>security/</code>, <code>auth/</code>, <code>infra/</code>, <code>config/</code></li>\n<li>No tests exist or coverage is low</li>\n</ul>\n<p><strong>For low-risk, tiny changes where tests are strong:</strong></p>\n<ul>\n<li>Tests + manifest is probably enough; skip reviewer to save cost</li>\n</ul>\n<h3 id=\"3-feedback-loops-quality-findings-feed-back-to-implementer\">3. Feedback Loops: Quality findings feed back to implementer<a class=\"heading-anchor\" href=\"#3-feedback-loops-quality-findings-feed-back-to-implementer\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Quality findings should feed back directly to next implementation attempt.</p>\n<p><strong>Implementation:</strong></p>\n<ul>\n<li>When quality pass opens \"follow-up issues\", attach:\n<ul>\n<li>Links to the original issue</li>\n<li>The quality agent's analysis (e.g., \"didn't handle null case in X\")</li>\n</ul>\n</li>\n<li>When implementer picks up follow-up issue, include analysis in prompt:<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">The previous attempt failed because: [list]. Fix these specific problems.\n</span></pre>\n\n</div></li>\n</ul>\n<p>This turns the quality pass into a teacher, not just a bug generator.</p>\n<h3 id=\"4-pass-ordering-planner-on-demand-per-issue-not-just-periodic\">4. Pass Ordering: Planner on-demand per-issue (not just periodic)<a class=\"heading-anchor\" href=\"#4-pass-ordering-planner-on-demand-per-issue-not-just-periodic\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Run planner on-demand for manifests, keep periodic for backlog management.</p>\n<p><strong>Flow:</strong></p>\n<ol>\n<li>New issue arrives</li>\n<li>Planner runs, generates manifest + hints</li>\n<li>Implementation runs</li>\n</ol>\n<p><strong>Periodic planner still exists for:</strong></p>\n<ul>\n<li>Rebalancing / reprioritizing backlog</li>\n<li>Suggesting new refactor issues</li>\n</ul>\n<p>Relying only on periodic planner is where \"stale manifests\" come from.</p>\n<h3 id=\"5-diminishing-returns-data-driven-pass-value-metrics\">5. Diminishing Returns: Data-driven pass value metrics<a class=\"heading-anchor\" href=\"#5-diminishing-returns-data-driven-pass-value-metrics\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Log per-pass value metrics and make decisions data-driven.</p>\n<p><strong>For each pass type (planner, reviewer, quality), count:</strong></p>\n<ul>\n<li>How often it changes outcome:\n<ul>\n<li>Reviewer finds a bug → implementation corrected</li>\n<li>Quality pass yields issues not caught otherwise</li>\n</ul>\n</li>\n</ul>\n<p><strong>Track:</strong></p>\n<ul>\n<li>Average tokens / time per pass</li>\n<li>Compute: \"Bugs caught per thousand tokens\" or \"per minute\"</li>\n</ul>\n<p>If reviewer catches issues in 1/50 changes but costs a lot, restrict it to riskier paths.</p>\n<h3 id=\"6-model-selection-abstract-behind-roles-default-to-strong-model\">6. Model Selection: Abstract behind roles, default to strong model<a class=\"heading-anchor\" href=\"#6-model-selection-abstract-behind-roles-default-to-strong-model\" aria-label=\"Link to heading\">#</a></h3>\n<p><strong>Decision:</strong> Short term, keep it simple and unify models where reasoning matters.</p>\n<p><strong>Design:</strong></p>\n<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">[models]\n</span><span style=\"color:#323232;\">planner = &quot;claude&quot;       # strong model for planning\n</span><span style=\"color:#323232;\">implementer = &quot;claude&quot;   # strong model for implementation\n</span><span style=\"color:#323232;\">reviewer = &quot;claude&quot;      # can use cheaper if cost-sensitive\n</span><span style=\"color:#323232;\">quality = &quot;codex&quot;        # can use cheaper for triage\n</span></pre>\n\n</div>\n<p><strong>Rationale:</strong></p>\n<ul>\n<li>Planner quality matters a lot; using same strong model as implementer often improves plans</li>\n<li>Quality/triage can often use cheaper model if cost is a concern</li>\n</ul>\n<p>Make it easy to A/B test different combinations.</p>\n<h2 id=\"implementation-notes\">Implementation Notes<a class=\"heading-anchor\" href=\"#implementation-notes\" aria-label=\"Link to heading\">#</a></h2>\n<p>See <code>src/loop.zig:runPlannerPass</code>, <code>runQualityPass</code>, <code>runIteration</code>.</p>\n<h3 id=\"todo\">TODO<a class=\"heading-anchor\" href=\"#todo\" aria-label=\"Link to heading\">#</a></h3>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"/>\nImplement on-demand planner trigger for new issues</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd reviewer pass with risk-based triggering</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAttach quality findings to follow-up issues</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAdd per-pass metrics logging</li>\n<li><input disabled=\"\" type=\"checkbox\"/>\nAbstract model selection behind role config</li>\n</ul>\n","frontmatter":{"title":"Multi-Pass Architecture","description":null,"summary":null,"date":null,"type":"essay","tags":["design","planner","reviewer","passes"],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":["design","planner","reviewer","passes"],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":["orchestration-survey"],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#multi-pass-architecture\">Multi-Pass Architecture</a></li><li class=\"toc-level-2\"><a href=\"#current-design\">Current Design</a></li><li class=\"toc-level-2\"><a href=\"#relation-to-survey\">Relation to Survey</a></li><li class=\"toc-level-2\"><a href=\"#design-decisions\">Design Decisions</a></li><li class=\"toc-level-3\"><a href=\"#1-pass-intervals-event-driven-adaptive-not-fixed\">1. Pass Intervals: Event-driven + adaptive (not fixed)</a></li><li class=\"toc-level-3\"><a href=\"#2-reviewer-pass-yes-for-high-risk-changes\">2. Reviewer Pass: Yes, for high-risk changes</a></li><li class=\"toc-level-3\"><a href=\"#3-feedback-loops-quality-findings-feed-back-to-implementer\">3. Feedback Loops: Quality findings feed back to implementer</a></li><li class=\"toc-level-3\"><a href=\"#4-pass-ordering-planner-on-demand-per-issue-not-just-periodic\">4. Pass Ordering: Planner on-demand per-issue (not just periodic)</a></li><li class=\"toc-level-3\"><a href=\"#5-diminishing-returns-data-driven-pass-value-metrics\">5. Diminishing Returns: Data-driven pass value metrics</a></li><li class=\"toc-level-3\"><a href=\"#6-model-selection-abstract-behind-roles-default-to-strong-model\">6. Model Selection: Abstract behind roles, default to strong model</a></li><li class=\"toc-level-2\"><a href=\"#implementation-notes\">Implementation Notes</a></li><li class=\"toc-level-3\"><a href=\"#todo\">TODO</a></li></ul></nav>","raw_body":"# Multi-Pass Architecture\n\nHow noface uses multiple agent passes to improve quality.\n\n## Current Design\n\nnoface runs three types of passes:\n\n1. **Planner pass** (Codex) — every N iterations\n   - Analyzes issue backlog\n   - Generates file manifests for each issue\n   - Groups non-conflicting issues into parallel batches\n\n2. **Implementation pass** (Claude) — per issue\n   - Receives issue + manifest + context\n   - Implements the change, runs tests, commits\n\n3. **Quality pass** (Codex) — every M iterations\n   - Scans codebase for tech debt\n   - Creates new issues for findings\n\n## Relation to Survey\n\nThis follows the **Planner → Implementer** pattern from [[orchestration-survey]]:\n\n> \"The self-planning study by Jiang et al. (2025) exemplifies this: the LLM generates concise, structured planning steps from the intent, then in the implementation phase it 'generates code step by step, guided by the preceding planning steps'. This two-pass approach yielded significantly higher correctness than a single-pass solution.\"\n\nThe survey also discusses **Reviewer passes** for catching mistakes. noface's quality pass is similar but focuses on proactive debt detection rather than reviewing specific changes.\n\n## Design Decisions\n\n### 1. Pass Intervals: Event-driven + adaptive (not fixed)\n\n**Decision:** Move from fixed intervals to event-driven + adaptive.\n\n**Planner:**\n- Run on-demand for new issues (always)\n- Run periodically when:\n  - A manifest violation occurs (planner \"missed\" a file)\n  - Large codebase change merged (e.g., big refactor)\n\n**Quality:**\n- Run when:\n  - A batch of N issues is completed\n  - A spike in failures or bug reports is observed\n\nKeep `planner_interval` and `quality_interval` as fallback periodic jobs, but teach orchestrator to invoke them in response to signals.\n\n### 2. Reviewer Pass: Yes, for high-risk changes\n\n**Decision:** Add a dedicated reviewer pass, but only for certain classes of changes.\n\n**Trigger reviewer when:**\n- Diff size > threshold\n- Changed files in `security/`, `auth/`, `infra/`, `config/`\n- No tests exist or coverage is low\n\n**For low-risk, tiny changes where tests are strong:**\n- Tests + manifest is probably enough; skip reviewer to save cost\n\n### 3. Feedback Loops: Quality findings feed back to implementer\n\n**Decision:** Quality findings should feed back directly to next implementation attempt.\n\n**Implementation:**\n- When quality pass opens \"follow-up issues\", attach:\n  - Links to the original issue\n  - The quality agent's analysis (e.g., \"didn't handle null case in X\")\n- When implementer picks up follow-up issue, include analysis in prompt:\n  ```\n  The previous attempt failed because: [list]. Fix these specific problems.\n  ```\n\nThis turns the quality pass into a teacher, not just a bug generator.\n\n### 4. Pass Ordering: Planner on-demand per-issue (not just periodic)\n\n**Decision:** Run planner on-demand for manifests, keep periodic for backlog management.\n\n**Flow:**\n1. New issue arrives\n2. Planner runs, generates manifest + hints\n3. Implementation runs\n\n**Periodic planner still exists for:**\n- Rebalancing / reprioritizing backlog\n- Suggesting new refactor issues\n\nRelying only on periodic planner is where \"stale manifests\" come from.\n\n### 5. Diminishing Returns: Data-driven pass value metrics\n\n**Decision:** Log per-pass value metrics and make decisions data-driven.\n\n**For each pass type (planner, reviewer, quality), count:**\n- How often it changes outcome:\n  - Reviewer finds a bug → implementation corrected\n  - Quality pass yields issues not caught otherwise\n\n**Track:**\n- Average tokens / time per pass\n- Compute: \"Bugs caught per thousand tokens\" or \"per minute\"\n\nIf reviewer catches issues in 1/50 changes but costs a lot, restrict it to riskier paths.\n\n### 6. Model Selection: Abstract behind roles, default to strong model\n\n**Decision:** Short term, keep it simple and unify models where reasoning matters.\n\n**Design:**\n```toml\n[models]\nplanner = \"claude\"       # strong model for planning\nimplementer = \"claude\"   # strong model for implementation\nreviewer = \"claude\"      # can use cheaper if cost-sensitive\nquality = \"codex\"        # can use cheaper for triage\n```\n\n**Rationale:**\n- Planner quality matters a lot; using same strong model as implementer often improves plans\n- Quality/triage can often use cheaper model if cost is a concern\n\nMake it easy to A/B test different combinations.\n\n## Implementation Notes\n\nSee `src/loop.zig:runPlannerPass`, `runQualityPass`, `runIteration`.\n\n### TODO\n- [ ] Implement on-demand planner trigger for new issues\n- [ ] Add reviewer pass with risk-based triggering\n- [ ] Attach quality findings to follow-up issues\n- [ ] Add per-pass metrics logging\n- [ ] Abstract model selection behind role config\n","source_path":"design/multi-pass.md"},{"slug":"index","title":"noface","content_html":"<h1 id=\"noface\">noface<a class=\"heading-anchor\" href=\"#noface\" aria-label=\"Link to heading\">#</a></h1>\n<p>An autonomous agent orchestrator for software development. noface coordinates black-box agents (Claude Code, Codex) to work on issues from your backlog — in parallel, with conflict detection, crash recovery, and quality gates.</p>\n<h2 id=\"research\">Research<a class=\"heading-anchor\" href=\"#research\" aria-label=\"Link to heading\">#</a></h2>\n<ul>\n<li><a href=\"/orchestration-survey.html\">orchestration-survey</a> — Literature survey on design patterns for orchestrating code agents</li>\n</ul>\n<h2 id=\"design\">Design<a class=\"heading-anchor\" href=\"#design\" aria-label=\"Link to heading\">#</a></h2>\n<p>Core infrastructure decisions:</p>\n<ul>\n<li><a href=\"/manifests.html\">manifests</a> — File access control via PRIMARY/READ/FORBIDDEN declarations</li>\n<li><a href=\"/parallel-execution.html\">parallel-execution</a> — Batching, locking, and conflict avoidance</li>\n<li><a href=\"/context-injection.html\">context-injection</a> — What information agents receive and how</li>\n<li><a href=\"/multi-pass.html\">multi-pass</a> — Planner → Implementer → Reviewer architecture</li>\n<li><a href=\"/failure-recovery.html\">failure-recovery</a> — Retry, rollback, escalation strategies</li>\n<li><a href=\"/verification.html\">verification</a> — How we know an agent succeeded</li>\n</ul>\n<h2 id=\"quick-links\">Quick Links<a class=\"heading-anchor\" href=\"#quick-links\" aria-label=\"Link to heading\">#</a></h2>\n<ul>\n<li><a href=\"https://github.com/...\">GitHub</a></li>\n<li><a href=\"/.beads/issues.jsonl\">Issue Tracker</a></li>\n</ul>\n","frontmatter":{"title":"noface","description":"Design documentation for the noface autonomous agent orchestrator","summary":null,"date":null,"type":null,"tags":[],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":[],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":["orchestration-survey","manifests","parallel-execution","context-injection","multi-pass","failure-recovery","verification"],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#noface\">noface</a></li><li class=\"toc-level-2\"><a href=\"#research\">Research</a></li><li class=\"toc-level-2\"><a href=\"#design\">Design</a></li><li class=\"toc-level-2\"><a href=\"#quick-links\">Quick Links</a></li></ul></nav>","raw_body":"# noface\n\nAn autonomous agent orchestrator for software development. noface coordinates black-box agents (Claude Code, Codex) to work on issues from your backlog — in parallel, with conflict detection, crash recovery, and quality gates.\n\n## Research\n\n- [[orchestration-survey]] — Literature survey on design patterns for orchestrating code agents\n\n## Design\n\nCore infrastructure decisions:\n\n- [[manifests]] — File access control via PRIMARY/READ/FORBIDDEN declarations\n- [[parallel-execution]] — Batching, locking, and conflict avoidance\n- [[context-injection]] — What information agents receive and how\n- [[multi-pass]] — Planner → Implementer → Reviewer architecture\n- [[failure-recovery]] — Retry, rollback, escalation strategies\n- [[verification]] — How we know an agent succeeded\n\n## Quick Links\n\n- [GitHub](https://github.com/...)\n- [Issue Tracker](/.beads/issues.jsonl)\n","source_path":"index.md"}],"graph":{"outgoing":{"verification":["failure-recovery"],"manifests":["orchestration-survey"],"parallel-execution":["orchestration-survey"],"multi-pass":["orchestration-survey"],"index":["orchestration-survey","manifests","parallel-execution","context-injection","multi-pass","failure-recovery","verification"]},"incoming":{"context-injection":["index"],"failure-recovery":["verification","index"],"orchestration-survey":["parallel-execution","manifests","multi-pass","index"],"manifests":["index"],"verification":["index"],"multi-pass":["index"],"parallel-execution":["index"]}},"diagnostics":[],"comments":[]}}