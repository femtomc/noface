{"version":"1","generated_at":"2025-12-10T19:36:44.471123+00:00","site_index":{"notes":[{"slug":"agent","title":"AGENT","content_html":"<h1 id=\"monowiki-agent-guide\">monowiki Agent Guide<a class=\"heading-anchor\" href=\"#monowiki-agent-guide\" aria-label=\"Link to heading\">#</a></h1>\n<p>This vault ships with a CLI that is designed to be agent-friendly. Key commands:</p>\n<ul>\n<li><code>monowiki build</code>: render the site to <code>docs/</code> and emit <code>index.json</code>, <code>graph.json</code>, <code>previews.json</code>, and a cached site index at <code>docs/.site_index.json</code>.</li>\n<li><code>monowiki dev</code>: serve the site locally with live rebuilds <strong>and</strong> JSON endpoints:\n<ul>\n<li><code>/api/search?q=term&amp;limit=10</code></li>\n<li><code>/api/note/&lt;slug&gt;</code></li>\n<li><code>/api/graph/&lt;slug&gt;?depth=2&amp;direction=both</code></li>\n<li><code>/api/graph/path?from=a&amp;to=b&amp;max_depth=5</code></li>\n</ul>\n</li>\n<li><code>monowiki search \"&lt;query&gt;\" --json --limit 5 --types essay,thought --tags rust,notes --with-links</code> for machine-readable results.</li>\n<li><code>monowiki note &lt;slug&gt; --format json --with-links</code> to fetch a single note (frontmatter, rendered HTML, raw body, links).</li>\n<li><code>monowiki graph neighbors --slug &lt;slug&gt; --depth 2 --direction outgoing --json</code> to fan out.</li>\n<li><code>monowiki graph path --from a --to b --max-depth 4 --json</code> to find shortest paths.</li>\n<li><code>monowiki export sections --format jsonl --with-links</code> to stream embedding-ready chunks.</li>\n<li><code>monowiki watch</code> streams JSON change events from <code>vault/</code> (one line per event).</li>\n</ul>\n<p>JSON schemas</p>\n<ul>\n<li>CLI <code>--json</code> and dev server <code>/api/*</code> responses are wrapped in:</li>\n</ul>\n<div class=\"code-block\">\n<div class=\"code-toolbar\"><button class=\"copy-code-btn\" type=\"button\" aria-label=\"Copy code\">Copy</button></div>\n<pre style=\"background-color:#ffffff;\">\n<span style=\"color:#323232;\">{\n</span><span style=\"color:#323232;\">  </span><span style=\"font-weight:bold;color:#183691;\">&quot;schema_version&quot;</span><span style=\"color:#323232;\">: &quot;2024-11-llm-v1&quot;,\n</span><span style=\"color:#323232;\">  </span><span style=\"font-weight:bold;color:#183691;\">&quot;kind&quot;</span><span style=\"color:#323232;\">: &quot;search.results | note.full | graph.neighbors | graph.path&quot;,\n</span><span style=\"color:#323232;\">  </span><span style=\"font-weight:bold;color:#183691;\">&quot;data&quot;</span><span style=\"color:#323232;\">: { </span><span style=\"background-color:#f5f5f5;font-weight:bold;color:#b52a1d;\">...</span><span style=\"color:#323232;\"> }\n</span><span style=\"color:#323232;\">}\n</span></pre>\n\n</div>\n<ul>\n<li>Search results include <code>id</code>, <code>slug</code>, <code>url</code>, <code>title</code>, <code>section_title</code>, <code>snippet</code>, <code>tags</code>, <code>type</code>, <code>score</code>, <code>outgoing</code>, <code>backlinks</code>.</li>\n<li>Notes include frontmatter, HTML, raw markdown, toc, outgoing, backlinks, and dates.</li>\n<li>Graph neighbors include nodes with <code>slug/title/url/tags/type</code> and edges; graph path returns the path array.</li>\n</ul>\n<p>Performance tips</p>\n<ul>\n<li><code>monowiki build</code>/<code>dev</code> write <code>docs/.site_index.json</code>. The <code>note</code>, <code>graph</code>, and <code>export</code> commands reuse this cache to avoid rebuilding when only reading data. Delete it if you need a fresh rebuild.</li>\n</ul>\n<p>Conventions:</p>\n<ul>\n<li>Slugs come from frontmatter <code>slug</code>, otherwise the filename slugified.</li>\n<li>Drafts are excluded from exports/search when <code>type: draft</code> or <code>draft: true</code>.</li>\n<li>Backlinks are computed from <code>[[WikiLinks]]</code> in markdown and exposed via <code>graph.json</code> and the CLI/API.</li>\n<li>Section-level search slices HTML headings into chunks; IDs match rendered anchors.</li>\n</ul>\n<p>Tips for agents:</p>\n<ul>\n<li>Use <code>monowiki export sections</code> to build retrieval datasets without scraping.</li>\n<li>Use <code>monowiki note &lt;slug&gt; --format json</code> to fetch full context (toc, html, raw markdown) before editing.</li>\n<li>Prefer the dev server APIs during interactive sessions; they reflect live rebuilds.</li>\n</ul>\n<p>Happy hacking!</p>\n","frontmatter":{"title":"","description":null,"summary":null,"date":null,"type":null,"tags":[],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":[],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":[],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#monowiki-agent-guide\">monowiki Agent Guide</a></li></ul></nav>","raw_body":"# monowiki Agent Guide\n\nThis vault ships with a CLI that is designed to be agent-friendly. Key commands:\n\n- `monowiki build`: render the site to `docs/` and emit `index.json`, `graph.json`, `previews.json`, and a cached site index at `docs/.site_index.json`.\n- `monowiki dev`: serve the site locally with live rebuilds **and** JSON endpoints:\n  - `/api/search?q=term&limit=10`\n  - `/api/note/<slug>`\n  - `/api/graph/<slug>?depth=2&direction=both`\n  - `/api/graph/path?from=a&to=b&max_depth=5`\n- `monowiki search \"<query>\" --json --limit 5 --types essay,thought --tags rust,notes --with-links` for machine-readable results.\n- `monowiki note <slug> --format json --with-links` to fetch a single note (frontmatter, rendered HTML, raw body, links).\n- `monowiki graph neighbors --slug <slug> --depth 2 --direction outgoing --json` to fan out.\n- `monowiki graph path --from a --to b --max-depth 4 --json` to find shortest paths.\n- `monowiki export sections --format jsonl --with-links` to stream embedding-ready chunks.\n- `monowiki watch` streams JSON change events from `vault/` (one line per event).\n\nJSON schemas\n\n- CLI `--json` and dev server `/api/*` responses are wrapped in:\n\n```json\n{\n  \"schema_version\": \"2024-11-llm-v1\",\n  \"kind\": \"search.results | note.full | graph.neighbors | graph.path\",\n  \"data\": { ... }\n}\n```\n\n- Search results include `id`, `slug`, `url`, `title`, `section_title`, `snippet`, `tags`, `type`, `score`, `outgoing`, `backlinks`.\n- Notes include frontmatter, HTML, raw markdown, toc, outgoing, backlinks, and dates.\n- Graph neighbors include nodes with `slug/title/url/tags/type` and edges; graph path returns the path array.\n\nPerformance tips\n\n- `monowiki build`/`dev` write `docs/.site_index.json`. The `note`, `graph`, and `export` commands reuse this cache to avoid rebuilding when only reading data. Delete it if you need a fresh rebuild.\n\nConventions:\n- Slugs come from frontmatter `slug`, otherwise the filename slugified.\n- Drafts are excluded from exports/search when `type: draft` or `draft: true`.\n- Backlinks are computed from `[[WikiLinks]]` in markdown and exposed via `graph.json` and the CLI/API.\n- Section-level search slices HTML headings into chunks; IDs match rendered anchors.\n\nTips for agents:\n- Use `monowiki export sections` to build retrieval datasets without scraping.\n- Use `monowiki note <slug> --format json` to fetch full context (toc, html, raw markdown) before editing.\n- Prefer the dev server APIs during interactive sessions; they reflect live rebuilds.\n\nHappy hacking!\n","source_path":"AGENT.md"},{"slug":"orchestration-survey","title":"Design Patterns for Orchestrating Black-Box Code Agents","content_html":"<h1 id=\"design-patterns-for-orchestrating-black-box-code-agents-a-literature-survey\">Design Patterns for Orchestrating Black-Box Code Agents: A Literature Survey<a class=\"heading-anchor\" href=\"#design-patterns-for-orchestrating-black-box-code-agents-a-literature-survey\" aria-label=\"Link to heading\">#</a></h1>\n<p>Orchestrating black-box code-generation agents requires careful planning and robust design patterns to ensure these AI \"developers\" work effectively. Researchers and practitioners have proposed various strategies to assign tasks, manage context, engineer prompts, run agents in parallel safely, verify outputs, handle failures, and integrate multiple passes or human oversight. This survey reviews key patterns from academic literature and real-world case studies, organized by orchestration concern, and provides a pattern catalog of proven approaches.</p>\n<h2 id=\"task-assignment-and-granularity\">Task Assignment and Granularity<a class=\"heading-anchor\" href=\"#task-assignment-and-granularity\" aria-label=\"Link to heading\">#</a></h2>\n<p><strong>Dynamic Task Decomposition:</strong> A central pattern is to use an orchestrator agent (or component) that breaks down a software project into manageable subtasks and assigns them to specialized agents. Rather than having one AI attempt an entire project, orchestrators analyze the request and determine necessary subtasks (e.g. implement a feature, write tests, update docs). In Microsoft's taxonomy of agent architectures, an Orchestrator-Workers pattern is defined where a lead agent \"dynamically decomposes tasks, delegates them to worker agents, and synthesizes their outputs\". For example, in a coding context the orchestrator could decide which files or modules need changes and coordinate sub-agents for code generation, testing, and documentation. This dynamic assignment allows tackling large development goals by partitioning work.</p>\n<p><strong>Specialization and Routing:</strong> Task assignment often leverages specialization. The orchestrator may classify each task and route it to an agent best suited for that category (using a Routing Workflow pattern) [@pujals2025agentic]. For instance, code-generation requests might go to a \"Coder\" agent, while bug-fix tasks route to a \"Debugger\" agent, and documentation tasks to a \"Doc Writer\" agent. The input can be analyzed (via rules or an LLM) to decide which agent is most appropriate. This ensures that each agent works on tasks aligned with its expertise, improving effectiveness. Ellipsis's production code review system is an example: it runs multiple specialized agents in parallel, each looking for a specific type of issue in a pull request (security, style, duplication, etc.), rather than one monolithic agent [@zenml2024ellipsis].</p>\n<p><strong>Granularity – Sizing the Task:</strong> Deciding how large a task to give a single agent call is crucial. Literature suggests that tasks should be fine-grained enough to fit the agent's context window and capabilities, yet not so granular that the overhead of orchestration dominates. If a task is too large (e.g. \"implement an entire feature spanning many files\"), agents may struggle with context or produce incomplete solutions. Researchers have addressed this by explicit task planning. Self-planning code generation has the LLM first outline a plan of steps for a complex request, then tackle each step sequentially [@jiang2024selfplanning]. This two-phase approach of planning then coding yields up to 25.4% higher success (Pass@1) than one-shot generation for complex problems. Similarly, the Prompt-Chaining workflow breaks a complex job into a sequence of smaller subtasks, each handled by a dedicated prompt/agent call [@pujals2025agentic]. This not only makes each step easier for the model to execute but also improves transparency and debuggability. In practice, a rule of thumb is to treat one well-defined issue or function as a unit of work for an agent, and if the request spans multiple concerns or components, the orchestrator should subdivide it.</p>\n<p><strong>Handling Dependencies and Readiness:</strong> An orchestrator must also respect task dependencies when assigning work. Some tasks can run in parallel (e.g. implement UI and backend separately), while others depend on earlier outputs (e.g. tests should run after code is written). One practical approach is to maintain a shared task board with statuses and dependencies. In a user's multi-agent Claude setup, tasks were tracked in a Markdown plan file with fields for Assigned Agent, Status, and Dependencies – e.g. \"Write Integration Tests – Assigned to Validator – Pending (waiting for Builder to complete auth module)\". This allows the orchestrator (and agents themselves) to know when a task is ready to start. If a task is too big or not well-defined, a planner agent might refine it further or ask for clarification before assignment. Ultimately, effective task assignment balances priority (which issues are most important), readiness (are prerequisites done?), and size (fits in one agent invocation).</p>\n<h2 id=\"context-curation-providing-helpful-context-and-avoiding-harmful-noise\">Context Curation: Providing Helpful Context (and Avoiding Harmful Noise)<a class=\"heading-anchor\" href=\"#context-curation-providing-helpful-context-and-avoiding-harmful-noise\" aria-label=\"Link to heading\">#</a></h2>\n<p><strong>Relevant Context Injections:</strong> The information an agent gets in its prompt (code context, design docs, examples) profoundly impacts its success. A key pattern is Retrieval-Augmented Code Generation (RACG), where the orchestrator fetches relevant snippets from the codebase or documentation to include in the agent's prompt. Real-world software tasks often require understanding cross-file dependencies and global architecture. Since an LLM has a limited context window, the orchestrator must be selective: for example, retrieving the function signature and related config file when asking the agent to implement a new feature in that function. A recent survey emphasizes that repository-level code generation needs to ensure global semantic consistency across files, and integrating a retrieval mechanism (e.g. searching a knowledge base of code) helps the LLM reason beyond a single file. In practice, tools like vector databases are used to find and feed the most relevant code pieces to the agent. Ellipsis's code review agents, for instance, use advanced code search backed by embeddings to find pertinent pieces of the codebase when analyzing a PR [@zenml2024ellipsis].</p>\n<p><strong>Curating Helpful Examples and Constraints:</strong> Apart from code context, orchestrators often supply design documents, API usage examples, or style guides as context if they will guide the agent toward the right solution. Providing a few-shot example of a similar function or test can dramatically improve accuracy, as the agent can analogize from the given pattern. Additionally, context can include explicit constraints – e.g. a list of files that are in scope, or performance requirements. By giving the agent a clear problem statement along with these auxiliary materials, the orchestrator sets it up for success. For example, if generating a new module, the orchestrator might include an outline of the module's design or a simplified spec in the prompt. In PerfOrch (Performance-Oriented Orchestration), contextual knowledge about which LLMs perform best for a given language and problem type is stored and used to pick the right model for the task [@wang2025perforch]. This \"strategic memory\" of model strengths is a form of context that improves outcomes by dynamically selecting the optimal agent for each subtask.</p>\n<p><strong>Avoiding Context Overload:</strong> Too much context can hurt agent performance. It might be tempting to provide every possibly relevant file to be safe, but large context size can confuse the model or dilute its focus. Recent studies have shown that even if all provided context is relevant, performance can degrade substantially (by 13–85%) as input length grows. In other words, beyond a certain point, the sheer volume of text can overwhelm the LLM's reasoning (\"context dilution\"). Thus, orchestrators should be judicious: include only the most relevant code and information. Irrelevant or contradictory context is even worse – it can lead the agent to draw false inferences or focus on the wrong problem. Best practices include using retrieval algorithms to filter context, and possibly summarizing or abstracting context to fit the window. For instance, rather than inserting an entire 500-line file, an orchestrator might insert a summary or the specific function signature needed. The Context-Augmentation pattern described by Hugging Face highlights that context expansion should be iterative and need-based: the agent identifies what extra info it needs, the orchestrator fetches it, and this cycle repeats, rather than dumping everything at once [@pujals2025agentic]. Techniques like embedding search, docstrings extraction, or code property graphs can help select which context truly matters.</p>\n<p><strong>Detecting and Removing Harmful Context:</strong> Orchestrators may also implement \"context pruning\" – removing any information from the prompt that isn't consistent or necessary. This can include stale data (e.g. outdated function definitions if a newer version exists) or irrelevant conversation history in iterative settings. The goal is to minimize noise. If an agent will be doing a focused code edit, the orchestrator might strip out any unrelated instructions from the conversation and supply a fresh prompt with just the needed background. In summary, curated context is a double-edged sword: the right snippets and examples can greatly enhance success, but too much or wrong context can mislead the agent. Effective orchestration employs retrieval, filtering, and pruning to optimize context.</p>\n<h2 id=\"prompt-engineering-at-the-orchestration-level\">Prompt Engineering at the Orchestration Level<a class=\"heading-anchor\" href=\"#prompt-engineering-at-the-orchestration-level\" aria-label=\"Link to heading\">#</a></h2>\n<p><strong>Structured Instructions vs. Plain Language:</strong> Orchestration involves not only what tasks agents do, but how those tasks are described in prompts. A recurring pattern is to use structured, formal instructions rather than leaving everything to freeform natural language. For example, instead of saying \"Please write some code to do X\", an orchestrator-generated prompt might include a template: a step-by-step checklist, or a specific format like pseudo-code or JSON plan that the agent must follow. This structure can guide the model's output and reduce ambiguity. A concrete case is the self-planning approach: the orchestrator first asks the LLM to output a formatted plan (e.g. a numbered list of steps), and then feeds that plan into the next prompt for code generation [@jiang2024selfplanning]. By explicitly separating \"thinking\" from \"coding,\" the instructions become more systematic, leading to better results. Similarly, multi-agent frameworks often start with a role acknowledgment and task breakdown. In a four-agent setup using Claude (Architect, Builder, Validator, Scribe), each agent's system prompt begins with a role declaration and primary tasks in bullet form (e.g. \"You are Agent 1 – The Architect. Primary Tasks: requirements analysis, architecture planning…\"). This kind of structured prompt ensures each agent knows its exact scope and constraints.</p>\n<p><strong>Communicating Constraints Clearly:</strong> The orchestration layer must convey any hard constraints or \"rules of engagement\" to the agent in the prompt. This can include: \"Do not modify code outside of Function A in file X\", \"Follow the project's style guide for naming\", or \"Make sure to run the provided tests and ensure they pass\". Explicitly listing files or sections not to touch can prevent collateral changes. However, LLMs may not always obey implicitly phrased constraints, so phrasing is important. Some systems adopt a manifest-driven approach, where the orchestrator provides a manifest of intended changes. For instance, the manifest might enumerate which functions to implement or which files to create, and the agent is instructed that any output will be checked against this manifest. A recent methodology called Manifest-Driven AI Development (MAID) takes this further by treating the manifest as an enforceable contract: after generation, a validator checks that only the declared files/functions were altered and no undeclared edits occurred. By front-loading these constraints (and later verifying them), the orchestrator greatly reduces the chance of the agent \"coloring outside the lines.\" Another way to impose structure is using tool-assisted prompting languages (like Microsoft's Guidance library) that allow templates with placeholders and even regex checks on the LLM's output, ensuring the agent adheres to formats.</p>\n<p><strong>Success Criteria and Framing:</strong> The prompt should also frame what constitutes success. A good orchestration prompt might end with a clear success condition, e.g.: \"Your solution will be considered correct if all given unit tests pass and it meets the performance constraint of &lt;2s runtime.\" By stating these criteria, the agent can self-evaluate its output against them, leading to better alignment with the goal. This is related to the Evaluator-Optimizer pattern [@carpintero2025evaluator], where the prompt or subsequent agent pass includes evaluation guidelines. For example, an orchestrator might instruct: \"First, implement the function. Then double-check that the function handles edge cases (null input, etc.) and meets the description. Only finalize if these criteria are satisfied.\" Such structured prompts push the agent to reflect on its answer against requirements, effectively building a spec into the prompt. Research by Anthropic also suggests that using systematic prompt patterns often outperforms ad-hoc prompting [@anthropic2024agents]. In their experience, many teams succeeded by \"simple, composable patterns\" in prompts (like stepwise reasoning or tool usage specifications) rather than overly open-ended prompts. In sum, the orchestration layer benefits from acting like a technical writer: providing the right scaffolding in the prompt (roles, steps, examples, constraints, and success checks) so the black-box agent is guided toward the desired outcome.</p>\n<p><strong>Example – Claude Subagents:</strong> As a practical example, Claude's subagents feature allows configuring specialized system prompts for different sub-tasks. Each subagent has \"a custom system prompt that guides its behavior\" and a defined expertise. The main agent automatically delegates to these subagents when appropriate. In effect, the orchestrator (Claude's internal logic) appends the specialized prompt and context for that sub-task. This demonstrates prompt engineering at scale: rather than one giant prompt for everything, the orchestration chooses a tailored prompt per task type. Overall, structured and deliberate prompt engineering at the orchestration level — using instructions, role context, examples, and criteria — is key to maximizing agent reliability.</p>\n<h2 id=\"parallelism-conflict-avoidance\">Parallelism &amp; Conflict Avoidance<a class=\"heading-anchor\" href=\"#parallelism-conflict-avoidance\" aria-label=\"Link to heading\">#</a></h2>\n<p><strong>When to Run Agents in Parallel:</strong> Orchestrating multiple agents in parallel can speed up development, but only if their tasks won't conflict. The Parallelization Workflow follows a divide-and-conquer approach: identify subtasks that are truly independent and execute them concurrently [@pujals2025agentic]. For example, an orchestrator might split a feature into frontend and backend implementations and assign each to a different agent at the same time. Another parallel pattern is \"voting\" – run multiple agents on the same task in parallel (perhaps with different prompting styles or different model seeds) and then choose the best result. In coding, this could mean generating multiple candidate solutions concurrently and later selecting one that passes tests (\"N-best\" approach). However, if parallel agents operate on shared artifacts (e.g. editing the same file), conflicts can arise. Thus, orchestrators should only parallelize tasks that either target separate components or use a strategy to reconcile outputs.</p>\n<p><strong>Locking and Work Queues:</strong> A common strategy for conflict avoidance is file-level locking or resource locking. Before an agent begins work, the orchestrator can lock the files or modules it will touch, preventing other agents from editing them until completion. A community-built \"Claude code agent farm\" demonstrates this with a coordination protocol: each agent must create a lock file listing the files/features it intends to work on, and consult a shared active work registry to ensure no overlap. If a conflict is detected (e.g. two agents want to modify utils.py), one agent will wait (queue the task) until the file is free. This effectively serializes conflicting sections while still allowing non-conflicting tasks in parallel. The registry and lock files also help avoid duplicate work – agents check the completed task log so they don't redo something another agent already finished. Such locking is a conservative heuristic but very effective: it \"eliminates merge conflicts\" and \"prevents wasted work\" in concurrent development. The downside is that overly coarse locks (e.g. locking an entire project) nullify the speed benefit, so orchestrators must strike a balance by locking at a sensible granularity (file or feature level).</p>\n<p><strong>Advanced Conflict Resolution (CRDTs):</strong> Recent research explores lock-free coordination using principles from distributed systems. CodeCRDT introduces an observation-driven approach where agents don't explicitly lock resources, but instead work on a shared state (a codebase) that automatically merges changes in a conflict-free manner [@pugachev2025codecrdt]. It uses Conflict-Free Replicated Data Types to ensure that as agents make edits, their updates converge deterministically without stepping on each other. In trials, this achieved 100% merge convergence with zero low-level conflicts, eliminating the need for manual conflict resolution. Essentially, each agent can edit concurrently and the CRDT mechanism integrates their changes (like Google Docs but for code). While CRDT-based orchestration can avoid the bottleneck of locks (which can become contention points as agent count N grows), it introduces complexity: agents must deal with semantic conflicts (e.g. two agents might implement the same feature in different ways, both syntactically merged but logically redundant). CodeCRDT reported about 5–10% of such higher-level conflicts, meaning an orchestrator or later review still has to reconcile duplicate or inconsistent implementations. Nonetheless, this approach is promising for truly scalable parallel agent teams, as it sidesteps the O(N×L) contention of lock-based systems.</p>\n<p><strong>Heuristics for Parallel Task Selection:</strong> In practice, orchestrators often use simple heuristics to decide parallelism: e.g. no two agents on the same file or closely related module. Some systems use static analysis of code dependency graphs to ensure two parallel tasks don't overlap in call graphs or data models. Others rely on the developer to specify which tasks are independent. For example, an orchestrator might know that adding a new API endpoint (task A) is largely independent of refactoring the logging library (task B), so it schedules those simultaneously. If there is uncertainty, a conservative orchestrator will serialize tasks to avoid the risk of conflict. In the worst case that a conflict does occur (say two agents inadvertently changed the same file), the orchestrator must have a conflict-resolution step: it could auto-merge the code (if changes are in different regions), prefer one agent's output over the other, or fall back to human intervention to reconcile differences. Generally, the goal is conflict avoidance through design: split work such that each agent has its own domain of responsibility. As one user's experience with 4 parallel Claude agents showed, dividing roles by concern (architecture vs. coding vs. testing vs. docs) inherently reduced direct conflicts and even brought positive effects like built-in peer review. Each agent had a clear lane, enabling parallel development with quality checks due to the separation of concerns.</p>\n<p>In summary, orchestrating parallel code agents requires careful coordination: lock or isolate shared resources, or adopt novel merging strategies, so that parallelism yields speed-up without chaos. When done right (e.g. with file locks or CRDTs), teams of agents can work \"like a well-oiled machine\" in parallel.</p>\n<h2 id=\"verification-acceptance-criteria\">Verification &amp; Acceptance Criteria<a class=\"heading-anchor\" href=\"#verification-acceptance-criteria\" aria-label=\"Link to heading\">#</a></h2>\n<p><strong>Automated Testing as Ground Truth:</strong> One of the most powerful verification techniques is running tests on the agent's output. If the task comes with a test suite or at least some example cases, the orchestrator can automatically execute the generated code and check for correctness. This pattern is evident in many frameworks. AgentCoder is explicitly built around a test-feedback loop: a Test Designer agent generates unit tests, a Test Executor runs the code on those tests, and the Programmer agent then debugs or refines the code based on failures [@huang2023agentcoder]. This ensures that code isn't accepted as \"done\" until it passes its tests. Even single-agent approaches like OpenAI's Codex have employed this idea (often called execute-and-fix): run the code, and if an error or failing test is detected, prompt the model to fix it. The orchestrator's job is to automate this loop. In PerfOrch's multi-stage pipeline, after generation they perform a validation: if the code is functionally incorrect, it triggers the Fix Stage, invoking specialized bug-fixing models iteratively until a correct solution is found [@wang2025perforch]. Only once all tests pass does it proceed to the next stage. This demonstrates an automated acceptance gate – test passing is the criterion for success. Orchestrators can also measure other aspects like performance: PerfOrch's final Refine Stage checked if runtime improved and only accepted refinements that preserved correctness and met the performance target.</p>\n<p><strong>Code Reviews and LLM Critics:</strong> Besides tests, another verification layer is reviewing the code, either by humans or by a second \"reviewer\" agent. Having a separate agent critique the generated code can catch issues that tests don't (e.g. poor readability, not following style, potential inefficiencies). A pattern often cited is dual-agent coding: one agent writes code, another agent reviews it. If the reviewer finds problems, the coder agent revises accordingly. This is essentially the Evaluator-Optimizer pattern applied to code [@carpintero2025evaluator] – where the Evaluator (which could be the same LLM or a different one tuned for critique) provides feedback, and the Optimizer (generator) uses it to improve the output in another pass. Academic and industry work supports this approach. In practice, companies like Meta have prototypes where an AI code generation is always followed by an AI code review, flagging issues before any human sees the code. Ellipsis's production system likewise emphasizes accuracy over latency: they run multiple comment-generation agents and then use a filtering pipeline (including an LLM-as-judge that evaluates the proposed comments) to ensure only high-quality, non-false-positive feedback is given [@zenml2024ellipsis]. By using an LLM judge on the outputs of LLM coders, they add a second layer of assurance.</p>\n<p><strong>Manifests and Static Verification:</strong> As discussed in prompt engineering, if the orchestrator provided a manifest of expected changes, verification includes checking the agent's output against that manifest. Tools like MAID runner perform static analysis on the diff: did the agent only modify the allowed files and functions? Did it include the required new function? This kind of structural verification can be automated by parsing the output or performing AST (Abstract Syntax Tree) comparison between the original and modified code. If the agent deviated (e.g. changed something unrelated), the orchestrator can reject that output or prompt a correction (\"You edited file Y which was out of scope; please revert that.\"). Additionally, the orchestrator might enforce style guides by running linters or formatters on the code and ensuring no new warnings were introduced.</p>\n<p><strong>Acceptance and Quality Gates:</strong> Complex orchestrations often define multi-dimensional acceptance criteria. Functional correctness (tests passing) is usually mandatory. Beyond that, style compliance, performance metrics, security scans, etc., can be gates. For example, after an agent's code passes tests, the orchestrator might also run a static security analyzer; if it flags issues, the agent's output isn't fully accepted and a fix task is spawned (or a human review is requested for that aspect). Hugging Face's guide suggests quality gates such as \"code passes all test suites\" as a primary stopping condition for an iterative refine loop – essentially, you keep refining until the code meets the quality bar [@pujals2025agentic]. They also note using a maximum iteration limit and similarity checks to avoid infinite loops.</p>\n<p><strong>Handling Partial Success:</strong> Sometimes an agent's output is partially correct – e.g. it implements one feature but misses another, or fixes some bugs but not all. In these cases, orchestrators can take a pragmatic approach: accept the improvements made and then create follow-up tasks for the remaining work. This is akin to how a human might commit a partial fix and then address the rest. Orchestrators with tracking systems (like the completed_work_log in the agent farm) can mark certain sub-goals as done and only reassign what failed. If tests are granular, the orchestrator can rerun them and see which specific tests still fail, then prompt the agent specifically about those failures on the next try. This targeted retry approach zeroes in on the unresolved issues rather than redoing everything.</p>\n<p><strong>Automated vs Human Acceptance:</strong> Ultimately, the output may either be auto-merged or require human approval. Many real-world setups keep a human-in-the-loop for final approval – for example, an orchestrator might open a pull request with the agent's changes and require a human developer to review and merge. The orchestrator in this case hands off to a human at the final step. Another checkpoint is when an agent is uncertain or stuck. If an agent expresses confusion (some advanced agents might output a special token or message like \"I am not sure how to proceed\"), the orchestrator should detect that and involve a human to clarify requirements or provide a hint.</p>\n<p>In summary, verification in agent orchestration is about establishing feedback loops and safety nets: tests to verify correctness, secondary agents or tools to verify quality, and structured checks (manifests, linters) to enforce constraints. By combining these, orchestrators can confidently determine when an agent \"succeeded\" and its output is acceptable.</p>\n<h2 id=\"failure-modes-recovery-strategies\">Failure Modes &amp; Recovery Strategies<a class=\"heading-anchor\" href=\"#failure-modes-recovery-strategies\" aria-label=\"Link to heading\">#</a></h2>\n<p>Even with the best planning, agents will sometimes fail – producing wrong code, no code, or even crashing. The orchestration layer must be resilient to these failures and have recovery strategies.</p>\n<p><strong>Graceful Retries:</strong> The simplest recovery pattern is retrying the same prompt. Because LLM agents are stochastic, a second attempt might yield a different (possibly correct) result. Orchestrators often implement a retry-on-failure (with a limit on retries to avoid loops). If an agent times out or returns an error (e.g. if using an API that errors out), the orchestrator can automatically retry after a short delay or with a smaller context. The Claude agent farm tool, for example, monitors each agent's health and will auto-restart an agent if it crashes or gets stuck. This ensures that a transient glitch doesn't halt the entire workflow.</p>\n<p><strong>Refining the Prompt or Task Breakdown:</strong> If a direct retry doesn't help, the orchestrator should try a modified approach. One common strategy is progressive prompting: if the agent failed to produce output, perhaps the instructions were too broad – so break the task into smaller sub-tasks and prompt those instead. For instance, if \"implement the foobar algorithm\" came back empty or incorrect, the orchestrator might break it into \"step 1: implement the helper function X\" and \"step 2: implement main foobar using X\". Alternatively, the orchestrator can supply more guidance in the prompt on a retry – e.g. provide a partial solution or additional hints. This resembles a fallback to a more guided prompt when the agent seems to be stuck with a high-level request. In the Prompt-Chaining considerations, it's suggested to re-prompt including details of the failure in the context if an intermediate step fails [@pujals2025agentic]. For example: \"The previous attempt resulted in a runtime error at line 20. Here is the error log… Please fix the code to handle this issue.\" This gives the agent new information to succeed on the next pass.</p>\n<p><strong>Alternate Agents or Models:</strong> Another recovery pattern is trying a different agent or model. If one model repeatedly fails to solve a problem, perhaps a more powerful model (or one fine-tuned differently) could succeed. PerfOrch demonstrates this by maintaining a ranking of models for each stage – if the top choice fails, it falls back to the next best model for that task [@wang2025perforch]. In their bug-fixing stage, they iteratively invoked up to n different specialized LLMs until one fixed the bug. Similarly, if an agent like GPT-3.5 is not producing a solution, an orchestrator might escalate to GPT-4 for that task. This is a cost-performance trade-off: use cheaper models for most tasks, but have a path to involve a more capable (or simply different) model on failure.</p>\n<p><strong>Preserving Partial Progress:</strong> In complex multi-step tasks, an agent might accomplish some steps successfully before failing at a later step. A robust orchestrator will preserve this partial progress so it's not lost. For example, if an agent wrote 80% of a file correctly and then struggled, the orchestrator can save the file (or keep it in memory) and instruct a subsequent agent to continue from that state. This could involve checking the agent's output for usable sections. Some frameworks treat the codebase like a persistent state that agents incrementally build – even if one agent stops mid-way, the next agent sees the codebase with whatever was added so far.</p>\n<p><strong>Escalating to Human or Halting:</strong> Not all failures can or should be handled autonomously. A critical safety mechanism is to escalate to a human when automated retries are exhausted or when the cost of more retries outweighs the benefit. The orchestrator might, for instance, make 3 attempts at a task (each possibly with refined prompts or different models). If none succeed, it can stop and label the task for human attention. In a CI/CD integration, this could mean leaving a comment like \"AI agent could not resolve this issue – please review manually.\"</p>\n<p><strong>Logging and Learning from Failures:</strong> Another aspect of handling failures is logging them for continuous improvement. The orchestrator should log error cases, so developers can later analyze if prompt tweaks or training could solve them. In some advanced setups, this log is fed into a \"reflexion\" agent that tries to learn from failures by adjusting future prompts.</p>\n<p>In essence, failure recovery in orchestration is about resilience and adaptability. Try again (maybe differently), switch strategies if needed, and know when to ask for help.</p>\n<h2 id=\"multi-pass-architectures-planner-implementer-reviewer-etc\">Multi-Pass Architectures (Planner, Implementer, Reviewer, etc.)<a class=\"heading-anchor\" href=\"#multi-pass-architectures-planner-implementer-reviewer-etc\" aria-label=\"Link to heading\">#</a></h2>\n<p>One of the most powerful design patterns emerging in AI code generation is the use of multiple passes or multiple agent roles to tackle a problem in stages. Instead of a single monolithic prompt-to-code step, the task is divided into phases such as planning, implementing, and reviewing.</p>\n<p><strong>Planner → Implementer Workflow:</strong> In this pattern, a Planner agent first creates a high-level plan or design, which is then used by an Implementer agent to write the actual code. The planner might outline which functions need to be created, what the approach will be, or even pseudo-code. The implementer then follows this blueprint. The self-planning study exemplifies this: the LLM generates concise, structured planning steps from the intent, then in the implementation phase it \"generates code step by step, guided by the preceding planning steps\" [@jiang2024selfplanning]. This two-pass approach yielded significantly higher correctness than a single-pass solution, because the model effectively did some problem decomposition and reasoning out loud before coding. MapCoder extends this idea to four stages: it uses one agent to recall relevant examples, another to plan the solution approach, a third to generate code, and a fourth to debug the result [@islam2024mapcoder]. Each agent is specialized for its role, and together they \"replicate the full cycle of program synthesis as observed in human developers\". The results were state-of-the-art on code benchmarks, underlining the efficacy of a multi-pass pipeline.</p>\n<p><strong>Reviewer or Refiner Pass:</strong> Another common multi-pass setup is having a Reviewer agent or second pass that checks and improves the code written by an Implementer (first pass). In a multi-pass architecture, this is built-in: after code generation, the process automatically goes to a review phase. The reviewer might be tasked with ensuring the code meets the spec and then either directly editing the code to fix issues or providing feedback for the implementer to act on. This can even become an iterative loop: implementer → reviewer → implementer (for revisions) → reviewer … until the reviewer is satisfied. This approach aligns with the Evaluator-Optimizer iterative refinement pattern [@carpintero2025evaluator].</p>\n<p><strong>When Multi-Pass Helps:</strong> Multi-pass shines especially for complex tasks that benefit from explicit reasoning or quality control. If a coding task requires sophisticated reasoning (algorithm design, complex logic), a planning pass helps. If it requires high assurance (safety-critical code, or just code in a large codebase where consistency matters), a review/testing pass helps. On simpler tasks (e.g. \"add a print statement\"), multi-pass might be overkill – a single agent can handle it and the overhead of planning/reviewing might outweigh the simplicity of the change.</p>\n<p><strong>Feedback Loops:</strong> Multi-pass architectures inherently create a feedback loop, especially with a reviewer/corrector in the mix. The orchestrator should feed the feedback in a form the implementer can act on. For example, if a reviewer says \"Function foo doesn't handle negative inputs properly,\" the orchestrator might prompt the implementer agent with that exact feedback: \"The code was reviewed and the following issue was found: it doesn't handle negative inputs. Please update the code to address this.\"</p>\n<h2 id=\"human-in-the-loop-in-orchestration\">Human-in-the-Loop in Orchestration<a class=\"heading-anchor\" href=\"#human-in-the-loop-in-orchestration\" aria-label=\"Link to heading\">#</a></h2>\n<p>Even with all the above automation, the human element remains important. Orchestration should consider when to involve human developers or stakeholders to guide or approve the process.</p>\n<p><strong>Checkpoints for Human Input:</strong> A well-designed orchestrator will define certain points at which it pauses and seeks human input. This might be after planning – e.g., once a Planner agent produces a design, the orchestrator could show that plan to a human for approval before coding. It could also be after code generation but before acceptance – essentially treating the AI-generated code like a regular code review that a human must OK.</p>\n<p><strong>Surviving \"I'm stuck\" vs \"I'm done\":</strong> Differentiating between an agent that believes it's finished vs one that gave up is key. If an agent says \"Solution complete\" but the tests are failing, clearly it was wrongly done. The orchestrator should treat that as a failure (and possibly loop in a reviewer agent or human). On the other hand, if after several tries the agent outputs, \"I cannot find an approach for this problem,\" then the orchestrator has an impasse. This is a prime moment for human-in-the-loop.</p>\n<p><strong>Human Oversight and Approval:</strong> Humans may also impose guardrails by setting policies the orchestrator follows. For example, an organization might require that any code touching security-critical files must be reviewed by a human, no matter what. The orchestrator can have a rule: if task involves auth.py (or generally high-risk area), always assign it to a human or at least get human approval after the AI suggests changes.</p>\n<p><strong>Balancing Autonomy and Oversight:</strong> The goal of orchestration is often to minimize human effort, but not to eliminate it entirely at the cost of quality. An insightful guideline from Anthropic is to use the simplest solution possible and only add complexity (autonomy) when needed [@anthropic2024agents]. This suggests that if a task can be handled with a single LLM call plus a quick human tweak, that might be preferable to spinning up a whole autonomous multi-agent system.</p>\n<h2 id=\"conclusion\">Conclusion<a class=\"heading-anchor\" href=\"#conclusion\" aria-label=\"Link to heading\">#</a></h2>\n<p>Orchestrating black-box code agents is an exercise in applying sound software engineering principles to AI-driven automation. The patterns identified – from task assignment and context curation to multi-pass workflows and human oversight – collectively form a toolkit for reliable AI-assisted development.</p>\n<p>In designing an orchestration framework, one should treat AI agents as fallible collaborators that need structure and guidance – much like human junior developers. The orchestrator's role is part project manager, part version-control system, and part QA engineer for this AI \"team.\" It decides who does what (task assignment), gives them what they need (context), tells them how to do it (prompt engineering and constraints), lets them work together safely (parallelism control), checks their work (verification), handles when they get it wrong (failure recovery), encourages iterative improvement (multi-pass), and knows when to ask a senior (human-in-loop).</p>\n<h2 id=\"references\">References<a class=\"heading-anchor\" href=\"#references\" aria-label=\"Link to heading\">#</a></h2>\n","frontmatter":{"title":"Design Patterns for Orchestrating Black-Box Code Agents","description":null,"summary":null,"date":null,"type":"essay","tags":["research","agents","orchestration","survey"],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":["research","agents","orchestration","survey"],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":[],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#design-patterns-for-orchestrating-black-box-code-agents-a-literature-survey\">Design Patterns for Orchestrating Black-Box Code Agents: A Literature Survey</a></li><li class=\"toc-level-2\"><a href=\"#task-assignment-and-granularity\">Task Assignment and Granularity</a></li><li class=\"toc-level-2\"><a href=\"#context-curation-providing-helpful-context-and-avoiding-harmful-noise\">Context Curation: Providing Helpful Context (and Avoiding Harmful Noise)</a></li><li class=\"toc-level-2\"><a href=\"#prompt-engineering-at-the-orchestration-level\">Prompt Engineering at the Orchestration Level</a></li><li class=\"toc-level-2\"><a href=\"#parallelism-conflict-avoidance\">Parallelism &amp; Conflict Avoidance</a></li><li class=\"toc-level-2\"><a href=\"#verification-acceptance-criteria\">Verification &amp; Acceptance Criteria</a></li><li class=\"toc-level-2\"><a href=\"#failure-modes-recovery-strategies\">Failure Modes &amp; Recovery Strategies</a></li><li class=\"toc-level-2\"><a href=\"#multi-pass-architectures-planner-implementer-reviewer-etc\">Multi-Pass Architectures (Planner, Implementer, Reviewer, etc.)</a></li><li class=\"toc-level-2\"><a href=\"#human-in-the-loop-in-orchestration\">Human-in-the-Loop in Orchestration</a></li><li class=\"toc-level-2\"><a href=\"#conclusion\">Conclusion</a></li><li class=\"toc-level-2\"><a href=\"#references\">References</a></li></ul></nav>","raw_body":"# Design Patterns for Orchestrating Black-Box Code Agents: A Literature Survey\n\nOrchestrating black-box code-generation agents requires careful planning and robust design patterns to ensure these AI \"developers\" work effectively. Researchers and practitioners have proposed various strategies to assign tasks, manage context, engineer prompts, run agents in parallel safely, verify outputs, handle failures, and integrate multiple passes or human oversight. This survey reviews key patterns from academic literature and real-world case studies, organized by orchestration concern, and provides a pattern catalog of proven approaches.\n\n## Task Assignment and Granularity\n\n**Dynamic Task Decomposition:** A central pattern is to use an orchestrator agent (or component) that breaks down a software project into manageable subtasks and assigns them to specialized agents. Rather than having one AI attempt an entire project, orchestrators analyze the request and determine necessary subtasks (e.g. implement a feature, write tests, update docs). In Microsoft's taxonomy of agent architectures, an Orchestrator-Workers pattern is defined where a lead agent \"dynamically decomposes tasks, delegates them to worker agents, and synthesizes their outputs\". For example, in a coding context the orchestrator could decide which files or modules need changes and coordinate sub-agents for code generation, testing, and documentation. This dynamic assignment allows tackling large development goals by partitioning work.\n\n**Specialization and Routing:** Task assignment often leverages specialization. The orchestrator may classify each task and route it to an agent best suited for that category (using a Routing Workflow pattern) [@pujals2025agentic]. For instance, code-generation requests might go to a \"Coder\" agent, while bug-fix tasks route to a \"Debugger\" agent, and documentation tasks to a \"Doc Writer\" agent. The input can be analyzed (via rules or an LLM) to decide which agent is most appropriate. This ensures that each agent works on tasks aligned with its expertise, improving effectiveness. Ellipsis's production code review system is an example: it runs multiple specialized agents in parallel, each looking for a specific type of issue in a pull request (security, style, duplication, etc.), rather than one monolithic agent [@zenml2024ellipsis].\n\n**Granularity – Sizing the Task:** Deciding how large a task to give a single agent call is crucial. Literature suggests that tasks should be fine-grained enough to fit the agent's context window and capabilities, yet not so granular that the overhead of orchestration dominates. If a task is too large (e.g. \"implement an entire feature spanning many files\"), agents may struggle with context or produce incomplete solutions. Researchers have addressed this by explicit task planning. Self-planning code generation has the LLM first outline a plan of steps for a complex request, then tackle each step sequentially [@jiang2024selfplanning]. This two-phase approach of planning then coding yields up to 25.4% higher success (Pass@1) than one-shot generation for complex problems. Similarly, the Prompt-Chaining workflow breaks a complex job into a sequence of smaller subtasks, each handled by a dedicated prompt/agent call [@pujals2025agentic]. This not only makes each step easier for the model to execute but also improves transparency and debuggability. In practice, a rule of thumb is to treat one well-defined issue or function as a unit of work for an agent, and if the request spans multiple concerns or components, the orchestrator should subdivide it.\n\n**Handling Dependencies and Readiness:** An orchestrator must also respect task dependencies when assigning work. Some tasks can run in parallel (e.g. implement UI and backend separately), while others depend on earlier outputs (e.g. tests should run after code is written). One practical approach is to maintain a shared task board with statuses and dependencies. In a user's multi-agent Claude setup, tasks were tracked in a Markdown plan file with fields for Assigned Agent, Status, and Dependencies – e.g. \"Write Integration Tests – Assigned to Validator – Pending (waiting for Builder to complete auth module)\". This allows the orchestrator (and agents themselves) to know when a task is ready to start. If a task is too big or not well-defined, a planner agent might refine it further or ask for clarification before assignment. Ultimately, effective task assignment balances priority (which issues are most important), readiness (are prerequisites done?), and size (fits in one agent invocation).\n\n## Context Curation: Providing Helpful Context (and Avoiding Harmful Noise)\n\n**Relevant Context Injections:** The information an agent gets in its prompt (code context, design docs, examples) profoundly impacts its success. A key pattern is Retrieval-Augmented Code Generation (RACG), where the orchestrator fetches relevant snippets from the codebase or documentation to include in the agent's prompt. Real-world software tasks often require understanding cross-file dependencies and global architecture. Since an LLM has a limited context window, the orchestrator must be selective: for example, retrieving the function signature and related config file when asking the agent to implement a new feature in that function. A recent survey emphasizes that repository-level code generation needs to ensure global semantic consistency across files, and integrating a retrieval mechanism (e.g. searching a knowledge base of code) helps the LLM reason beyond a single file. In practice, tools like vector databases are used to find and feed the most relevant code pieces to the agent. Ellipsis's code review agents, for instance, use advanced code search backed by embeddings to find pertinent pieces of the codebase when analyzing a PR [@zenml2024ellipsis].\n\n**Curating Helpful Examples and Constraints:** Apart from code context, orchestrators often supply design documents, API usage examples, or style guides as context if they will guide the agent toward the right solution. Providing a few-shot example of a similar function or test can dramatically improve accuracy, as the agent can analogize from the given pattern. Additionally, context can include explicit constraints – e.g. a list of files that are in scope, or performance requirements. By giving the agent a clear problem statement along with these auxiliary materials, the orchestrator sets it up for success. For example, if generating a new module, the orchestrator might include an outline of the module's design or a simplified spec in the prompt. In PerfOrch (Performance-Oriented Orchestration), contextual knowledge about which LLMs perform best for a given language and problem type is stored and used to pick the right model for the task [@wang2025perforch]. This \"strategic memory\" of model strengths is a form of context that improves outcomes by dynamically selecting the optimal agent for each subtask.\n\n**Avoiding Context Overload:** Too much context can hurt agent performance. It might be tempting to provide every possibly relevant file to be safe, but large context size can confuse the model or dilute its focus. Recent studies have shown that even if all provided context is relevant, performance can degrade substantially (by 13–85%) as input length grows. In other words, beyond a certain point, the sheer volume of text can overwhelm the LLM's reasoning (\"context dilution\"). Thus, orchestrators should be judicious: include only the most relevant code and information. Irrelevant or contradictory context is even worse – it can lead the agent to draw false inferences or focus on the wrong problem. Best practices include using retrieval algorithms to filter context, and possibly summarizing or abstracting context to fit the window. For instance, rather than inserting an entire 500-line file, an orchestrator might insert a summary or the specific function signature needed. The Context-Augmentation pattern described by Hugging Face highlights that context expansion should be iterative and need-based: the agent identifies what extra info it needs, the orchestrator fetches it, and this cycle repeats, rather than dumping everything at once [@pujals2025agentic]. Techniques like embedding search, docstrings extraction, or code property graphs can help select which context truly matters.\n\n**Detecting and Removing Harmful Context:** Orchestrators may also implement \"context pruning\" – removing any information from the prompt that isn't consistent or necessary. This can include stale data (e.g. outdated function definitions if a newer version exists) or irrelevant conversation history in iterative settings. The goal is to minimize noise. If an agent will be doing a focused code edit, the orchestrator might strip out any unrelated instructions from the conversation and supply a fresh prompt with just the needed background. In summary, curated context is a double-edged sword: the right snippets and examples can greatly enhance success, but too much or wrong context can mislead the agent. Effective orchestration employs retrieval, filtering, and pruning to optimize context.\n\n## Prompt Engineering at the Orchestration Level\n\n**Structured Instructions vs. Plain Language:** Orchestration involves not only what tasks agents do, but how those tasks are described in prompts. A recurring pattern is to use structured, formal instructions rather than leaving everything to freeform natural language. For example, instead of saying \"Please write some code to do X\", an orchestrator-generated prompt might include a template: a step-by-step checklist, or a specific format like pseudo-code or JSON plan that the agent must follow. This structure can guide the model's output and reduce ambiguity. A concrete case is the self-planning approach: the orchestrator first asks the LLM to output a formatted plan (e.g. a numbered list of steps), and then feeds that plan into the next prompt for code generation [@jiang2024selfplanning]. By explicitly separating \"thinking\" from \"coding,\" the instructions become more systematic, leading to better results. Similarly, multi-agent frameworks often start with a role acknowledgment and task breakdown. In a four-agent setup using Claude (Architect, Builder, Validator, Scribe), each agent's system prompt begins with a role declaration and primary tasks in bullet form (e.g. \"You are Agent 1 – The Architect. Primary Tasks: requirements analysis, architecture planning…\"). This kind of structured prompt ensures each agent knows its exact scope and constraints.\n\n**Communicating Constraints Clearly:** The orchestration layer must convey any hard constraints or \"rules of engagement\" to the agent in the prompt. This can include: \"Do not modify code outside of Function A in file X\", \"Follow the project's style guide for naming\", or \"Make sure to run the provided tests and ensure they pass\". Explicitly listing files or sections not to touch can prevent collateral changes. However, LLMs may not always obey implicitly phrased constraints, so phrasing is important. Some systems adopt a manifest-driven approach, where the orchestrator provides a manifest of intended changes. For instance, the manifest might enumerate which functions to implement or which files to create, and the agent is instructed that any output will be checked against this manifest. A recent methodology called Manifest-Driven AI Development (MAID) takes this further by treating the manifest as an enforceable contract: after generation, a validator checks that only the declared files/functions were altered and no undeclared edits occurred. By front-loading these constraints (and later verifying them), the orchestrator greatly reduces the chance of the agent \"coloring outside the lines.\" Another way to impose structure is using tool-assisted prompting languages (like Microsoft's Guidance library) that allow templates with placeholders and even regex checks on the LLM's output, ensuring the agent adheres to formats.\n\n**Success Criteria and Framing:** The prompt should also frame what constitutes success. A good orchestration prompt might end with a clear success condition, e.g.: \"Your solution will be considered correct if all given unit tests pass and it meets the performance constraint of <2s runtime.\" By stating these criteria, the agent can self-evaluate its output against them, leading to better alignment with the goal. This is related to the Evaluator-Optimizer pattern [@carpintero2025evaluator], where the prompt or subsequent agent pass includes evaluation guidelines. For example, an orchestrator might instruct: \"First, implement the function. Then double-check that the function handles edge cases (null input, etc.) and meets the description. Only finalize if these criteria are satisfied.\" Such structured prompts push the agent to reflect on its answer against requirements, effectively building a spec into the prompt. Research by Anthropic also suggests that using systematic prompt patterns often outperforms ad-hoc prompting [@anthropic2024agents]. In their experience, many teams succeeded by \"simple, composable patterns\" in prompts (like stepwise reasoning or tool usage specifications) rather than overly open-ended prompts. In sum, the orchestration layer benefits from acting like a technical writer: providing the right scaffolding in the prompt (roles, steps, examples, constraints, and success checks) so the black-box agent is guided toward the desired outcome.\n\n**Example – Claude Subagents:** As a practical example, Claude's subagents feature allows configuring specialized system prompts for different sub-tasks. Each subagent has \"a custom system prompt that guides its behavior\" and a defined expertise. The main agent automatically delegates to these subagents when appropriate. In effect, the orchestrator (Claude's internal logic) appends the specialized prompt and context for that sub-task. This demonstrates prompt engineering at scale: rather than one giant prompt for everything, the orchestration chooses a tailored prompt per task type. Overall, structured and deliberate prompt engineering at the orchestration level — using instructions, role context, examples, and criteria — is key to maximizing agent reliability.\n\n## Parallelism & Conflict Avoidance\n\n**When to Run Agents in Parallel:** Orchestrating multiple agents in parallel can speed up development, but only if their tasks won't conflict. The Parallelization Workflow follows a divide-and-conquer approach: identify subtasks that are truly independent and execute them concurrently [@pujals2025agentic]. For example, an orchestrator might split a feature into frontend and backend implementations and assign each to a different agent at the same time. Another parallel pattern is \"voting\" – run multiple agents on the same task in parallel (perhaps with different prompting styles or different model seeds) and then choose the best result. In coding, this could mean generating multiple candidate solutions concurrently and later selecting one that passes tests (\"N-best\" approach). However, if parallel agents operate on shared artifacts (e.g. editing the same file), conflicts can arise. Thus, orchestrators should only parallelize tasks that either target separate components or use a strategy to reconcile outputs.\n\n**Locking and Work Queues:** A common strategy for conflict avoidance is file-level locking or resource locking. Before an agent begins work, the orchestrator can lock the files or modules it will touch, preventing other agents from editing them until completion. A community-built \"Claude code agent farm\" demonstrates this with a coordination protocol: each agent must create a lock file listing the files/features it intends to work on, and consult a shared active work registry to ensure no overlap. If a conflict is detected (e.g. two agents want to modify utils.py), one agent will wait (queue the task) until the file is free. This effectively serializes conflicting sections while still allowing non-conflicting tasks in parallel. The registry and lock files also help avoid duplicate work – agents check the completed task log so they don't redo something another agent already finished. Such locking is a conservative heuristic but very effective: it \"eliminates merge conflicts\" and \"prevents wasted work\" in concurrent development. The downside is that overly coarse locks (e.g. locking an entire project) nullify the speed benefit, so orchestrators must strike a balance by locking at a sensible granularity (file or feature level).\n\n**Advanced Conflict Resolution (CRDTs):** Recent research explores lock-free coordination using principles from distributed systems. CodeCRDT introduces an observation-driven approach where agents don't explicitly lock resources, but instead work on a shared state (a codebase) that automatically merges changes in a conflict-free manner [@pugachev2025codecrdt]. It uses Conflict-Free Replicated Data Types to ensure that as agents make edits, their updates converge deterministically without stepping on each other. In trials, this achieved 100% merge convergence with zero low-level conflicts, eliminating the need for manual conflict resolution. Essentially, each agent can edit concurrently and the CRDT mechanism integrates their changes (like Google Docs but for code). While CRDT-based orchestration can avoid the bottleneck of locks (which can become contention points as agent count N grows), it introduces complexity: agents must deal with semantic conflicts (e.g. two agents might implement the same feature in different ways, both syntactically merged but logically redundant). CodeCRDT reported about 5–10% of such higher-level conflicts, meaning an orchestrator or later review still has to reconcile duplicate or inconsistent implementations. Nonetheless, this approach is promising for truly scalable parallel agent teams, as it sidesteps the O(N×L) contention of lock-based systems.\n\n**Heuristics for Parallel Task Selection:** In practice, orchestrators often use simple heuristics to decide parallelism: e.g. no two agents on the same file or closely related module. Some systems use static analysis of code dependency graphs to ensure two parallel tasks don't overlap in call graphs or data models. Others rely on the developer to specify which tasks are independent. For example, an orchestrator might know that adding a new API endpoint (task A) is largely independent of refactoring the logging library (task B), so it schedules those simultaneously. If there is uncertainty, a conservative orchestrator will serialize tasks to avoid the risk of conflict. In the worst case that a conflict does occur (say two agents inadvertently changed the same file), the orchestrator must have a conflict-resolution step: it could auto-merge the code (if changes are in different regions), prefer one agent's output over the other, or fall back to human intervention to reconcile differences. Generally, the goal is conflict avoidance through design: split work such that each agent has its own domain of responsibility. As one user's experience with 4 parallel Claude agents showed, dividing roles by concern (architecture vs. coding vs. testing vs. docs) inherently reduced direct conflicts and even brought positive effects like built-in peer review. Each agent had a clear lane, enabling parallel development with quality checks due to the separation of concerns.\n\nIn summary, orchestrating parallel code agents requires careful coordination: lock or isolate shared resources, or adopt novel merging strategies, so that parallelism yields speed-up without chaos. When done right (e.g. with file locks or CRDTs), teams of agents can work \"like a well-oiled machine\" in parallel.\n\n## Verification & Acceptance Criteria\n\n**Automated Testing as Ground Truth:** One of the most powerful verification techniques is running tests on the agent's output. If the task comes with a test suite or at least some example cases, the orchestrator can automatically execute the generated code and check for correctness. This pattern is evident in many frameworks. AgentCoder is explicitly built around a test-feedback loop: a Test Designer agent generates unit tests, a Test Executor runs the code on those tests, and the Programmer agent then debugs or refines the code based on failures [@huang2023agentcoder]. This ensures that code isn't accepted as \"done\" until it passes its tests. Even single-agent approaches like OpenAI's Codex have employed this idea (often called execute-and-fix): run the code, and if an error or failing test is detected, prompt the model to fix it. The orchestrator's job is to automate this loop. In PerfOrch's multi-stage pipeline, after generation they perform a validation: if the code is functionally incorrect, it triggers the Fix Stage, invoking specialized bug-fixing models iteratively until a correct solution is found [@wang2025perforch]. Only once all tests pass does it proceed to the next stage. This demonstrates an automated acceptance gate – test passing is the criterion for success. Orchestrators can also measure other aspects like performance: PerfOrch's final Refine Stage checked if runtime improved and only accepted refinements that preserved correctness and met the performance target.\n\n**Code Reviews and LLM Critics:** Besides tests, another verification layer is reviewing the code, either by humans or by a second \"reviewer\" agent. Having a separate agent critique the generated code can catch issues that tests don't (e.g. poor readability, not following style, potential inefficiencies). A pattern often cited is dual-agent coding: one agent writes code, another agent reviews it. If the reviewer finds problems, the coder agent revises accordingly. This is essentially the Evaluator-Optimizer pattern applied to code [@carpintero2025evaluator] – where the Evaluator (which could be the same LLM or a different one tuned for critique) provides feedback, and the Optimizer (generator) uses it to improve the output in another pass. Academic and industry work supports this approach. In practice, companies like Meta have prototypes where an AI code generation is always followed by an AI code review, flagging issues before any human sees the code. Ellipsis's production system likewise emphasizes accuracy over latency: they run multiple comment-generation agents and then use a filtering pipeline (including an LLM-as-judge that evaluates the proposed comments) to ensure only high-quality, non-false-positive feedback is given [@zenml2024ellipsis]. By using an LLM judge on the outputs of LLM coders, they add a second layer of assurance.\n\n**Manifests and Static Verification:** As discussed in prompt engineering, if the orchestrator provided a manifest of expected changes, verification includes checking the agent's output against that manifest. Tools like MAID runner perform static analysis on the diff: did the agent only modify the allowed files and functions? Did it include the required new function? This kind of structural verification can be automated by parsing the output or performing AST (Abstract Syntax Tree) comparison between the original and modified code. If the agent deviated (e.g. changed something unrelated), the orchestrator can reject that output or prompt a correction (\"You edited file Y which was out of scope; please revert that.\"). Additionally, the orchestrator might enforce style guides by running linters or formatters on the code and ensuring no new warnings were introduced.\n\n**Acceptance and Quality Gates:** Complex orchestrations often define multi-dimensional acceptance criteria. Functional correctness (tests passing) is usually mandatory. Beyond that, style compliance, performance metrics, security scans, etc., can be gates. For example, after an agent's code passes tests, the orchestrator might also run a static security analyzer; if it flags issues, the agent's output isn't fully accepted and a fix task is spawned (or a human review is requested for that aspect). Hugging Face's guide suggests quality gates such as \"code passes all test suites\" as a primary stopping condition for an iterative refine loop – essentially, you keep refining until the code meets the quality bar [@pujals2025agentic]. They also note using a maximum iteration limit and similarity checks to avoid infinite loops.\n\n**Handling Partial Success:** Sometimes an agent's output is partially correct – e.g. it implements one feature but misses another, or fixes some bugs but not all. In these cases, orchestrators can take a pragmatic approach: accept the improvements made and then create follow-up tasks for the remaining work. This is akin to how a human might commit a partial fix and then address the rest. Orchestrators with tracking systems (like the completed_work_log in the agent farm) can mark certain sub-goals as done and only reassign what failed. If tests are granular, the orchestrator can rerun them and see which specific tests still fail, then prompt the agent specifically about those failures on the next try. This targeted retry approach zeroes in on the unresolved issues rather than redoing everything.\n\n**Automated vs Human Acceptance:** Ultimately, the output may either be auto-merged or require human approval. Many real-world setups keep a human-in-the-loop for final approval – for example, an orchestrator might open a pull request with the agent's changes and require a human developer to review and merge. The orchestrator in this case hands off to a human at the final step. Another checkpoint is when an agent is uncertain or stuck. If an agent expresses confusion (some advanced agents might output a special token or message like \"I am not sure how to proceed\"), the orchestrator should detect that and involve a human to clarify requirements or provide a hint.\n\nIn summary, verification in agent orchestration is about establishing feedback loops and safety nets: tests to verify correctness, secondary agents or tools to verify quality, and structured checks (manifests, linters) to enforce constraints. By combining these, orchestrators can confidently determine when an agent \"succeeded\" and its output is acceptable.\n\n## Failure Modes & Recovery Strategies\n\nEven with the best planning, agents will sometimes fail – producing wrong code, no code, or even crashing. The orchestration layer must be resilient to these failures and have recovery strategies.\n\n**Graceful Retries:** The simplest recovery pattern is retrying the same prompt. Because LLM agents are stochastic, a second attempt might yield a different (possibly correct) result. Orchestrators often implement a retry-on-failure (with a limit on retries to avoid loops). If an agent times out or returns an error (e.g. if using an API that errors out), the orchestrator can automatically retry after a short delay or with a smaller context. The Claude agent farm tool, for example, monitors each agent's health and will auto-restart an agent if it crashes or gets stuck. This ensures that a transient glitch doesn't halt the entire workflow.\n\n**Refining the Prompt or Task Breakdown:** If a direct retry doesn't help, the orchestrator should try a modified approach. One common strategy is progressive prompting: if the agent failed to produce output, perhaps the instructions were too broad – so break the task into smaller sub-tasks and prompt those instead. For instance, if \"implement the foobar algorithm\" came back empty or incorrect, the orchestrator might break it into \"step 1: implement the helper function X\" and \"step 2: implement main foobar using X\". Alternatively, the orchestrator can supply more guidance in the prompt on a retry – e.g. provide a partial solution or additional hints. This resembles a fallback to a more guided prompt when the agent seems to be stuck with a high-level request. In the Prompt-Chaining considerations, it's suggested to re-prompt including details of the failure in the context if an intermediate step fails [@pujals2025agentic]. For example: \"The previous attempt resulted in a runtime error at line 20. Here is the error log… Please fix the code to handle this issue.\" This gives the agent new information to succeed on the next pass.\n\n**Alternate Agents or Models:** Another recovery pattern is trying a different agent or model. If one model repeatedly fails to solve a problem, perhaps a more powerful model (or one fine-tuned differently) could succeed. PerfOrch demonstrates this by maintaining a ranking of models for each stage – if the top choice fails, it falls back to the next best model for that task [@wang2025perforch]. In their bug-fixing stage, they iteratively invoked up to n different specialized LLMs until one fixed the bug. Similarly, if an agent like GPT-3.5 is not producing a solution, an orchestrator might escalate to GPT-4 for that task. This is a cost-performance trade-off: use cheaper models for most tasks, but have a path to involve a more capable (or simply different) model on failure.\n\n**Preserving Partial Progress:** In complex multi-step tasks, an agent might accomplish some steps successfully before failing at a later step. A robust orchestrator will preserve this partial progress so it's not lost. For example, if an agent wrote 80% of a file correctly and then struggled, the orchestrator can save the file (or keep it in memory) and instruct a subsequent agent to continue from that state. This could involve checking the agent's output for usable sections. Some frameworks treat the codebase like a persistent state that agents incrementally build – even if one agent stops mid-way, the next agent sees the codebase with whatever was added so far.\n\n**Escalating to Human or Halting:** Not all failures can or should be handled autonomously. A critical safety mechanism is to escalate to a human when automated retries are exhausted or when the cost of more retries outweighs the benefit. The orchestrator might, for instance, make 3 attempts at a task (each possibly with refined prompts or different models). If none succeed, it can stop and label the task for human attention. In a CI/CD integration, this could mean leaving a comment like \"AI agent could not resolve this issue – please review manually.\"\n\n**Logging and Learning from Failures:** Another aspect of handling failures is logging them for continuous improvement. The orchestrator should log error cases, so developers can later analyze if prompt tweaks or training could solve them. In some advanced setups, this log is fed into a \"reflexion\" agent that tries to learn from failures by adjusting future prompts.\n\nIn essence, failure recovery in orchestration is about resilience and adaptability. Try again (maybe differently), switch strategies if needed, and know when to ask for help.\n\n## Multi-Pass Architectures (Planner, Implementer, Reviewer, etc.)\n\nOne of the most powerful design patterns emerging in AI code generation is the use of multiple passes or multiple agent roles to tackle a problem in stages. Instead of a single monolithic prompt-to-code step, the task is divided into phases such as planning, implementing, and reviewing.\n\n**Planner → Implementer Workflow:** In this pattern, a Planner agent first creates a high-level plan or design, which is then used by an Implementer agent to write the actual code. The planner might outline which functions need to be created, what the approach will be, or even pseudo-code. The implementer then follows this blueprint. The self-planning study exemplifies this: the LLM generates concise, structured planning steps from the intent, then in the implementation phase it \"generates code step by step, guided by the preceding planning steps\" [@jiang2024selfplanning]. This two-pass approach yielded significantly higher correctness than a single-pass solution, because the model effectively did some problem decomposition and reasoning out loud before coding. MapCoder extends this idea to four stages: it uses one agent to recall relevant examples, another to plan the solution approach, a third to generate code, and a fourth to debug the result [@islam2024mapcoder]. Each agent is specialized for its role, and together they \"replicate the full cycle of program synthesis as observed in human developers\". The results were state-of-the-art on code benchmarks, underlining the efficacy of a multi-pass pipeline.\n\n**Reviewer or Refiner Pass:** Another common multi-pass setup is having a Reviewer agent or second pass that checks and improves the code written by an Implementer (first pass). In a multi-pass architecture, this is built-in: after code generation, the process automatically goes to a review phase. The reviewer might be tasked with ensuring the code meets the spec and then either directly editing the code to fix issues or providing feedback for the implementer to act on. This can even become an iterative loop: implementer → reviewer → implementer (for revisions) → reviewer … until the reviewer is satisfied. This approach aligns with the Evaluator-Optimizer iterative refinement pattern [@carpintero2025evaluator].\n\n**When Multi-Pass Helps:** Multi-pass shines especially for complex tasks that benefit from explicit reasoning or quality control. If a coding task requires sophisticated reasoning (algorithm design, complex logic), a planning pass helps. If it requires high assurance (safety-critical code, or just code in a large codebase where consistency matters), a review/testing pass helps. On simpler tasks (e.g. \"add a print statement\"), multi-pass might be overkill – a single agent can handle it and the overhead of planning/reviewing might outweigh the simplicity of the change.\n\n**Feedback Loops:** Multi-pass architectures inherently create a feedback loop, especially with a reviewer/corrector in the mix. The orchestrator should feed the feedback in a form the implementer can act on. For example, if a reviewer says \"Function foo doesn't handle negative inputs properly,\" the orchestrator might prompt the implementer agent with that exact feedback: \"The code was reviewed and the following issue was found: it doesn't handle negative inputs. Please update the code to address this.\"\n\n## Human-in-the-Loop in Orchestration\n\nEven with all the above automation, the human element remains important. Orchestration should consider when to involve human developers or stakeholders to guide or approve the process.\n\n**Checkpoints for Human Input:** A well-designed orchestrator will define certain points at which it pauses and seeks human input. This might be after planning – e.g., once a Planner agent produces a design, the orchestrator could show that plan to a human for approval before coding. It could also be after code generation but before acceptance – essentially treating the AI-generated code like a regular code review that a human must OK.\n\n**Surviving \"I'm stuck\" vs \"I'm done\":** Differentiating between an agent that believes it's finished vs one that gave up is key. If an agent says \"Solution complete\" but the tests are failing, clearly it was wrongly done. The orchestrator should treat that as a failure (and possibly loop in a reviewer agent or human). On the other hand, if after several tries the agent outputs, \"I cannot find an approach for this problem,\" then the orchestrator has an impasse. This is a prime moment for human-in-the-loop.\n\n**Human Oversight and Approval:** Humans may also impose guardrails by setting policies the orchestrator follows. For example, an organization might require that any code touching security-critical files must be reviewed by a human, no matter what. The orchestrator can have a rule: if task involves auth.py (or generally high-risk area), always assign it to a human or at least get human approval after the AI suggests changes.\n\n**Balancing Autonomy and Oversight:** The goal of orchestration is often to minimize human effort, but not to eliminate it entirely at the cost of quality. An insightful guideline from Anthropic is to use the simplest solution possible and only add complexity (autonomy) when needed [@anthropic2024agents]. This suggests that if a task can be handled with a single LLM call plus a quick human tweak, that might be preferable to spinning up a whole autonomous multi-agent system.\n\n## Conclusion\n\nOrchestrating black-box code agents is an exercise in applying sound software engineering principles to AI-driven automation. The patterns identified – from task assignment and context curation to multi-pass workflows and human oversight – collectively form a toolkit for reliable AI-assisted development.\n\nIn designing an orchestration framework, one should treat AI agents as fallible collaborators that need structure and guidance – much like human junior developers. The orchestrator's role is part project manager, part version-control system, and part QA engineer for this AI \"team.\" It decides who does what (task assignment), gives them what they need (context), tells them how to do it (prompt engineering and constraints), lets them work together safely (parallelism control), checks their work (verification), handles when they get it wrong (failure recovery), encourages iterative improvement (multi-pass), and knows when to ask a senior (human-in-loop).\n\n## References\n","source_path":"research/orchestration-survey.md"},{"slug":"context-injection","title":"Context Injection","content_html":"<h1 id=\"context-injection\">Context Injection<a class=\"heading-anchor\" href=\"#context-injection\" aria-label=\"Link to heading\">#</a></h1>\n<p>What information noface provides to agents and how.</p>\n<h2 id=\"current-design\">Current Design<a class=\"heading-anchor\" href=\"#current-design\" aria-label=\"Link to heading\">#</a></h2>\n<p>The implementation prompt includes:</p>\n<ol>\n<li><strong>Issue description</strong> — from beads</li>\n<li><strong>Manifest</strong> — which files the agent can touch</li>\n<li><strong>Build/test commands</strong> — from <code>.noface.toml</code></li>\n<li><strong>Design docs</strong> — fetched from monowiki if configured</li>\n<li><strong>Workflow instructions</strong> — step-by-step process (implement → test → commit)</li>\n</ol>\n<p>Context is capped by <code>max_context_docs</code> setting (default 5).</p>\n<h2 id=\"relation-to-survey\">Relation to Survey<a class=\"heading-anchor\" href=\"#relation-to-survey\" aria-label=\"Link to heading\">#</a></h2>\n<p>The survey warns about <strong>context dilution</strong>:</p>\n<blockquote>\n<p>\"Recent studies have shown that even if all provided context is relevant, performance can degrade substantially (by 13–85%) as input length grows.\"</p>\n</blockquote>\n<p>And recommends <strong>iterative context expansion</strong>:</p>\n<blockquote>\n<p>\"The Context-Augmentation pattern described by Hugging Face highlights that context expansion should be iterative and need-based: the agent identifies what extra info it needs, the orchestrator fetches it.\"</p>\n</blockquote>\n<h2 id=\"open-questions\">Open Questions<a class=\"heading-anchor\" href=\"#open-questions\" aria-label=\"Link to heading\">#</a></h2>\n<ol>\n<li>\n<p><strong>Context Budget</strong> — Is <code>max_context_docs = 5</code> the right number? Should it be dynamic based on issue complexity?</p>\n</li>\n<li>\n<p><strong>Relevance Ranking</strong> — Currently we fetch docs by wikilink or search. Should we score/rank by semantic relevance? Embeddings?</p>\n</li>\n<li>\n<p><strong>Code Context</strong> — Should we inject relevant source files? Which ones? The survey mentions BM25 search over codebase (noface has <code>src/bm25.zig</code>).</p>\n</li>\n<li>\n<p><strong>Context Freshness</strong> — If a doc is stale (refers to old API), it could mislead the agent. How do we detect/handle this?</p>\n</li>\n<li>\n<p><strong>Negative Context</strong> — What explicitly <em>shouldn't</em> be included? Large generated files? Vendored deps? How do we filter?</p>\n</li>\n<li>\n<p><strong>Agent-Requested Context</strong> — Should agents be able to ask for more context mid-run? Or does the harness (Claude Code) handle this internally?</p>\n</li>\n</ol>\n<h2 id=\"implementation-notes\">Implementation Notes<a class=\"heading-anchor\" href=\"#implementation-notes\" aria-label=\"Link to heading\">#</a></h2>\n<p>See <code>src/loop.zig:buildImplementationPrompt</code> and <code>src/monowiki.zig</code>.</p>\n","frontmatter":{"title":"Context Injection","description":null,"summary":null,"date":null,"type":"essay","tags":["design","context","prompts","monowiki"],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":["design","context","prompts","monowiki"],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":[],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#context-injection\">Context Injection</a></li><li class=\"toc-level-2\"><a href=\"#current-design\">Current Design</a></li><li class=\"toc-level-2\"><a href=\"#relation-to-survey\">Relation to Survey</a></li><li class=\"toc-level-2\"><a href=\"#open-questions\">Open Questions</a></li><li class=\"toc-level-2\"><a href=\"#implementation-notes\">Implementation Notes</a></li></ul></nav>","raw_body":"# Context Injection\n\nWhat information noface provides to agents and how.\n\n## Current Design\n\nThe implementation prompt includes:\n\n1. **Issue description** — from beads\n2. **Manifest** — which files the agent can touch\n3. **Build/test commands** — from `.noface.toml`\n4. **Design docs** — fetched from monowiki if configured\n5. **Workflow instructions** — step-by-step process (implement → test → commit)\n\nContext is capped by `max_context_docs` setting (default 5).\n\n## Relation to Survey\n\nThe survey warns about **context dilution**:\n\n> \"Recent studies have shown that even if all provided context is relevant, performance can degrade substantially (by 13–85%) as input length grows.\"\n\nAnd recommends **iterative context expansion**:\n\n> \"The Context-Augmentation pattern described by Hugging Face highlights that context expansion should be iterative and need-based: the agent identifies what extra info it needs, the orchestrator fetches it.\"\n\n## Open Questions\n\n1. **Context Budget** — Is `max_context_docs = 5` the right number? Should it be dynamic based on issue complexity?\n\n2. **Relevance Ranking** — Currently we fetch docs by wikilink or search. Should we score/rank by semantic relevance? Embeddings?\n\n3. **Code Context** — Should we inject relevant source files? Which ones? The survey mentions BM25 search over codebase (noface has `src/bm25.zig`).\n\n4. **Context Freshness** — If a doc is stale (refers to old API), it could mislead the agent. How do we detect/handle this?\n\n5. **Negative Context** — What explicitly *shouldn't* be included? Large generated files? Vendored deps? How do we filter?\n\n6. **Agent-Requested Context** — Should agents be able to ask for more context mid-run? Or does the harness (Claude Code) handle this internally?\n\n## Implementation Notes\n\nSee `src/loop.zig:buildImplementationPrompt` and `src/monowiki.zig`.\n","source_path":"design/context-injection.md"},{"slug":"parallel-execution","title":"Parallel Execution","content_html":"<h1 id=\"parallel-execution\">Parallel Execution<a class=\"heading-anchor\" href=\"#parallel-execution\" aria-label=\"Link to heading\">#</a></h1>\n<p>How noface runs multiple agents concurrently without conflicts.</p>\n<h2 id=\"current-design\">Current Design<a class=\"heading-anchor\" href=\"#current-design\" aria-label=\"Link to heading\">#</a></h2>\n<ol>\n<li><strong>Planner pass</strong> groups issues into batches based on manifest analysis</li>\n<li>Issues in the same batch have <strong>disjoint primary_files</strong> (no overlap)</li>\n<li><strong>Worker pool</strong> spawns up to N parallel processes (configurable, default 8)</li>\n<li><strong>Lock entries</strong> track which files are held by which worker</li>\n<li>Batches execute sequentially; issues within a batch execute in parallel</li>\n</ol>\n<h2 id=\"relation-to-survey\">Relation to Survey<a class=\"heading-anchor\" href=\"#relation-to-survey\" aria-label=\"Link to heading\">#</a></h2>\n<p>This implements the <strong>file-level locking</strong> pattern from <a href=\"/orchestration-survey.html\">orchestration-survey</a>:</p>\n<blockquote>\n<p>\"A community-built 'Claude code agent farm' demonstrates this with a coordination protocol: each agent must create a lock file listing the files/features it intends to work on, and consult a shared active work registry to ensure no overlap.\"</p>\n</blockquote>\n<p>The survey also discusses <strong>CRDTs</strong> for lock-free coordination (CodeCRDT achieved 100% merge convergence). This is more scalable but more complex.</p>\n<h2 id=\"open-questions\">Open Questions<a class=\"heading-anchor\" href=\"#open-questions\" aria-label=\"Link to heading\">#</a></h2>\n<ol>\n<li>\n<p><strong>Lock Granularity</strong> — File-level prevents conflicts but may be too conservative. Two agents could safely edit different functions in the same file. Is function-level locking worth the complexity?</p>\n</li>\n<li>\n<p><strong>Batch Sizing</strong> — How do we decide how many issues to batch together? More parallelism = faster, but also more resource contention and harder debugging.</p>\n</li>\n<li>\n<p><strong>Lock Contention</strong> — With many issues, popular files (e.g., <code>main.zig</code>) become bottlenecks. How do we detect and mitigate hot files?</p>\n</li>\n<li>\n<p><strong>Conflict Recovery</strong> — If a conflict somehow occurs (manifest was wrong), what's the resolution strategy? Currently undefined.</p>\n</li>\n<li>\n<p><strong>CRDT Exploration</strong> — Is there a simpler middle ground between locks and full CRDTs? AST-aware merging?</p>\n</li>\n</ol>\n<h2 id=\"implementation-notes\">Implementation Notes<a class=\"heading-anchor\" href=\"#implementation-notes\" aria-label=\"Link to heading\">#</a></h2>\n<p>See <code>src/worker_pool.zig:WorkerPool</code> and <code>src/state.zig:LockEntry</code>.</p>\n","frontmatter":{"title":"Parallel Execution","description":null,"summary":null,"date":null,"type":"essay","tags":["design","parallelism","batching","locking"],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":["design","parallelism","batching","locking"],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":["orchestration-survey"],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#parallel-execution\">Parallel Execution</a></li><li class=\"toc-level-2\"><a href=\"#current-design\">Current Design</a></li><li class=\"toc-level-2\"><a href=\"#relation-to-survey\">Relation to Survey</a></li><li class=\"toc-level-2\"><a href=\"#open-questions\">Open Questions</a></li><li class=\"toc-level-2\"><a href=\"#implementation-notes\">Implementation Notes</a></li></ul></nav>","raw_body":"# Parallel Execution\n\nHow noface runs multiple agents concurrently without conflicts.\n\n## Current Design\n\n1. **Planner pass** groups issues into batches based on manifest analysis\n2. Issues in the same batch have **disjoint primary_files** (no overlap)\n3. **Worker pool** spawns up to N parallel processes (configurable, default 8)\n4. **Lock entries** track which files are held by which worker\n5. Batches execute sequentially; issues within a batch execute in parallel\n\n## Relation to Survey\n\nThis implements the **file-level locking** pattern from [[orchestration-survey]]:\n\n> \"A community-built 'Claude code agent farm' demonstrates this with a coordination protocol: each agent must create a lock file listing the files/features it intends to work on, and consult a shared active work registry to ensure no overlap.\"\n\nThe survey also discusses **CRDTs** for lock-free coordination (CodeCRDT achieved 100% merge convergence). This is more scalable but more complex.\n\n## Open Questions\n\n1. **Lock Granularity** — File-level prevents conflicts but may be too conservative. Two agents could safely edit different functions in the same file. Is function-level locking worth the complexity?\n\n2. **Batch Sizing** — How do we decide how many issues to batch together? More parallelism = faster, but also more resource contention and harder debugging.\n\n3. **Lock Contention** — With many issues, popular files (e.g., `main.zig`) become bottlenecks. How do we detect and mitigate hot files?\n\n4. **Conflict Recovery** — If a conflict somehow occurs (manifest was wrong), what's the resolution strategy? Currently undefined.\n\n5. **CRDT Exploration** — Is there a simpler middle ground between locks and full CRDTs? AST-aware merging?\n\n## Implementation Notes\n\nSee `src/worker_pool.zig:WorkerPool` and `src/state.zig:LockEntry`.\n","source_path":"design/parallel-execution.md"},{"slug":"failure-recovery","title":"Failure Recovery","content_html":"<h1 id=\"failure-recovery\">Failure Recovery<a class=\"heading-anchor\" href=\"#failure-recovery\" aria-label=\"Link to heading\">#</a></h1>\n<p>How noface handles agent failures.</p>\n<h2 id=\"current-design\">Current Design<a class=\"heading-anchor\" href=\"#current-design\" aria-label=\"Link to heading\">#</a></h2>\n<p>noface handles several failure modes:</p>\n<ol>\n<li><strong>Transient failures</strong> (429, 5xx, network) — retry up to 3x with exponential backoff</li>\n<li><strong>Manifest violations</strong> — rollback offending files, retry with stricter prompt</li>\n<li><strong>Timeouts</strong> (no output for N seconds) — break down issue into smaller tasks</li>\n<li><strong>Crash recovery</strong> — on startup, detect in-progress work from previous run, reset stale locks, restore state</li>\n</ol>\n<p>Each attempt is recorded in state with outcome (success/failed/timeout/violation).</p>\n<h2 id=\"relation-to-survey\">Relation to Survey<a class=\"heading-anchor\" href=\"#relation-to-survey\" aria-label=\"Link to heading\">#</a></h2>\n<p>The survey describes several recovery patterns:</p>\n<p><strong>Graceful retries:</strong></p>\n<blockquote>\n<p>\"The simplest recovery pattern is retrying the same prompt. Because LLM agents are stochastic, a second attempt might yield a different (possibly correct) result.\"</p>\n</blockquote>\n<p><strong>Progressive prompting:</strong></p>\n<blockquote>\n<p>\"If a direct retry doesn't help, the orchestrator should try a modified approach... break the task into smaller sub-tasks and prompt those instead.\"</p>\n</blockquote>\n<p><strong>Preserving partial progress:</strong></p>\n<blockquote>\n<p>\"A robust orchestrator will preserve this partial progress so it's not lost.\"</p>\n</blockquote>\n<h2 id=\"open-questions\">Open Questions<a class=\"heading-anchor\" href=\"#open-questions\" aria-label=\"Link to heading\">#</a></h2>\n<ol>\n<li>\n<p><strong>Retry Strategies</strong> — Current retry is simple backoff. Should we modify the prompt on retry? Add more context? Summarize the failure?</p>\n</li>\n<li>\n<p><strong>Task Breakdown</strong> — Timeout triggers breakdown, but how does noface actually break down an issue? Is this implemented or aspirational?</p>\n</li>\n<li>\n<p><strong>Partial Progress</strong> — If an agent edits 3 files correctly but fails on the 4th, do we keep the 3? Currently unclear.</p>\n</li>\n<li>\n<p><strong>Model Escalation</strong> — The survey mentions falling back to more powerful models. Should noface try a different model on failure?</p>\n</li>\n<li>\n<p><strong>Failure Classification</strong> — Not all failures are equal. \"Syntax error\" vs \"doesn't understand the task\" vs \"timeout\" need different responses.</p>\n</li>\n<li>\n<p><strong>Human Escalation</strong> — After N failures, should noface pause and ask for help? Current behavior is unclear.</p>\n</li>\n<li>\n<p><strong>Learning from Failures</strong> — Should failure patterns inform future prompts? \"This type of issue often fails, add extra guidance.\"</p>\n</li>\n</ol>\n<h2 id=\"implementation-notes\">Implementation Notes<a class=\"heading-anchor\" href=\"#implementation-notes\" aria-label=\"Link to heading\">#</a></h2>\n<p>See <code>src/loop.zig:runIteration</code> retry logic and <code>src/state.zig:IssueState</code>.</p>\n","frontmatter":{"title":"Failure Recovery","description":null,"summary":null,"date":null,"type":"essay","tags":["design","failure","retry","recovery"],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":["design","failure","retry","recovery"],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":[],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#failure-recovery\">Failure Recovery</a></li><li class=\"toc-level-2\"><a href=\"#current-design\">Current Design</a></li><li class=\"toc-level-2\"><a href=\"#relation-to-survey\">Relation to Survey</a></li><li class=\"toc-level-2\"><a href=\"#open-questions\">Open Questions</a></li><li class=\"toc-level-2\"><a href=\"#implementation-notes\">Implementation Notes</a></li></ul></nav>","raw_body":"# Failure Recovery\n\nHow noface handles agent failures.\n\n## Current Design\n\nnoface handles several failure modes:\n\n1. **Transient failures** (429, 5xx, network) — retry up to 3x with exponential backoff\n2. **Manifest violations** — rollback offending files, retry with stricter prompt\n3. **Timeouts** (no output for N seconds) — break down issue into smaller tasks\n4. **Crash recovery** — on startup, detect in-progress work from previous run, reset stale locks, restore state\n\nEach attempt is recorded in state with outcome (success/failed/timeout/violation).\n\n## Relation to Survey\n\nThe survey describes several recovery patterns:\n\n**Graceful retries:**\n> \"The simplest recovery pattern is retrying the same prompt. Because LLM agents are stochastic, a second attempt might yield a different (possibly correct) result.\"\n\n**Progressive prompting:**\n> \"If a direct retry doesn't help, the orchestrator should try a modified approach... break the task into smaller sub-tasks and prompt those instead.\"\n\n**Preserving partial progress:**\n> \"A robust orchestrator will preserve this partial progress so it's not lost.\"\n\n## Open Questions\n\n1. **Retry Strategies** — Current retry is simple backoff. Should we modify the prompt on retry? Add more context? Summarize the failure?\n\n2. **Task Breakdown** — Timeout triggers breakdown, but how does noface actually break down an issue? Is this implemented or aspirational?\n\n3. **Partial Progress** — If an agent edits 3 files correctly but fails on the 4th, do we keep the 3? Currently unclear.\n\n4. **Model Escalation** — The survey mentions falling back to more powerful models. Should noface try a different model on failure?\n\n5. **Failure Classification** — Not all failures are equal. \"Syntax error\" vs \"doesn't understand the task\" vs \"timeout\" need different responses.\n\n6. **Human Escalation** — After N failures, should noface pause and ask for help? Current behavior is unclear.\n\n7. **Learning from Failures** — Should failure patterns inform future prompts? \"This type of issue often fails, add extra guidance.\"\n\n## Implementation Notes\n\nSee `src/loop.zig:runIteration` retry logic and `src/state.zig:IssueState`.\n","source_path":"design/failure-recovery.md"},{"slug":"manifests","title":"File Manifests","content_html":"<h1 id=\"file-manifests\">File Manifests<a class=\"heading-anchor\" href=\"#file-manifests\" aria-label=\"Link to heading\">#</a></h1>\n<p>How noface controls what files each agent can touch.</p>\n<h2 id=\"current-design\">Current Design<a class=\"heading-anchor\" href=\"#current-design\" aria-label=\"Link to heading\">#</a></h2>\n<p>Manifests declare three access levels per issue:</p>\n<ul>\n<li><strong>PRIMARY_FILES</strong> — exclusive write access (locked during execution)</li>\n<li><strong>READ_FILES</strong> — shared read-only access</li>\n<li><strong>FORBIDDEN_FILES</strong> — must never be touched</li>\n</ul>\n<p>The planner generates manifests by analyzing each issue. After an agent completes, noface runs <code>git diff</code> and verifies compliance. Violations trigger rollback of offending files and retry with a stricter prompt.</p>\n<h2 id=\"relation-to-survey\">Relation to Survey<a class=\"heading-anchor\" href=\"#relation-to-survey\" aria-label=\"Link to heading\">#</a></h2>\n<p>This implements the <strong>Manifest-Driven AI Development (MAID)</strong> pattern from <a href=\"/orchestration-survey.html\">orchestration-survey</a>:</p>\n<blockquote>\n<p>\"A recent methodology called Manifest-Driven AI Development (MAID) takes this further by treating the manifest as an enforceable contract: after generation, a validator checks that only the declared files/functions were altered and no undeclared edits occurred.\"</p>\n</blockquote>\n<h2 id=\"open-questions\">Open Questions<a class=\"heading-anchor\" href=\"#open-questions\" aria-label=\"Link to heading\">#</a></h2>\n<ol>\n<li>\n<p><strong>Granularity</strong> — File-level is coarse. Should we support function-level or line-range manifests?</p>\n</li>\n<li>\n<p><strong>Manifest Generation</strong> — How good is the planner at predicting which files an issue will touch? What's the false-negative rate (files needed but not declared)?</p>\n</li>\n<li>\n<p><strong>Manifest Violations</strong> — Current behavior is rollback + retry. Should we ever accept a violation if the change is clearly correct?</p>\n</li>\n<li>\n<p><strong>Dynamic Expansion</strong> — Should agents be able to request additional files mid-execution? Or does that break the isolation model?</p>\n</li>\n</ol>\n<h2 id=\"implementation-notes\">Implementation Notes<a class=\"heading-anchor\" href=\"#implementation-notes\" aria-label=\"Link to heading\">#</a></h2>\n<p>See <code>src/state.zig:Manifest</code> and <code>src/loop.zig:verifyManifestCompliance</code>.</p>\n","frontmatter":{"title":"File Manifests","description":null,"summary":null,"date":null,"type":"essay","tags":["design","manifests","access-control"],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":["design","manifests","access-control"],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":["orchestration-survey"],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#file-manifests\">File Manifests</a></li><li class=\"toc-level-2\"><a href=\"#current-design\">Current Design</a></li><li class=\"toc-level-2\"><a href=\"#relation-to-survey\">Relation to Survey</a></li><li class=\"toc-level-2\"><a href=\"#open-questions\">Open Questions</a></li><li class=\"toc-level-2\"><a href=\"#implementation-notes\">Implementation Notes</a></li></ul></nav>","raw_body":"# File Manifests\n\nHow noface controls what files each agent can touch.\n\n## Current Design\n\nManifests declare three access levels per issue:\n\n- **PRIMARY_FILES** — exclusive write access (locked during execution)\n- **READ_FILES** — shared read-only access\n- **FORBIDDEN_FILES** — must never be touched\n\nThe planner generates manifests by analyzing each issue. After an agent completes, noface runs `git diff` and verifies compliance. Violations trigger rollback of offending files and retry with a stricter prompt.\n\n## Relation to Survey\n\nThis implements the **Manifest-Driven AI Development (MAID)** pattern from [[orchestration-survey]]:\n\n> \"A recent methodology called Manifest-Driven AI Development (MAID) takes this further by treating the manifest as an enforceable contract: after generation, a validator checks that only the declared files/functions were altered and no undeclared edits occurred.\"\n\n## Open Questions\n\n1. **Granularity** — File-level is coarse. Should we support function-level or line-range manifests?\n\n2. **Manifest Generation** — How good is the planner at predicting which files an issue will touch? What's the false-negative rate (files needed but not declared)?\n\n3. **Manifest Violations** — Current behavior is rollback + retry. Should we ever accept a violation if the change is clearly correct?\n\n4. **Dynamic Expansion** — Should agents be able to request additional files mid-execution? Or does that break the isolation model?\n\n## Implementation Notes\n\nSee `src/state.zig:Manifest` and `src/loop.zig:verifyManifestCompliance`.\n","source_path":"design/manifests.md"},{"slug":"verification","title":"Verification","content_html":"<h1 id=\"verification\">Verification<a class=\"heading-anchor\" href=\"#verification\" aria-label=\"Link to heading\">#</a></h1>\n<p>How noface determines whether an agent succeeded.</p>\n<h2 id=\"current-design\">Current Design<a class=\"heading-anchor\" href=\"#current-design\" aria-label=\"Link to heading\">#</a></h2>\n<p>noface uses multiple verification layers:</p>\n<ol>\n<li><strong>Test execution</strong> — runs the configured test command; failure = not done</li>\n<li><strong>Manifest compliance</strong> — <code>git diff</code> checked against declared files; violations = rollback</li>\n<li><strong>Build check</strong> — runs the configured build command (implicit in agent workflow)</li>\n</ol>\n<p>The agent is instructed to self-verify (run tests, check output) before committing.</p>\n<h2 id=\"relation-to-survey\">Relation to Survey<a class=\"heading-anchor\" href=\"#relation-to-survey\" aria-label=\"Link to heading\">#</a></h2>\n<p>The survey emphasizes <strong>automated testing as ground truth</strong>:</p>\n<blockquote>\n<p>\"This ensures that code isn't accepted as 'done' until it passes its tests. Even single-agent approaches like OpenAI's Codex have employed this idea (often called execute-and-fix): run the code, and if an error or failing test is detected, prompt the model to fix it.\"</p>\n</blockquote>\n<p>And <strong>manifest verification</strong>:</p>\n<blockquote>\n<p>\"Tools like MAID runner perform static analysis on the diff: did the agent only modify the allowed files and functions?\"</p>\n</blockquote>\n<p>The survey also discusses <strong>LLM critics</strong> as an additional layer — a second agent that reviews the code.</p>\n<h2 id=\"open-questions\">Open Questions<a class=\"heading-anchor\" href=\"#open-questions\" aria-label=\"Link to heading\">#</a></h2>\n<ol>\n<li>\n<p><strong>Test Coverage</strong> — What if the tests don't cover the change? Agent could introduce a bug that passes existing tests.</p>\n</li>\n<li>\n<p><strong>Review Pass</strong> — Should noface run a dedicated reviewer agent on each change before accepting? Cost vs. benefit?</p>\n</li>\n<li>\n<p><strong>Static Analysis</strong> — Should we run linters, type checkers, security scanners as additional gates?</p>\n</li>\n<li>\n<p><strong>Semantic Verification</strong> — Tests check behavior, manifests check scope. But what about \"did the agent actually address the issue?\" Sometimes code passes tests but doesn't solve the problem.</p>\n</li>\n<li>\n<p><strong>Confidence Signals</strong> — Can we detect low-confidence completions? Agent says \"done\" but hedges in comments? Unusual patterns?</p>\n</li>\n<li>\n<p><strong>Partial Acceptance</strong> — If 2 of 3 acceptance criteria pass, do we accept partially? Or all-or-nothing?</p>\n</li>\n<li>\n<p><strong>Human Review Gate</strong> — For high-risk changes (security, config), should noface require human approval?</p>\n</li>\n</ol>\n<h2 id=\"implementation-notes\">Implementation Notes<a class=\"heading-anchor\" href=\"#implementation-notes\" aria-label=\"Link to heading\">#</a></h2>\n<p>See <code>src/loop.zig:verifyManifestCompliance</code> and prompt instructions for self-testing.</p>\n","frontmatter":{"title":"Verification","description":null,"summary":null,"date":null,"type":"essay","tags":["design","verification","testing","acceptance"],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":["design","verification","testing","acceptance"],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":[],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#verification\">Verification</a></li><li class=\"toc-level-2\"><a href=\"#current-design\">Current Design</a></li><li class=\"toc-level-2\"><a href=\"#relation-to-survey\">Relation to Survey</a></li><li class=\"toc-level-2\"><a href=\"#open-questions\">Open Questions</a></li><li class=\"toc-level-2\"><a href=\"#implementation-notes\">Implementation Notes</a></li></ul></nav>","raw_body":"# Verification\n\nHow noface determines whether an agent succeeded.\n\n## Current Design\n\nnoface uses multiple verification layers:\n\n1. **Test execution** — runs the configured test command; failure = not done\n2. **Manifest compliance** — `git diff` checked against declared files; violations = rollback\n3. **Build check** — runs the configured build command (implicit in agent workflow)\n\nThe agent is instructed to self-verify (run tests, check output) before committing.\n\n## Relation to Survey\n\nThe survey emphasizes **automated testing as ground truth**:\n\n> \"This ensures that code isn't accepted as 'done' until it passes its tests. Even single-agent approaches like OpenAI's Codex have employed this idea (often called execute-and-fix): run the code, and if an error or failing test is detected, prompt the model to fix it.\"\n\nAnd **manifest verification**:\n\n> \"Tools like MAID runner perform static analysis on the diff: did the agent only modify the allowed files and functions?\"\n\nThe survey also discusses **LLM critics** as an additional layer — a second agent that reviews the code.\n\n## Open Questions\n\n1. **Test Coverage** — What if the tests don't cover the change? Agent could introduce a bug that passes existing tests.\n\n2. **Review Pass** — Should noface run a dedicated reviewer agent on each change before accepting? Cost vs. benefit?\n\n3. **Static Analysis** — Should we run linters, type checkers, security scanners as additional gates?\n\n4. **Semantic Verification** — Tests check behavior, manifests check scope. But what about \"did the agent actually address the issue?\" Sometimes code passes tests but doesn't solve the problem.\n\n5. **Confidence Signals** — Can we detect low-confidence completions? Agent says \"done\" but hedges in comments? Unusual patterns?\n\n6. **Partial Acceptance** — If 2 of 3 acceptance criteria pass, do we accept partially? Or all-or-nothing?\n\n7. **Human Review Gate** — For high-risk changes (security, config), should noface require human approval?\n\n## Implementation Notes\n\nSee `src/loop.zig:verifyManifestCompliance` and prompt instructions for self-testing.\n","source_path":"design/verification.md"},{"slug":"multi-pass","title":"Multi-Pass Architecture","content_html":"<h1 id=\"multi-pass-architecture\">Multi-Pass Architecture<a class=\"heading-anchor\" href=\"#multi-pass-architecture\" aria-label=\"Link to heading\">#</a></h1>\n<p>How noface uses multiple agent passes to improve quality.</p>\n<h2 id=\"current-design\">Current Design<a class=\"heading-anchor\" href=\"#current-design\" aria-label=\"Link to heading\">#</a></h2>\n<p>noface runs three types of passes:</p>\n<ol>\n<li>\n<p><strong>Planner pass</strong> (Codex) — every N iterations</p>\n<ul>\n<li>Analyzes issue backlog</li>\n<li>Generates file manifests for each issue</li>\n<li>Groups non-conflicting issues into parallel batches</li>\n</ul>\n</li>\n<li>\n<p><strong>Implementation pass</strong> (Claude) — per issue</p>\n<ul>\n<li>Receives issue + manifest + context</li>\n<li>Implements the change, runs tests, commits</li>\n</ul>\n</li>\n<li>\n<p><strong>Quality pass</strong> (Codex) — every M iterations</p>\n<ul>\n<li>Scans codebase for tech debt</li>\n<li>Creates new issues for findings</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"relation-to-survey\">Relation to Survey<a class=\"heading-anchor\" href=\"#relation-to-survey\" aria-label=\"Link to heading\">#</a></h2>\n<p>This follows the <strong>Planner → Implementer</strong> pattern from <a href=\"/orchestration-survey.html\">orchestration-survey</a>:</p>\n<blockquote>\n<p>\"The self-planning study by Jiang et al. (2025) exemplifies this: the LLM generates concise, structured planning steps from the intent, then in the implementation phase it 'generates code step by step, guided by the preceding planning steps'. This two-pass approach yielded significantly higher correctness than a single-pass solution.\"</p>\n</blockquote>\n<p>The survey also discusses <strong>Reviewer passes</strong> for catching mistakes. noface's quality pass is similar but focuses on proactive debt detection rather than reviewing specific changes.</p>\n<h2 id=\"open-questions\">Open Questions<a class=\"heading-anchor\" href=\"#open-questions\" aria-label=\"Link to heading\">#</a></h2>\n<ol>\n<li>\n<p><strong>Pass Intervals</strong> — <code>planner_interval = 5</code> and <code>quality_interval = 10</code> are arbitrary. What's optimal? Should they be adaptive?</p>\n</li>\n<li>\n<p><strong>Reviewer Pass</strong> — Should noface add a dedicated reviewer pass that checks each implementation before accepting? Current flow trusts tests + manifest.</p>\n</li>\n<li>\n<p><strong>Feedback Loops</strong> — If the quality pass finds issues, they become new issues. But there's no direct feedback to the implementer. Should there be?</p>\n</li>\n<li>\n<p><strong>Pass Ordering</strong> — Planner runs periodically, not before every implementation. Does this cause stale manifests? Should planner run on-demand?</p>\n</li>\n<li>\n<p><strong>Diminishing Returns</strong> — The survey notes additional passes have diminishing returns. How do we measure if a pass is worth the cost?</p>\n</li>\n<li>\n<p><strong>Model Selection</strong> — Planner/quality use Codex; implementation uses Claude. Is this the right split? Should we try other combinations?</p>\n</li>\n</ol>\n<h2 id=\"implementation-notes\">Implementation Notes<a class=\"heading-anchor\" href=\"#implementation-notes\" aria-label=\"Link to heading\">#</a></h2>\n<p>See <code>src/loop.zig:runPlannerPass</code>, <code>runQualityPass</code>, <code>runIteration</code>.</p>\n","frontmatter":{"title":"Multi-Pass Architecture","description":null,"summary":null,"date":null,"type":"essay","tags":["design","planner","reviewer","passes"],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":["design","planner","reviewer","passes"],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":["orchestration-survey"],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#multi-pass-architecture\">Multi-Pass Architecture</a></li><li class=\"toc-level-2\"><a href=\"#current-design\">Current Design</a></li><li class=\"toc-level-2\"><a href=\"#relation-to-survey\">Relation to Survey</a></li><li class=\"toc-level-2\"><a href=\"#open-questions\">Open Questions</a></li><li class=\"toc-level-2\"><a href=\"#implementation-notes\">Implementation Notes</a></li></ul></nav>","raw_body":"# Multi-Pass Architecture\n\nHow noface uses multiple agent passes to improve quality.\n\n## Current Design\n\nnoface runs three types of passes:\n\n1. **Planner pass** (Codex) — every N iterations\n   - Analyzes issue backlog\n   - Generates file manifests for each issue\n   - Groups non-conflicting issues into parallel batches\n\n2. **Implementation pass** (Claude) — per issue\n   - Receives issue + manifest + context\n   - Implements the change, runs tests, commits\n\n3. **Quality pass** (Codex) — every M iterations\n   - Scans codebase for tech debt\n   - Creates new issues for findings\n\n## Relation to Survey\n\nThis follows the **Planner → Implementer** pattern from [[orchestration-survey]]:\n\n> \"The self-planning study by Jiang et al. (2025) exemplifies this: the LLM generates concise, structured planning steps from the intent, then in the implementation phase it 'generates code step by step, guided by the preceding planning steps'. This two-pass approach yielded significantly higher correctness than a single-pass solution.\"\n\nThe survey also discusses **Reviewer passes** for catching mistakes. noface's quality pass is similar but focuses on proactive debt detection rather than reviewing specific changes.\n\n## Open Questions\n\n1. **Pass Intervals** — `planner_interval = 5` and `quality_interval = 10` are arbitrary. What's optimal? Should they be adaptive?\n\n2. **Reviewer Pass** — Should noface add a dedicated reviewer pass that checks each implementation before accepting? Current flow trusts tests + manifest.\n\n3. **Feedback Loops** — If the quality pass finds issues, they become new issues. But there's no direct feedback to the implementer. Should there be?\n\n4. **Pass Ordering** — Planner runs periodically, not before every implementation. Does this cause stale manifests? Should planner run on-demand?\n\n5. **Diminishing Returns** — The survey notes additional passes have diminishing returns. How do we measure if a pass is worth the cost?\n\n6. **Model Selection** — Planner/quality use Codex; implementation uses Claude. Is this the right split? Should we try other combinations?\n\n## Implementation Notes\n\nSee `src/loop.zig:runPlannerPass`, `runQualityPass`, `runIteration`.\n","source_path":"design/multi-pass.md"},{"slug":"index","title":"noface","content_html":"<h1 id=\"noface\">noface<a class=\"heading-anchor\" href=\"#noface\" aria-label=\"Link to heading\">#</a></h1>\n<p>An autonomous agent orchestrator for software development. noface coordinates black-box agents (Claude Code, Codex) to work on issues from your backlog — in parallel, with conflict detection, crash recovery, and quality gates.</p>\n<h2 id=\"research\">Research<a class=\"heading-anchor\" href=\"#research\" aria-label=\"Link to heading\">#</a></h2>\n<ul>\n<li><a href=\"/orchestration-survey.html\">orchestration-survey</a> — Literature survey on design patterns for orchestrating code agents</li>\n</ul>\n<h2 id=\"design\">Design<a class=\"heading-anchor\" href=\"#design\" aria-label=\"Link to heading\">#</a></h2>\n<p>Core infrastructure decisions:</p>\n<ul>\n<li><a href=\"/manifests.html\">manifests</a> — File access control via PRIMARY/READ/FORBIDDEN declarations</li>\n<li><a href=\"/parallel-execution.html\">parallel-execution</a> — Batching, locking, and conflict avoidance</li>\n<li><a href=\"/context-injection.html\">context-injection</a> — What information agents receive and how</li>\n<li><a href=\"/multi-pass.html\">multi-pass</a> — Planner → Implementer → Reviewer architecture</li>\n<li><a href=\"/failure-recovery.html\">failure-recovery</a> — Retry, rollback, escalation strategies</li>\n<li><a href=\"/verification.html\">verification</a> — How we know an agent succeeded</li>\n</ul>\n<h2 id=\"quick-links\">Quick Links<a class=\"heading-anchor\" href=\"#quick-links\" aria-label=\"Link to heading\">#</a></h2>\n<ul>\n<li><a href=\"https://github.com/...\">GitHub</a></li>\n<li><a href=\"/.beads/issues.jsonl\">Issue Tracker</a></li>\n</ul>\n","frontmatter":{"title":"noface","description":"Design documentation for the noface autonomous agent orchestrator","summary":null,"date":null,"type":null,"tags":[],"draft":false,"updated":null,"slug":null,"permalink":null,"aliases":[],"typst_preamble":null,"bibliography":[],"target_slug":null,"target_anchor":null,"git_ref":null,"quote":null,"author":null,"status":null,"parent_id":null},"note_type":"essay","tags":[],"date":null,"updated":null,"aliases":[],"permalink":null,"outgoing_links":["orchestration-survey","manifests","parallel-execution","context-injection","multi-pass","failure-recovery","verification"],"preview":null,"toc_html":"<nav class=\"toc-nav\"><h3>Contents</h3><ul class=\"toc-list\"><li class=\"toc-level-1\"><a href=\"#noface\">noface</a></li><li class=\"toc-level-2\"><a href=\"#research\">Research</a></li><li class=\"toc-level-2\"><a href=\"#design\">Design</a></li><li class=\"toc-level-2\"><a href=\"#quick-links\">Quick Links</a></li></ul></nav>","raw_body":"# noface\n\nAn autonomous agent orchestrator for software development. noface coordinates black-box agents (Claude Code, Codex) to work on issues from your backlog — in parallel, with conflict detection, crash recovery, and quality gates.\n\n## Research\n\n- [[orchestration-survey]] — Literature survey on design patterns for orchestrating code agents\n\n## Design\n\nCore infrastructure decisions:\n\n- [[manifests]] — File access control via PRIMARY/READ/FORBIDDEN declarations\n- [[parallel-execution]] — Batching, locking, and conflict avoidance\n- [[context-injection]] — What information agents receive and how\n- [[multi-pass]] — Planner → Implementer → Reviewer architecture\n- [[failure-recovery]] — Retry, rollback, escalation strategies\n- [[verification]] — How we know an agent succeeded\n\n## Quick Links\n\n- [GitHub](https://github.com/...)\n- [Issue Tracker](/.beads/issues.jsonl)\n","source_path":"index.md"}],"graph":{"outgoing":{"index":["orchestration-survey","manifests","parallel-execution","context-injection","multi-pass","failure-recovery","verification"],"manifests":["orchestration-survey"],"multi-pass":["orchestration-survey"],"parallel-execution":["orchestration-survey"]},"incoming":{"failure-recovery":["index"],"verification":["index"],"manifests":["index"],"parallel-execution":["index"],"orchestration-survey":["parallel-execution","manifests","multi-pass","index"],"context-injection":["index"],"multi-pass":["index"]}},"diagnostics":[],"comments":[]}}