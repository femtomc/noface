[
  {
    "id": "agent#monowiki-agent-guide",
    "url": "/agent.html#monowiki-agent-guide",
    "section_id": "monowiki-agent-guide-83bafda4",
    "section_hash": "83bafda4f98f83fd03d25c9db3328b15c1a8f2d1f6699bbeb2c49d06b2ce4496",
    "title": "AGENT",
    "section_title": "monowiki Agent Guide #",
    "content": "This vault ships with a CLI that is designed to be agent-friendly. Key commands: monowiki build : render the site to docs/ and emit index.json , graph.json , previews.json , and a cached site index at docs/.site_index.json . monowiki dev : serve the site locally with live rebuilds and JSON endpoints: /api/search?q=term&limit=10 /api/note/<slug> /api/graph/<slug>?depth=2&direction=both /api/graph/path?from=a&to=b&max_depth=5 monowiki search \"<query>\" --json --limit 5 --types essay,thought --tags rust,notes --with-links for machine-readable results. monowiki note <slug> --format json --with-links to fetch a single note (frontmatter, rendered HTML, raw body, links). monowiki graph neighbors --slug <slug> --depth 2 --direction outgoing --json to fan out. monowiki graph path --from a --to b --max-depth 4 --json to find shortest paths. monowiki export sections --format jsonl --with-links to stream embedding-ready chunks. monowiki watch streams JSON change events from vault/ (one line per event). JSON schemas CLI --json and dev server /api/* responses are wrapped in: Copy { \"schema_version\" : \"2024-11-llm-v1\", \"kind\" : \"search.results | note.full | graph.neighbors | graph.path\", \"data\" : { ... } } Search results include id , slug , url , title , section_title , snippet , tags , type , score , outgoing , backlinks . Notes include frontmatter, HTML, raw markdown, toc, outgoing, backlinks, and dates. Graph neighbors include nodes with slug/title/url/tags/type and edges; graph path returns the path array. Performance tips monowiki build / dev write docs/.site_index.json . The note , graph , and export commands reuse this cache to avoid rebuilding when only reading data. Delete it if you need a fresh rebuild. Conventions: Slugs come from frontmatter slug , otherwise the filename slugified. Drafts are excluded from exports/search when type: draft or draft: true . Backlinks are computed from [[WikiLinks]] in markdown and exposed via graph.json and the CLI/API. Section-level search slices HTML headings into chunks; IDs match rendered anchors. Tips for agents: Use monowiki export sections to build retrieval datasets without scraping. Use monowiki note <slug> --format json to fetch full context (toc, html, raw markdown) before editing. Prefer the dev server APIs during interactive sessions; they reflect live rebuilds. Happy hacking!",
    "snippet": "This vault ships with a CLI that is designed to be agent-friendly. Key commands: monowiki build : render the site to docs/ and emit index.json , graph.json , previews.json , and a cached site index...",
    "tags": [],
    "type": "essay"
  },
  {
    "id": "orchestration-survey#design-patterns-for-orchestrating-black-box-code-agents-a-literature-survey",
    "url": "/orchestration-survey.html#design-patterns-for-orchestrating-black-box-code-agents-a-literature-survey",
    "section_id": "design-patterns-for-orchestrating-black-box-code-agents-a-literature-survey-cd25f60a",
    "section_hash": "cd25f60af3b5e8f64104237a2e7c761f9bf5d9c152c9d91c03e3a4214c9b3a34",
    "title": "Design Patterns for Orchestrating Black-Box Code Agents",
    "section_title": "Design Patterns for Orchestrating Black-Box Code Agents: A Literature Survey #",
    "content": "Orchestrating black-box code-generation agents requires careful planning and robust design patterns to ensure these AI \"developers\" work effectively. Researchers and practitioners have proposed various strategies to assign tasks, manage context, engineer prompts, run agents in parallel safely, verify outputs, handle failures, and integrate multiple passes or human oversight. This survey reviews key patterns from academic literature and real-world case studies, organized by orchestration concern, and provides a pattern catalog of proven approaches.",
    "snippet": "Orchestrating black-box code-generation agents requires careful planning and robust design patterns to ensure these AI \"developers\" work effectively. Researchers and practitioners have proposed...",
    "tags": [
      "research",
      "agents",
      "orchestration",
      "survey"
    ],
    "type": "essay"
  },
  {
    "id": "orchestration-survey#task-assignment-and-granularity",
    "url": "/orchestration-survey.html#task-assignment-and-granularity",
    "section_id": "task-assignment-and-granularity-2a5ac648",
    "section_hash": "2a5ac648a8629e22cfb3a700a622fb4f6cbb65b419a66fab6fd4b640f170d489",
    "title": "Design Patterns for Orchestrating Black-Box Code Agents",
    "section_title": "Task Assignment and Granularity #",
    "content": "Dynamic Task Decomposition: A central pattern is to use an orchestrator agent (or component) that breaks down a software project into manageable subtasks and assigns them to specialized agents. Rather than having one AI attempt an entire project, orchestrators analyze the request and determine necessary subtasks (e.g. implement a feature, write tests, update docs). In Microsoft's taxonomy of agent architectures, an Orchestrator-Workers pattern is defined where a lead agent \"dynamically decomposes tasks, delegates them to worker agents, and synthesizes their outputs\". For example, in a coding context the orchestrator could decide which files or modules need changes and coordinate sub-agents for code generation, testing, and documentation. This dynamic assignment allows tackling large development goals by partitioning work. Specialization and Routing: Task assignment often leverages specialization. The orchestrator may classify each task and route it to an agent best suited for that category (using a Routing Workflow pattern) [@pujals2025agentic]. For instance, code-generation requests might go to a \"Coder\" agent, while bug-fix tasks route to a \"Debugger\" agent, and documentation tasks to a \"Doc Writer\" agent. The input can be analyzed (via rules or an LLM) to decide which agent is most appropriate. This ensures that each agent works on tasks aligned with its expertise, improving effectiveness. Ellipsis's production code review system is an example: it runs multiple specialized agents in parallel, each looking for a specific type of issue in a pull request (security, style, duplication, etc.), rather than one monolithic agent [@zenml2024ellipsis]. Granularity – Sizing the Task: Deciding how large a task to give a single agent call is crucial. Literature suggests that tasks should be fine-grained enough to fit the agent's context window and capabilities, yet not so granular that the overhead of orchestration dominates. If a task is too large (e.g. \"implement an entire feature spanning many files\"), agents may struggle with context or produce incomplete solutions. Researchers have addressed this by explicit task planning. Self-planning code generation has the LLM first outline a plan of steps for a complex request, then tackle each step sequentially [@jiang2024selfplanning]. This two-phase approach of planning then coding yields up to 25.4% higher success (Pass@1) than one-shot generation for complex problems. Similarly, the Prompt-Chaining workflow breaks a complex job into a sequence of smaller subtasks, each handled by a dedicated prompt/agent call [@pujals2025agentic]. This not only makes each step easier for the model to execute but also improves transparency and debuggability. In practice, a rule of thumb is to treat one well-defined issue or function as a unit of work for an agent, and if the request spans multiple concerns or components, the orchestrator should subdivide it. Handling Dependencies and Readiness: An orchestrator must also respect task dependencies when assigning work. Some tasks can run in parallel (e.g. implement UI and backend separately), while others depend on earlier outputs (e.g. tests should run after code is written). One practical approach is to maintain a shared task board with statuses and dependencies. In a user's multi-agent Claude setup, tasks were tracked in a Markdown plan file with fields for Assigned Agent, Status, and Dependencies – e.g. \"Write Integration Tests – Assigned to Validator – Pending (waiting for Builder to complete auth module)\". This allows the orchestrator (and agents themselves) to know when a task is ready to start. If a task is too big or not well-defined, a planner agent might refine it further or ask for clarification before assignment. Ultimately, effective task assignment balances priority (which issues are most important), readiness (are prerequisites done?), and size (fits in one agent invocation).",
    "snippet": "Dynamic Task Decomposition: A central pattern is to use an orchestrator agent (or component) that breaks down a software project into manageable subtasks and assigns them to specialized agents....",
    "tags": [
      "research",
      "agents",
      "orchestration",
      "survey"
    ],
    "type": "essay"
  },
  {
    "id": "orchestration-survey#context-curation-providing-helpful-context-and-avoiding-harmful-noise",
    "url": "/orchestration-survey.html#context-curation-providing-helpful-context-and-avoiding-harmful-noise",
    "section_id": "context-curation-providing-helpful-context-and-avoiding-harmful-noise-fd7b663d",
    "section_hash": "fd7b663d6605bb6d058cc5a01f75babac3f1c25489009f8cbe8afdd2417b5907",
    "title": "Design Patterns for Orchestrating Black-Box Code Agents",
    "section_title": "Context Curation: Providing Helpful Context (and Avoiding Harmful Noise) #",
    "content": "Relevant Context Injections: The information an agent gets in its prompt (code context, design docs, examples) profoundly impacts its success. A key pattern is Retrieval-Augmented Code Generation (RACG), where the orchestrator fetches relevant snippets from the codebase or documentation to include in the agent's prompt. Real-world software tasks often require understanding cross-file dependencies and global architecture. Since an LLM has a limited context window, the orchestrator must be selective: for example, retrieving the function signature and related config file when asking the agent to implement a new feature in that function. A recent survey emphasizes that repository-level code generation needs to ensure global semantic consistency across files, and integrating a retrieval mechanism (e.g. searching a knowledge base of code) helps the LLM reason beyond a single file. In practice, tools like vector databases are used to find and feed the most relevant code pieces to the agent. Ellipsis's code review agents, for instance, use advanced code search backed by embeddings to find pertinent pieces of the codebase when analyzing a PR [@zenml2024ellipsis]. Curating Helpful Examples and Constraints: Apart from code context, orchestrators often supply design documents, API usage examples, or style guides as context if they will guide the agent toward the right solution. Providing a few-shot example of a similar function or test can dramatically improve accuracy, as the agent can analogize from the given pattern. Additionally, context can include explicit constraints – e.g. a list of files that are in scope, or performance requirements. By giving the agent a clear problem statement along with these auxiliary materials, the orchestrator sets it up for success. For example, if generating a new module, the orchestrator might include an outline of the module's design or a simplified spec in the prompt. In PerfOrch (Performance-Oriented Orchestration), contextual knowledge about which LLMs perform best for a given language and problem type is stored and used to pick the right model for the task [@wang2025perforch]. This \"strategic memory\" of model strengths is a form of context that improves outcomes by dynamically selecting the optimal agent for each subtask. Avoiding Context Overload: Too much context can hurt agent performance. It might be tempting to provide every possibly relevant file to be safe, but large context size can confuse the model or dilute its focus. Recent studies have shown that even if all provided context is relevant, performance can degrade substantially (by 13–85%) as input length grows. In other words, beyond a certain point, the sheer volume of text can overwhelm the LLM's reasoning (\"context dilution\"). Thus, orchestrators should be judicious: include only the most relevant code and information. Irrelevant or contradictory context is even worse – it can lead the agent to draw false inferences or focus on the wrong problem. Best practices include using retrieval algorithms to filter context, and possibly summarizing or abstracting context to fit the window. For instance, rather than inserting an entire 500-line file, an orchestrator might insert a summary or the specific function signature needed. The Context-Augmentation pattern described by Hugging Face highlights that context expansion should be iterative and need-based: the agent identifies what extra info it needs, the orchestrator fetches it, and this cycle repeats, rather than dumping everything at once [@pujals2025agentic]. Techniques like embedding search, docstrings extraction, or code property graphs can help select which context truly matters. Detecting and Removing Harmful Context: Orchestrators may also implement \"context pruning\" – removing any information from the prompt that isn't consistent or necessary. This can include stale data (e.g. outdated function definitions if a newer version exists) or irrelevant conversation history in iterative settings. The goal is to minimize noise. If an agent will be doing a focused code edit, the orchestrator might strip out any unrelated instructions from the conversation and supply a fresh prompt with just the needed background. In summary, curated context is a double-edged sword: the right snippets and examples can greatly enhance success, but too much or wrong context can mislead the agent. Effective orchestration employs retrieval, filtering, and pruning to optimize context.",
    "snippet": "Relevant Context Injections: The information an agent gets in its prompt (code context, design docs, examples) profoundly impacts its success. A key pattern is Retrieval-Augmented Code Generation...",
    "tags": [
      "research",
      "agents",
      "orchestration",
      "survey"
    ],
    "type": "essay"
  },
  {
    "id": "orchestration-survey#prompt-engineering-at-the-orchestration-level",
    "url": "/orchestration-survey.html#prompt-engineering-at-the-orchestration-level",
    "section_id": "prompt-engineering-at-the-orchestration-level-30056102",
    "section_hash": "30056102db3bde76b0add9165e2e528b4ed007dfb9290b628ed89fb8813617c8",
    "title": "Design Patterns for Orchestrating Black-Box Code Agents",
    "section_title": "Prompt Engineering at the Orchestration Level #",
    "content": "Structured Instructions vs. Plain Language: Orchestration involves not only what tasks agents do, but how those tasks are described in prompts. A recurring pattern is to use structured, formal instructions rather than leaving everything to freeform natural language. For example, instead of saying \"Please write some code to do X\", an orchestrator-generated prompt might include a template: a step-by-step checklist, or a specific format like pseudo-code or JSON plan that the agent must follow. This structure can guide the model's output and reduce ambiguity. A concrete case is the self-planning approach: the orchestrator first asks the LLM to output a formatted plan (e.g. a numbered list of steps), and then feeds that plan into the next prompt for code generation [@jiang2024selfplanning]. By explicitly separating \"thinking\" from \"coding,\" the instructions become more systematic, leading to better results. Similarly, multi-agent frameworks often start with a role acknowledgment and task breakdown. In a four-agent setup using Claude (Architect, Builder, Validator, Scribe), each agent's system prompt begins with a role declaration and primary tasks in bullet form (e.g. \"You are Agent 1 – The Architect. Primary Tasks: requirements analysis, architecture planning…\"). This kind of structured prompt ensures each agent knows its exact scope and constraints. Communicating Constraints Clearly: The orchestration layer must convey any hard constraints or \"rules of engagement\" to the agent in the prompt. This can include: \"Do not modify code outside of Function A in file X\", \"Follow the project's style guide for naming\", or \"Make sure to run the provided tests and ensure they pass\". Explicitly listing files or sections not to touch can prevent collateral changes. However, LLMs may not always obey implicitly phrased constraints, so phrasing is important. Some systems adopt a manifest-driven approach, where the orchestrator provides a manifest of intended changes. For instance, the manifest might enumerate which functions to implement or which files to create, and the agent is instructed that any output will be checked against this manifest. A recent methodology called Manifest-Driven AI Development (MAID) takes this further by treating the manifest as an enforceable contract: after generation, a validator checks that only the declared files/functions were altered and no undeclared edits occurred. By front-loading these constraints (and later verifying them), the orchestrator greatly reduces the chance of the agent \"coloring outside the lines.\" Another way to impose structure is using tool-assisted prompting languages (like Microsoft's Guidance library) that allow templates with placeholders and even regex checks on the LLM's output, ensuring the agent adheres to formats. Success Criteria and Framing: The prompt should also frame what constitutes success. A good orchestration prompt might end with a clear success condition, e.g.: \"Your solution will be considered correct if all given unit tests pass and it meets the performance constraint of <2s runtime.\" By stating these criteria, the agent can self-evaluate its output against them, leading to better alignment with the goal. This is related to the Evaluator-Optimizer pattern [@carpintero2025evaluator], where the prompt or subsequent agent pass includes evaluation guidelines. For example, an orchestrator might instruct: \"First, implement the function. Then double-check that the function handles edge cases (null input, etc.) and meets the description. Only finalize if these criteria are satisfied.\" Such structured prompts push the agent to reflect on its answer against requirements, effectively building a spec into the prompt. Research by Anthropic also suggests that using systematic prompt patterns often outperforms ad-hoc prompting [@anthropic2024agents]. In their experience, many teams succeeded by \"simple, composable patterns\" in prompts (like stepwise reasoning or tool usage specifications) rather than overly open-ended prompts. In sum, the orchestration layer benefits from acting like a technical writer: providing the right scaffolding in the prompt (roles, steps, examples, constraints, and success checks) so the black-box agent is guided toward the desired outcome. Example – Claude Subagents: As a practical example, Claude's subagents feature allows configuring specialized system prompts for different sub-tasks. Each subagent has \"a custom system prompt that guides its behavior\" and a defined expertise. The main agent automatically delegates to these subagents when appropriate. In effect, the orchestrator (Claude's internal logic) appends the specialized prompt and context for that sub-task. This demonstrates prompt engineering at scale: rather than one giant prompt for everything, the orchestration chooses a tailored prompt per task type. Overall, structured and deliberate prompt engineering at the orchestration level — using instructions, role context, examples, and criteria — is key to maximizing agent reliability.",
    "snippet": "Structured Instructions vs. Plain Language: Orchestration involves not only what tasks agents do, but how those tasks are described in prompts. A recurring pattern is to use structured, formal...",
    "tags": [
      "research",
      "agents",
      "orchestration",
      "survey"
    ],
    "type": "essay"
  },
  {
    "id": "orchestration-survey#parallelism-conflict-avoidance",
    "url": "/orchestration-survey.html#parallelism-conflict-avoidance",
    "section_id": "parallelism-conflict-avoidance-8ccab7ee",
    "section_hash": "8ccab7eebfad0bab04a3300ebbc0c0ee3d60682e84a2d3dec400d90f63534988",
    "title": "Design Patterns for Orchestrating Black-Box Code Agents",
    "section_title": "Parallelism & Conflict Avoidance #",
    "content": "When to Run Agents in Parallel: Orchestrating multiple agents in parallel can speed up development, but only if their tasks won't conflict. The Parallelization Workflow follows a divide-and-conquer approach: identify subtasks that are truly independent and execute them concurrently [@pujals2025agentic]. For example, an orchestrator might split a feature into frontend and backend implementations and assign each to a different agent at the same time. Another parallel pattern is \"voting\" – run multiple agents on the same task in parallel (perhaps with different prompting styles or different model seeds) and then choose the best result. In coding, this could mean generating multiple candidate solutions concurrently and later selecting one that passes tests (\"N-best\" approach). However, if parallel agents operate on shared artifacts (e.g. editing the same file), conflicts can arise. Thus, orchestrators should only parallelize tasks that either target separate components or use a strategy to reconcile outputs. Locking and Work Queues: A common strategy for conflict avoidance is file-level locking or resource locking. Before an agent begins work, the orchestrator can lock the files or modules it will touch, preventing other agents from editing them until completion. A community-built \"Claude code agent farm\" demonstrates this with a coordination protocol: each agent must create a lock file listing the files/features it intends to work on, and consult a shared active work registry to ensure no overlap. If a conflict is detected (e.g. two agents want to modify utils.py), one agent will wait (queue the task) until the file is free. This effectively serializes conflicting sections while still allowing non-conflicting tasks in parallel. The registry and lock files also help avoid duplicate work – agents check the completed task log so they don't redo something another agent already finished. Such locking is a conservative heuristic but very effective: it \"eliminates merge conflicts\" and \"prevents wasted work\" in concurrent development. The downside is that overly coarse locks (e.g. locking an entire project) nullify the speed benefit, so orchestrators must strike a balance by locking at a sensible granularity (file or feature level). Advanced Conflict Resolution (CRDTs): Recent research explores lock-free coordination using principles from distributed systems. CodeCRDT introduces an observation-driven approach where agents don't explicitly lock resources, but instead work on a shared state (a codebase) that automatically merges changes in a conflict-free manner [@pugachev2025codecrdt]. It uses Conflict-Free Replicated Data Types to ensure that as agents make edits, their updates converge deterministically without stepping on each other. In trials, this achieved 100% merge convergence with zero low-level conflicts, eliminating the need for manual conflict resolution. Essentially, each agent can edit concurrently and the CRDT mechanism integrates their changes (like Google Docs but for code). While CRDT-based orchestration can avoid the bottleneck of locks (which can become contention points as agent count N grows), it introduces complexity: agents must deal with semantic conflicts (e.g. two agents might implement the same feature in different ways, both syntactically merged but logically redundant). CodeCRDT reported about 5–10% of such higher-level conflicts, meaning an orchestrator or later review still has to reconcile duplicate or inconsistent implementations. Nonetheless, this approach is promising for truly scalable parallel agent teams, as it sidesteps the O(N×L) contention of lock-based systems. Heuristics for Parallel Task Selection: In practice, orchestrators often use simple heuristics to decide parallelism: e.g. no two agents on the same file or closely related module. Some systems use static analysis of code dependency graphs to ensure two parallel tasks don't overlap in call graphs or data models. Others rely on the developer to specify which tasks are independent. For example, an orchestrator might know that adding a new API endpoint (task A) is largely independent of refactoring the logging library (task B), so it schedules those simultaneously. If there is uncertainty, a conservative orchestrator will serialize tasks to avoid the risk of conflict. In the worst case that a conflict does occur (say two agents inadvertently changed the same file), the orchestrator must have a conflict-resolution step: it could auto-merge the code (if changes are in different regions), prefer one agent's output over the other, or fall back to human intervention to reconcile differences. Generally, the goal is conflict avoidance through design: split work such that each agent has its own domain of responsibility. As one user's experience with 4 parallel Claude agents showed, dividing roles by concern (architecture vs. coding vs. testing vs. docs) inherently reduced direct conflicts and even brought positive effects like built-in peer review. Each agent had a clear lane, enabling parallel development with quality checks due to the separation of concerns. In summary, orchestrating parallel code agents requires careful coordination: lock or isolate shared resources, or adopt novel merging strategies, so that parallelism yields speed-up without chaos. When done right (e.g. with file locks or CRDTs), teams of agents can work \"like a well-oiled machine\" in parallel.",
    "snippet": "When to Run Agents in Parallel: Orchestrating multiple agents in parallel can speed up development, but only if their tasks won't conflict. The Parallelization Workflow follows a divide-and-conquer...",
    "tags": [
      "research",
      "agents",
      "orchestration",
      "survey"
    ],
    "type": "essay"
  },
  {
    "id": "orchestration-survey#verification-acceptance-criteria",
    "url": "/orchestration-survey.html#verification-acceptance-criteria",
    "section_id": "verification-acceptance-criteria-2aa7d138",
    "section_hash": "2aa7d1386c7e1e18709d771f6a4603fe30e83d641597fff72f8dcb54ec1efdf0",
    "title": "Design Patterns for Orchestrating Black-Box Code Agents",
    "section_title": "Verification & Acceptance Criteria #",
    "content": "Automated Testing as Ground Truth: One of the most powerful verification techniques is running tests on the agent's output. If the task comes with a test suite or at least some example cases, the orchestrator can automatically execute the generated code and check for correctness. This pattern is evident in many frameworks. AgentCoder is explicitly built around a test-feedback loop: a Test Designer agent generates unit tests, a Test Executor runs the code on those tests, and the Programmer agent then debugs or refines the code based on failures [@huang2023agentcoder]. This ensures that code isn't accepted as \"done\" until it passes its tests. Even single-agent approaches like OpenAI's Codex have employed this idea (often called execute-and-fix): run the code, and if an error or failing test is detected, prompt the model to fix it. The orchestrator's job is to automate this loop. In PerfOrch's multi-stage pipeline, after generation they perform a validation: if the code is functionally incorrect, it triggers the Fix Stage, invoking specialized bug-fixing models iteratively until a correct solution is found [@wang2025perforch]. Only once all tests pass does it proceed to the next stage. This demonstrates an automated acceptance gate – test passing is the criterion for success. Orchestrators can also measure other aspects like performance: PerfOrch's final Refine Stage checked if runtime improved and only accepted refinements that preserved correctness and met the performance target. Code Reviews and LLM Critics: Besides tests, another verification layer is reviewing the code, either by humans or by a second \"reviewer\" agent. Having a separate agent critique the generated code can catch issues that tests don't (e.g. poor readability, not following style, potential inefficiencies). A pattern often cited is dual-agent coding: one agent writes code, another agent reviews it. If the reviewer finds problems, the coder agent revises accordingly. This is essentially the Evaluator-Optimizer pattern applied to code [@carpintero2025evaluator] – where the Evaluator (which could be the same LLM or a different one tuned for critique) provides feedback, and the Optimizer (generator) uses it to improve the output in another pass. Academic and industry work supports this approach. In practice, companies like Meta have prototypes where an AI code generation is always followed by an AI code review, flagging issues before any human sees the code. Ellipsis's production system likewise emphasizes accuracy over latency: they run multiple comment-generation agents and then use a filtering pipeline (including an LLM-as-judge that evaluates the proposed comments) to ensure only high-quality, non-false-positive feedback is given [@zenml2024ellipsis]. By using an LLM judge on the outputs of LLM coders, they add a second layer of assurance. Manifests and Static Verification: As discussed in prompt engineering, if the orchestrator provided a manifest of expected changes, verification includes checking the agent's output against that manifest. Tools like MAID runner perform static analysis on the diff: did the agent only modify the allowed files and functions? Did it include the required new function? This kind of structural verification can be automated by parsing the output or performing AST (Abstract Syntax Tree) comparison between the original and modified code. If the agent deviated (e.g. changed something unrelated), the orchestrator can reject that output or prompt a correction (\"You edited file Y which was out of scope; please revert that.\"). Additionally, the orchestrator might enforce style guides by running linters or formatters on the code and ensuring no new warnings were introduced. Acceptance and Quality Gates: Complex orchestrations often define multi-dimensional acceptance criteria. Functional correctness (tests passing) is usually mandatory. Beyond that, style compliance, performance metrics, security scans, etc., can be gates. For example, after an agent's code passes tests, the orchestrator might also run a static security analyzer; if it flags issues, the agent's output isn't fully accepted and a fix task is spawned (or a human review is requested for that aspect). Hugging Face's guide suggests quality gates such as \"code passes all test suites\" as a primary stopping condition for an iterative refine loop – essentially, you keep refining until the code meets the quality bar [@pujals2025agentic]. They also note using a maximum iteration limit and similarity checks to avoid infinite loops. Handling Partial Success: Sometimes an agent's output is partially correct – e.g. it implements one feature but misses another, or fixes some bugs but not all. In these cases, orchestrators can take a pragmatic approach: accept the improvements made and then create follow-up tasks for the remaining work. This is akin to how a human might commit a partial fix and then address the rest. Orchestrators with tracking systems (like the completed_work_log in the agent farm) can mark certain sub-goals as done and only reassign what failed. If tests are granular, the orchestrator can rerun them and see which specific tests still fail, then prompt the agent specifically about those failures on the next try. This targeted retry approach zeroes in on the unresolved issues rather than redoing everything. Automated vs Human Acceptance: Ultimately, the output may either be auto-merged or require human approval. Many real-world setups keep a human-in-the-loop for final approval – for example, an orchestrator might open a pull request with the agent's changes and require a human developer to review and merge. The orchestrator in this case hands off to a human at the final step. Another checkpoint is when an agent is uncertain or stuck. If an agent expresses confusion (some advanced agents might output a special token or message like \"I am not sure how to proceed\"), the orchestrator should detect that and involve a human to clarify requirements or provide a hint. In summary, verification in agent orchestration is about establishing feedback loops and safety nets: tests to verify correctness, secondary agents or tools to verify quality, and structured checks (manifests, linters) to enforce constraints. By combining these, orchestrators can confidently determine when an agent \"succeeded\" and its output is acceptable.",
    "snippet": "Automated Testing as Ground Truth: One of the most powerful verification techniques is running tests on the agent's output. If the task comes with a test suite or at least some example cases, the...",
    "tags": [
      "research",
      "agents",
      "orchestration",
      "survey"
    ],
    "type": "essay"
  },
  {
    "id": "orchestration-survey#failure-modes-recovery-strategies",
    "url": "/orchestration-survey.html#failure-modes-recovery-strategies",
    "section_id": "failure-modes-recovery-strategies-2b388601",
    "section_hash": "2b38860147078fd4b353d785b2768bec9681999c236325fef05a0265f6797237",
    "title": "Design Patterns for Orchestrating Black-Box Code Agents",
    "section_title": "Failure Modes & Recovery Strategies #",
    "content": "Even with the best planning, agents will sometimes fail – producing wrong code, no code, or even crashing. The orchestration layer must be resilient to these failures and have recovery strategies. Graceful Retries: The simplest recovery pattern is retrying the same prompt. Because LLM agents are stochastic, a second attempt might yield a different (possibly correct) result. Orchestrators often implement a retry-on-failure (with a limit on retries to avoid loops). If an agent times out or returns an error (e.g. if using an API that errors out), the orchestrator can automatically retry after a short delay or with a smaller context. The Claude agent farm tool, for example, monitors each agent's health and will auto-restart an agent if it crashes or gets stuck. This ensures that a transient glitch doesn't halt the entire workflow. Refining the Prompt or Task Breakdown: If a direct retry doesn't help, the orchestrator should try a modified approach. One common strategy is progressive prompting: if the agent failed to produce output, perhaps the instructions were too broad – so break the task into smaller sub-tasks and prompt those instead. For instance, if \"implement the foobar algorithm\" came back empty or incorrect, the orchestrator might break it into \"step 1: implement the helper function X\" and \"step 2: implement main foobar using X\". Alternatively, the orchestrator can supply more guidance in the prompt on a retry – e.g. provide a partial solution or additional hints. This resembles a fallback to a more guided prompt when the agent seems to be stuck with a high-level request. In the Prompt-Chaining considerations, it's suggested to re-prompt including details of the failure in the context if an intermediate step fails [@pujals2025agentic]. For example: \"The previous attempt resulted in a runtime error at line 20. Here is the error log… Please fix the code to handle this issue.\" This gives the agent new information to succeed on the next pass. Alternate Agents or Models: Another recovery pattern is trying a different agent or model. If one model repeatedly fails to solve a problem, perhaps a more powerful model (or one fine-tuned differently) could succeed. PerfOrch demonstrates this by maintaining a ranking of models for each stage – if the top choice fails, it falls back to the next best model for that task [@wang2025perforch]. In their bug-fixing stage, they iteratively invoked up to n different specialized LLMs until one fixed the bug. Similarly, if an agent like GPT-3.5 is not producing a solution, an orchestrator might escalate to GPT-4 for that task. This is a cost-performance trade-off: use cheaper models for most tasks, but have a path to involve a more capable (or simply different) model on failure. Preserving Partial Progress: In complex multi-step tasks, an agent might accomplish some steps successfully before failing at a later step. A robust orchestrator will preserve this partial progress so it's not lost. For example, if an agent wrote 80% of a file correctly and then struggled, the orchestrator can save the file (or keep it in memory) and instruct a subsequent agent to continue from that state. This could involve checking the agent's output for usable sections. Some frameworks treat the codebase like a persistent state that agents incrementally build – even if one agent stops mid-way, the next agent sees the codebase with whatever was added so far. Escalating to Human or Halting: Not all failures can or should be handled autonomously. A critical safety mechanism is to escalate to a human when automated retries are exhausted or when the cost of more retries outweighs the benefit. The orchestrator might, for instance, make 3 attempts at a task (each possibly with refined prompts or different models). If none succeed, it can stop and label the task for human attention. In a CI/CD integration, this could mean leaving a comment like \"AI agent could not resolve this issue – please review manually.\" Logging and Learning from Failures: Another aspect of handling failures is logging them for continuous improvement. The orchestrator should log error cases, so developers can later analyze if prompt tweaks or training could solve them. In some advanced setups, this log is fed into a \"reflexion\" agent that tries to learn from failures by adjusting future prompts. In essence, failure recovery in orchestration is about resilience and adaptability. Try again (maybe differently), switch strategies if needed, and know when to ask for help.",
    "snippet": "Even with the best planning, agents will sometimes fail – producing wrong code, no code, or even crashing. The orchestration layer must be resilient to these failures and have recovery strategies....",
    "tags": [
      "research",
      "agents",
      "orchestration",
      "survey"
    ],
    "type": "essay"
  },
  {
    "id": "orchestration-survey#multi-pass-architectures-planner-implementer-reviewer-etc",
    "url": "/orchestration-survey.html#multi-pass-architectures-planner-implementer-reviewer-etc",
    "section_id": "multi-pass-architectures-planner-implementer-reviewer-etc-791d9829",
    "section_hash": "791d98295aad1365f936f4b58ae5f6f14b6a1e573c7746e4fa7b5f903a4f24ca",
    "title": "Design Patterns for Orchestrating Black-Box Code Agents",
    "section_title": "Multi-Pass Architectures (Planner, Implementer, Reviewer, etc.) #",
    "content": "One of the most powerful design patterns emerging in AI code generation is the use of multiple passes or multiple agent roles to tackle a problem in stages. Instead of a single monolithic prompt-to-code step, the task is divided into phases such as planning, implementing, and reviewing. Planner → Implementer Workflow: In this pattern, a Planner agent first creates a high-level plan or design, which is then used by an Implementer agent to write the actual code. The planner might outline which functions need to be created, what the approach will be, or even pseudo-code. The implementer then follows this blueprint. The self-planning study exemplifies this: the LLM generates concise, structured planning steps from the intent, then in the implementation phase it \"generates code step by step, guided by the preceding planning steps\" [@jiang2024selfplanning]. This two-pass approach yielded significantly higher correctness than a single-pass solution, because the model effectively did some problem decomposition and reasoning out loud before coding. MapCoder extends this idea to four stages: it uses one agent to recall relevant examples, another to plan the solution approach, a third to generate code, and a fourth to debug the result [@islam2024mapcoder]. Each agent is specialized for its role, and together they \"replicate the full cycle of program synthesis as observed in human developers\". The results were state-of-the-art on code benchmarks, underlining the efficacy of a multi-pass pipeline. Reviewer or Refiner Pass: Another common multi-pass setup is having a Reviewer agent or second pass that checks and improves the code written by an Implementer (first pass). In a multi-pass architecture, this is built-in: after code generation, the process automatically goes to a review phase. The reviewer might be tasked with ensuring the code meets the spec and then either directly editing the code to fix issues or providing feedback for the implementer to act on. This can even become an iterative loop: implementer → reviewer → implementer (for revisions) → reviewer … until the reviewer is satisfied. This approach aligns with the Evaluator-Optimizer iterative refinement pattern [@carpintero2025evaluator]. When Multi-Pass Helps: Multi-pass shines especially for complex tasks that benefit from explicit reasoning or quality control. If a coding task requires sophisticated reasoning (algorithm design, complex logic), a planning pass helps. If it requires high assurance (safety-critical code, or just code in a large codebase where consistency matters), a review/testing pass helps. On simpler tasks (e.g. \"add a print statement\"), multi-pass might be overkill – a single agent can handle it and the overhead of planning/reviewing might outweigh the simplicity of the change. Feedback Loops: Multi-pass architectures inherently create a feedback loop, especially with a reviewer/corrector in the mix. The orchestrator should feed the feedback in a form the implementer can act on. For example, if a reviewer says \"Function foo doesn't handle negative inputs properly,\" the orchestrator might prompt the implementer agent with that exact feedback: \"The code was reviewed and the following issue was found: it doesn't handle negative inputs. Please update the code to address this.\"",
    "snippet": "One of the most powerful design patterns emerging in AI code generation is the use of multiple passes or multiple agent roles to tackle a problem in stages. Instead of a single monolithic...",
    "tags": [
      "research",
      "agents",
      "orchestration",
      "survey"
    ],
    "type": "essay"
  },
  {
    "id": "orchestration-survey#human-in-the-loop-in-orchestration",
    "url": "/orchestration-survey.html#human-in-the-loop-in-orchestration",
    "section_id": "human-in-the-loop-in-orchestration-e1a3f7a0",
    "section_hash": "e1a3f7a01efcf2210f3db846b9c6e7d80f847898d27fc343fc4430dfc3cd0ca0",
    "title": "Design Patterns for Orchestrating Black-Box Code Agents",
    "section_title": "Human-in-the-Loop in Orchestration #",
    "content": "Even with all the above automation, the human element remains important. Orchestration should consider when to involve human developers or stakeholders to guide or approve the process. Checkpoints for Human Input: A well-designed orchestrator will define certain points at which it pauses and seeks human input. This might be after planning – e.g., once a Planner agent produces a design, the orchestrator could show that plan to a human for approval before coding. It could also be after code generation but before acceptance – essentially treating the AI-generated code like a regular code review that a human must OK. Surviving \"I'm stuck\" vs \"I'm done\": Differentiating between an agent that believes it's finished vs one that gave up is key. If an agent says \"Solution complete\" but the tests are failing, clearly it was wrongly done. The orchestrator should treat that as a failure (and possibly loop in a reviewer agent or human). On the other hand, if after several tries the agent outputs, \"I cannot find an approach for this problem,\" then the orchestrator has an impasse. This is a prime moment for human-in-the-loop. Human Oversight and Approval: Humans may also impose guardrails by setting policies the orchestrator follows. For example, an organization might require that any code touching security-critical files must be reviewed by a human, no matter what. The orchestrator can have a rule: if task involves auth.py (or generally high-risk area), always assign it to a human or at least get human approval after the AI suggests changes. Balancing Autonomy and Oversight: The goal of orchestration is often to minimize human effort, but not to eliminate it entirely at the cost of quality. An insightful guideline from Anthropic is to use the simplest solution possible and only add complexity (autonomy) when needed [@anthropic2024agents]. This suggests that if a task can be handled with a single LLM call plus a quick human tweak, that might be preferable to spinning up a whole autonomous multi-agent system.",
    "snippet": "Even with all the above automation, the human element remains important. Orchestration should consider when to involve human developers or stakeholders to guide or approve the process. Checkpoints...",
    "tags": [
      "research",
      "agents",
      "orchestration",
      "survey"
    ],
    "type": "essay"
  },
  {
    "id": "orchestration-survey#conclusion",
    "url": "/orchestration-survey.html#conclusion",
    "section_id": "conclusion-1758a73e",
    "section_hash": "1758a73e3430b090afb7a51d62bd4abb38a3e8530ac58d58763a391121d2fff1",
    "title": "Design Patterns for Orchestrating Black-Box Code Agents",
    "section_title": "Conclusion #",
    "content": "Orchestrating black-box code agents is an exercise in applying sound software engineering principles to AI-driven automation. The patterns identified – from task assignment and context curation to multi-pass workflows and human oversight – collectively form a toolkit for reliable AI-assisted development. In designing an orchestration framework, one should treat AI agents as fallible collaborators that need structure and guidance – much like human junior developers. The orchestrator's role is part project manager, part version-control system, and part QA engineer for this AI \"team.\" It decides who does what (task assignment), gives them what they need (context), tells them how to do it (prompt engineering and constraints), lets them work together safely (parallelism control), checks their work (verification), handles when they get it wrong (failure recovery), encourages iterative improvement (multi-pass), and knows when to ask a senior (human-in-loop).",
    "snippet": "Orchestrating black-box code agents is an exercise in applying sound software engineering principles to AI-driven automation. The patterns identified – from task assignment and context curation to...",
    "tags": [
      "research",
      "agents",
      "orchestration",
      "survey"
    ],
    "type": "essay"
  },
  {
    "id": "context-injection#context-injection",
    "url": "/context-injection.html#context-injection",
    "section_id": "context-injection-90dffdcd",
    "section_hash": "90dffdcd905c97635c6c28e649617aacf41eb5227ab2e59b78dcf0e38117be74",
    "title": "Context Injection",
    "section_title": "Context Injection #",
    "content": "What information noface provides to agents and how.",
    "snippet": "What information noface provides to agents and how.",
    "tags": [
      "design",
      "context",
      "prompts",
      "monowiki"
    ],
    "type": "essay"
  },
  {
    "id": "context-injection#current-design",
    "url": "/context-injection.html#current-design",
    "section_id": "current-design-b9e45972",
    "section_hash": "b9e459728a8bfb5540294ef6e56e5f366e6e700ea102dc848b79d668a5d7e592",
    "title": "Context Injection",
    "section_title": "Current Design #",
    "content": "The implementation prompt includes: Issue description — from beads Manifest — which files the agent can touch Build/test commands — from .noface.toml Design docs — fetched from monowiki if configured Workflow instructions — step-by-step process (implement → test → commit) Context is capped by max_context_docs setting (default 5).",
    "snippet": "The implementation prompt includes: Issue description — from beads Manifest — which files the agent can touch Build/test commands — from .noface.toml Design docs — fetched from monowiki if configured...",
    "tags": [
      "design",
      "context",
      "prompts",
      "monowiki"
    ],
    "type": "essay"
  },
  {
    "id": "context-injection#relation-to-survey",
    "url": "/context-injection.html#relation-to-survey",
    "section_id": "relation-to-survey-7b4d541e",
    "section_hash": "7b4d541eadd7fe36b7cebbd4c9217be5535fdf3c9843a8e767ee54e15ec540a9",
    "title": "Context Injection",
    "section_title": "Relation to Survey #",
    "content": "The survey warns about context dilution : \"Recent studies have shown that even if all provided context is relevant, performance can degrade substantially (by 13–85%) as input length grows.\" And recommends iterative context expansion : \"The Context-Augmentation pattern described by Hugging Face highlights that context expansion should be iterative and need-based: the agent identifies what extra info it needs, the orchestrator fetches it.\"",
    "snippet": "The survey warns about context dilution : \"Recent studies have shown that even if all provided context is relevant, performance can degrade substantially (by 13–85%) as input length grows.\" And...",
    "tags": [
      "design",
      "context",
      "prompts",
      "monowiki"
    ],
    "type": "essay"
  },
  {
    "id": "context-injection#1-context-budget-dynamic-based-on-token-budget-and-complexity",
    "url": "/context-injection.html#1-context-budget-dynamic-based-on-token-budget-and-complexity",
    "section_id": "1-context-budget-dynamic-based-on-token-budget-and-complexity-e1137167",
    "section_hash": "e1137167fa11f4f807f23646445c027aa735c91021740404a9396bded994f8a9",
    "title": "Context Injection",
    "section_title": "1. Context Budget: Dynamic based on token budget and complexity #",
    "content": "Decision: Make context budget dynamic, not fixed. Approach: Define a target fraction of prompt tokens for retrieved docs (e.g., 30–40%) Fill that slot with as many top-ranked docs as fit Complexity heuristics: More files / cross-cutting change → allow more docs Tiny, local change → maybe 0–2 docs only Keep max_context_docs = 5 as a hard cap, but choose K ∈ [0, 5] per issue based on: Issue complexity score Available token budget Relevance scores of candidate docs",
    "snippet": "Decision: Make context budget dynamic, not fixed. Approach: Define a target fraction of prompt tokens for retrieved docs (e.g., 30–40%) Fill that slot with as many top-ranked docs as fit Complexity...",
    "tags": [
      "design",
      "context",
      "prompts",
      "monowiki"
    ],
    "type": "essay"
  },
  {
    "id": "context-injection#2-relevance-ranking-hybrid-bm25-embeddings",
    "url": "/context-injection.html#2-relevance-ranking-hybrid-bm25-embeddings",
    "section_id": "2-relevance-ranking-hybrid-bm25-embeddings-756cb11e",
    "section_hash": "756cb11e8a17e628b46466bc186eed031b58bc38b0716912cd35a2de1d7f87e2",
    "title": "Context Injection",
    "section_title": "2. Relevance Ranking: Hybrid BM25 + embeddings #",
    "content": "Decision: Use hybrid ranking instead of simple wikilink fetching. Pipeline: Candidate generation: Wikilinks / explicit references from the issue BM25 search (already have src/bm25.zig ) Re-ranking: Embedding similarity (doc ↔ issue description) Final score: Copy score = α * BM25 + β * embedding_score + boost_if_explicitly_linked This will beat naive \"first N by wikilink\" in most repos.",
    "snippet": "Decision: Use hybrid ranking instead of simple wikilink fetching. Pipeline: Candidate generation: Wikilinks / explicit references from the issue BM25 search (already have src/bm25.zig ) Re-ranking:...",
    "tags": [
      "design",
      "context",
      "prompts",
      "monowiki"
    ],
    "type": "essay"
  },
  {
    "id": "context-injection#3-code-context-manifest-files-relevant-snippets",
    "url": "/context-injection.html#3-code-context-manifest-files-relevant-snippets",
    "section_id": "3-code-context-manifest-files-relevant-snippets-42be166c",
    "section_hash": "42be166ce253fab089f372f8f129e369ac9591c38770a7c154603dfaaaeccab0",
    "title": "Context Injection",
    "section_title": "3. Code Context: Manifest files + relevant snippets #",
    "content": "Decision: Include manifest files plus targeted snippets from related code. Always include: The files in the manifest that the agent is allowed to edit Optionally include: Small snippets from related files: Direct callers/callees (if xref info available) BM25/embedding matches for key identifiers in the issue Guardrails: Truncate to relevant functions rather than whole files when possible If file is huge, include: Signature + docstring + 1–2 nearby functions Use BM25 over code to pick relevant ranges, not entire files",
    "snippet": "Decision: Include manifest files plus targeted snippets from related code. Always include: The files in the manifest that the agent is allowed to edit Optionally include: Small snippets from related...",
    "tags": [
      "design",
      "context",
      "prompts",
      "monowiki"
    ],
    "type": "essay"
  },
  {
    "id": "context-injection#4-context-freshness-downrank-stale-docs-dont-hard-exclude",
    "url": "/context-injection.html#4-context-freshness-downrank-stale-docs-dont-hard-exclude",
    "section_id": "4-context-freshness-downrank-stale-docs-dont-hard-exclude-a54de363",
    "section_hash": "a54de363bdea6279077720f2f8d7c12df75a43a3568649ad2fc03bc8d9bc4c0f",
    "title": "Context Injection",
    "section_title": "4. Context Freshness: Downrank stale docs, don't hard exclude #",
    "content": "Decision: Add freshness metadata + downranking, not hard exclusion. For each doc, track: last_updated timestamp Optionally \"tied-to-commit\" info Heuristics: If doc references symbols that no longer exist in code → heavily downrank or mark stale If now - last_updated > threshold and code around it changed a lot → downrank In the prompt, for borderline docs: Copy Note: this doc may be outdated; prefer the current code as source of truth. Relevant but slightly stale docs can still help, but the model is warned.",
    "snippet": "Decision: Add freshness metadata + downranking, not hard exclusion. For each doc, track: last_updated timestamp Optionally \"tied-to-commit\" info Heuristics: If doc references symbols that no longer...",
    "tags": [
      "design",
      "context",
      "prompts",
      "monowiki"
    ],
    "type": "essay"
  },
  {
    "id": "context-injection#5-negative-context-explicit-exclusion-list",
    "url": "/context-injection.html#5-negative-context-explicit-exclusion-list",
    "section_id": "5-negative-context-explicit-exclusion-list-55395f0a",
    "section_hash": "55395f0a049637260f6acbdc95ae2ecd0e6e5924bee4d36cf2973990fce5e120",
    "title": "Context Injection",
    "section_title": "5. Negative Context: Explicit exclusion list #",
    "content": "Decision: Define explicit exclusions + heuristics. Path-based exclusions: vendor/ , node_modules/ , dist/ , build/ , .cache/ *.min.js , large generated protobufs Size-based: Files > X KB not included unless explicitly requested Type-based: Binary blobs, lockfiles, big JSON dumps Implement in retrieval pipeline so excluded files never show up as candidates.",
    "snippet": "Decision: Define explicit exclusions + heuristics. Path-based exclusions: vendor/ , node_modules/ , dist/ , build/ , .cache/ *.min.js , large generated protobufs Size-based: Files > X KB not included...",
    "tags": [
      "design",
      "context",
      "prompts",
      "monowiki"
    ],
    "type": "essay"
  },
  {
    "id": "context-injection#6-agent-requested-context-thin-orchestrator-layer-harness-delegation",
    "url": "/context-injection.html#6-agent-requested-context-thin-orchestrator-layer-harness-delegation",
    "section_id": "6-agent-requested-context-thin-orchestrator-layer-harness-delegation-dbbfeb62",
    "section_hash": "dbbfeb62c20f7478973f307ac2354e270e68e0179e68a5014679a86a607a62f2",
    "title": "Context Injection",
    "section_title": "6. Agent-Requested Context: Thin orchestrator layer + harness delegation #",
    "content": "Decision: Lean on Claude Code's native file-opening capabilities, add thin orchestrator layer for explicit requests. If using Claude Code / similar harness: Let the harness handle intra-session context (open file, run search, etc.) At orchestration level: Support structured \"needs more context\" signals: Copy NEED_CODE: auth/login.zig NEED_DOC: docs/auth.md When signal detected: Fetch & add context on next call, or Delegate to harness's file-opening APIs Don't build a whole second interactive context loop on top of Claude. Do add a thin layer to observe/log/respond when agent clearly asks for more info.",
    "snippet": "Decision: Lean on Claude Code's native file-opening capabilities, add thin orchestrator layer for explicit requests. If using Claude Code / similar harness: Let the harness handle intra-session...",
    "tags": [
      "design",
      "context",
      "prompts",
      "monowiki"
    ],
    "type": "essay"
  },
  {
    "id": "context-injection#implementation-notes",
    "url": "/context-injection.html#implementation-notes",
    "section_id": "implementation-notes-6112792c",
    "section_hash": "6112792c4a51b4b51a2d8559be77ed020368dd9403c1d57b3e5dae3f28935ef3",
    "title": "Context Injection",
    "section_title": "Implementation Notes #",
    "content": "See src/loop.zig:buildImplementationPrompt and src/monowiki.zig .",
    "snippet": "See src/loop.zig:buildImplementationPrompt and src/monowiki.zig .",
    "tags": [
      "design",
      "context",
      "prompts",
      "monowiki"
    ],
    "type": "essay"
  },
  {
    "id": "context-injection#todo",
    "url": "/context-injection.html#todo",
    "section_id": "todo-22218856",
    "section_hash": "2221885678d42157df1e3b35e6f8443f5f88943853d6fd7a4a4ca6d3e4d45d69",
    "title": "Context Injection",
    "section_title": "TODO #",
    "content": "Implement dynamic context budget based on issue complexity Add hybrid ranking (BM25 + embeddings) Add freshness tracking to monowiki docs Implement exclusion list in retrieval pipeline Parse NEED_CODE / NEED_DOC signals from agent output",
    "snippet": "Implement dynamic context budget based on issue complexity Add hybrid ranking (BM25 + embeddings) Add freshness tracking to monowiki docs Implement exclusion list in retrieval pipeline Parse...",
    "tags": [
      "design",
      "context",
      "prompts",
      "monowiki"
    ],
    "type": "essay"
  },
  {
    "id": "parallel-execution#parallel-execution",
    "url": "/parallel-execution.html#parallel-execution",
    "section_id": "parallel-execution-2cec95ca",
    "section_hash": "2cec95ca473208e1e362f46d3d0029890ea9452eb842eef274641537a034b217",
    "title": "Parallel Execution",
    "section_title": "Parallel Execution #",
    "content": "How noface runs multiple agents concurrently without conflicts.",
    "snippet": "How noface runs multiple agents concurrently without conflicts.",
    "tags": [
      "design",
      "parallelism",
      "batching",
      "locking"
    ],
    "type": "essay"
  },
  {
    "id": "parallel-execution#current-design",
    "url": "/parallel-execution.html#current-design",
    "section_id": "current-design-684d83b1",
    "section_hash": "684d83b1c716965444f1ce6b54f8ec246d07b9c3cd5aa85da8992accd941ac0b",
    "title": "Parallel Execution",
    "section_title": "Current Design #",
    "content": "Planner pass groups issues into batches based on manifest analysis Issues in the same batch have disjoint primary_files (no overlap) Worker pool spawns up to N parallel processes (configurable, default 8) Lock entries track which files are held by which worker Batches execute sequentially; issues within a batch execute in parallel",
    "snippet": "Planner pass groups issues into batches based on manifest analysis Issues in the same batch have disjoint primary_files (no overlap) Worker pool spawns up to N parallel processes (configurable,...",
    "tags": [
      "design",
      "parallelism",
      "batching",
      "locking"
    ],
    "type": "essay"
  },
  {
    "id": "parallel-execution#relation-to-survey",
    "url": "/parallel-execution.html#relation-to-survey",
    "section_id": "relation-to-survey-0826a9ae",
    "section_hash": "0826a9aee8d0140e821d300db8cdde601fbca85428f550adb4e2a4e8fdca5571",
    "title": "Parallel Execution",
    "section_title": "Relation to Survey #",
    "content": "This implements the file-level locking pattern from orchestration-survey : \"A community-built 'Claude code agent farm' demonstrates this with a coordination protocol: each agent must create a lock file listing the files/features it intends to work on, and consult a shared active work registry to ensure no overlap.\" The survey also discusses CRDTs for lock-free coordination (CodeCRDT achieved 100% merge convergence). This is more scalable but more complex.",
    "snippet": "This implements the file-level locking pattern from orchestration-survey : \"A community-built 'Claude code agent farm' demonstrates this with a coordination protocol: each agent must create a lock...",
    "tags": [
      "design",
      "parallelism",
      "batching",
      "locking"
    ],
    "type": "essay"
  },
  {
    "id": "parallel-execution#1-lock-granularity-file-level-only-for-now",
    "url": "/parallel-execution.html#1-lock-granularity-file-level-only-for-now",
    "section_id": "1-lock-granularity-file-level-only-for-now-1d37a559",
    "section_hash": "1d37a5592cc2f093110cc8441de9f8869e67b0e222dad4122d9c5afd290f9a13",
    "title": "Parallel Execution",
    "section_title": "1. Lock Granularity: File-level only (for now) #",
    "content": "Decision: Start with file-level locking only. Add function-level as a future optimization for specific languages. File-level: Easy to reason about Guarantees no diff overlap Function-level (deferred): Needs parser, symbol table, stable function IDs Harder for languages with macros, generated code, ad-hoc patterns Instead, get most of the win by: Task design: aim one issue per file or small set of files Using manifest to ensure each issue's scope is small and minimally overlapping If later we need function-level: Limit to languages where we already have a good AST + symbol index Keep file-level as fallback",
    "snippet": "Decision: Start with file-level locking only. Add function-level as a future optimization for specific languages. File-level: Easy to reason about Guarantees no diff overlap Function-level...",
    "tags": [
      "design",
      "parallelism",
      "batching",
      "locking"
    ],
    "type": "essay"
  },
  {
    "id": "parallel-execution#2-batch-sizing-adaptive-concurrency",
    "url": "/parallel-execution.html#2-batch-sizing-adaptive-concurrency",
    "section_id": "2-batch-sizing-adaptive-concurrency-10e572c1",
    "section_hash": "10e572c17b7a8bbb8912afb03abaecca1ab771e15f419480d31c1cc2831951c1",
    "title": "Parallel Execution",
    "section_title": "2. Batch Sizing: Adaptive concurrency #",
    "content": "Decision: Use adaptive concurrency based on runtime signals. Config: Copy [parallelism] max_concurrent_global = 8 # hard cap max_concurrent_per_repo = 4 # per-repo cap initial_concurrency = 2 # starting point Runtime signals: Average agent latency Queue length Error rate Policy: Start with N = 2 If queue backlog grows and success rate is high → increase toward max If failures/timeouts increase or host is resource constrained → back off Also: maintain a \"serial bucket\" for issues touching very hot files (see below).",
    "snippet": "Decision: Use adaptive concurrency based on runtime signals. Config: Copy [parallelism] max_concurrent_global = 8 # hard cap max_concurrent_per_repo = 4 # per-repo cap initial_concurrency = 2 #...",
    "tags": [
      "design",
      "parallelism",
      "batching",
      "locking"
    ],
    "type": "essay"
  },
  {
    "id": "parallel-execution#3-lock-contention-hot-file-detection-mitigation",
    "url": "/parallel-execution.html#3-lock-contention-hot-file-detection-mitigation",
    "section_id": "3-lock-contention-hot-file-detection-mitigation-a38ebee2",
    "section_hash": "a38ebee24fe7a9ba8bfdec9f41df58b642a346a1caf2ecd818e423bad726f0cb",
    "title": "Parallel Execution",
    "section_title": "3. Lock Contention: Hot file detection + mitigation #",
    "content": "Decision: Track per-file lock statistics and treat \"hot files\" specially. Metrics to track: Lock wait time per file Number of pending issues referencing each file If a file (e.g., main.zig ) is often a bottleneck: Force tasks that touch that file into a single-threaded lane (queue) Encourage planner to batch related issues touching that file into one larger issue Suggest refactor tasks: spawn meta-issue \"split main.zig into modules\" Implementation: Copy // In-memory or persisted const HotFileStats = struct { lock_count: u32, avg_wait_ms: u64, queue_depth: u32, }; // Map: file_path -> HotFileStats Use this when scheduling new issues.",
    "snippet": "Decision: Track per-file lock statistics and treat \"hot files\" specially. Metrics to track: Lock wait time per file Number of pending issues referencing each file If a file (e.g., main.zig ) is often...",
    "tags": [
      "design",
      "parallelism",
      "batching",
      "locking"
    ],
    "type": "essay"
  },
  {
    "id": "parallel-execution#4-conflict-recovery-three-step-policy",
    "url": "/parallel-execution.html#4-conflict-recovery-three-step-policy",
    "section_id": "4-conflict-recovery-three-step-policy-d2ac5c61",
    "section_hash": "d2ac5c61529cab8eaaf75c1745abca2c264220567546abb7b10290322e5e7438",
    "title": "Parallel Execution",
    "section_title": "4. Conflict Recovery: Three-step policy #",
    "content": "Decision: Define a concrete conflict resolution pipeline. Step 1: Detect Two diffs touch overlapping hunks, or git apply fails Step 2: Auto-merge Try a 3-way merge (base, A, B) If succeeds cleanly, run tests If tests pass, accept merged result Step 3: LLM-assisted merge Spawn a \"merge agent\" with both diffs + conflicts Ask it to produce a unified patch Run tests again Step 4: Escalate If merge agent fails or tests still fail Escalate to human Leave a merge PR with both original diffs + explanation",
    "snippet": "Decision: Define a concrete conflict resolution pipeline. Step 1: Detect Two diffs touch overlapping hunks, or git apply fails Step 2: Auto-merge Try a 3-way merge (base, A, B) If succeeds cleanly,...",
    "tags": [
      "design",
      "parallelism",
      "batching",
      "locking"
    ],
    "type": "essay"
  },
  {
    "id": "parallel-execution#5-crdt-exploration-ast-aware-patching-as-middle-ground",
    "url": "/parallel-execution.html#5-crdt-exploration-ast-aware-patching-as-middle-ground",
    "section_id": "5-crdt-exploration-ast-aware-patching-as-middle-ground-f7f98dea",
    "section_hash": "f7f98deaa551f4ec97548ab14cfe21ada63830d82af9c1cd62633d37d0292776",
    "title": "Parallel Execution",
    "section_title": "5. CRDT Exploration: AST-aware patching as middle ground #",
    "content": "Decision: Use AST-aware patching + 3-way merge instead of full CRDTs. Approach: Ask agents to emit structured edits per file: \"Replace function foo body with …\" \"Add new function bar…\" Internally apply these as AST transformations, not raw text splice Merge behavior: Two agents modifying different functions in the same file → trivially merge Same function → conflict; go through the conflict policy above This gets most of the benefits of CRDT-style structural merges without full-blown replicated state machinery.",
    "snippet": "Decision: Use AST-aware patching + 3-way merge instead of full CRDTs. Approach: Ask agents to emit structured edits per file: \"Replace function foo body with …\" \"Add new function bar…\" Internally...",
    "tags": [
      "design",
      "parallelism",
      "batching",
      "locking"
    ],
    "type": "essay"
  },
  {
    "id": "parallel-execution#implementation-notes",
    "url": "/parallel-execution.html#implementation-notes",
    "section_id": "implementation-notes-263b3aa2",
    "section_hash": "263b3aa23ca4b472de7568bb9b1ade6e97d6d58b6c668893a9a42e1842c94ca9",
    "title": "Parallel Execution",
    "section_title": "Implementation Notes #",
    "content": "See src/worker_pool.zig:WorkerPool and src/state.zig:LockEntry .",
    "snippet": "See src/worker_pool.zig:WorkerPool and src/state.zig:LockEntry .",
    "tags": [
      "design",
      "parallelism",
      "batching",
      "locking"
    ],
    "type": "essay"
  },
  {
    "id": "parallel-execution#todo",
    "url": "/parallel-execution.html#todo",
    "section_id": "todo-45184ce8",
    "section_hash": "45184ce845bcb1b966d08597b5456f3292bf83a22d6234f5d77a9fc9ef5e85fb",
    "title": "Parallel Execution",
    "section_title": "TODO #",
    "content": "Add HotFileStats tracking Implement adaptive concurrency (start low, ramp up) Add serial lane for hot files Implement 3-way merge fallback Design structured edit format for AST-aware patching",
    "snippet": "Add HotFileStats tracking Implement adaptive concurrency (start low, ramp up) Add serial lane for hot files Implement 3-way merge fallback Design structured edit format for AST-aware patching",
    "tags": [
      "design",
      "parallelism",
      "batching",
      "locking"
    ],
    "type": "essay"
  },
  {
    "id": "failure-recovery#failure-recovery",
    "url": "/failure-recovery.html#failure-recovery",
    "section_id": "failure-recovery-b953c4f5",
    "section_hash": "b953c4f52789451edbefe8d0fc6c7a05f03b14b15a1a0bd74d38219d2afa1e34",
    "title": "Failure Recovery",
    "section_title": "Failure Recovery #",
    "content": "How noface handles agent failures.",
    "snippet": "How noface handles agent failures.",
    "tags": [
      "design",
      "failure",
      "retry",
      "recovery"
    ],
    "type": "essay"
  },
  {
    "id": "failure-recovery#current-design",
    "url": "/failure-recovery.html#current-design",
    "section_id": "current-design-0345ec91",
    "section_hash": "0345ec9189ef76d200518f587036a7c21df559e3eeb10df3153d7acf5c4a11da",
    "title": "Failure Recovery",
    "section_title": "Current Design #",
    "content": "noface handles several failure modes: Transient failures (429, 5xx, network) — retry up to 3x with exponential backoff Manifest violations — rollback offending files, retry with stricter prompt Timeouts (no output for N seconds) — break down issue into smaller tasks Crash recovery — on startup, detect in-progress work from previous run, reset stale locks, restore state Each attempt is recorded in state with outcome (success/failed/timeout/violation).",
    "snippet": "noface handles several failure modes: Transient failures (429, 5xx, network) — retry up to 3x with exponential backoff Manifest violations — rollback offending files, retry with stricter prompt...",
    "tags": [
      "design",
      "failure",
      "retry",
      "recovery"
    ],
    "type": "essay"
  },
  {
    "id": "failure-recovery#relation-to-survey",
    "url": "/failure-recovery.html#relation-to-survey",
    "section_id": "relation-to-survey-04fc4d15",
    "section_hash": "04fc4d1505436e53c036f0a2149bf7ff7624db04916eaa562567cfc6aeb6f797",
    "title": "Failure Recovery",
    "section_title": "Relation to Survey #",
    "content": "The survey describes several recovery patterns: Graceful retries: \"The simplest recovery pattern is retrying the same prompt. Because LLM agents are stochastic, a second attempt might yield a different (possibly correct) result.\" Progressive prompting: \"If a direct retry doesn't help, the orchestrator should try a modified approach... break the task into smaller sub-tasks and prompt those instead.\" Preserving partial progress: \"A robust orchestrator will preserve this partial progress so it's not lost.\"",
    "snippet": "The survey describes several recovery patterns: Graceful retries: \"The simplest recovery pattern is retrying the same prompt. Because LLM agents are stochastic, a second attempt might yield a...",
    "tags": [
      "design",
      "failure",
      "retry",
      "recovery"
    ],
    "type": "essay"
  },
  {
    "id": "failure-recovery#1-retry-strategies-informed-retries-not-blind",
    "url": "/failure-recovery.html#1-retry-strategies-informed-retries-not-blind",
    "section_id": "1-retry-strategies-informed-retries-not-blind-8943a372",
    "section_hash": "8943a372677cdbe6c7c6a890a85b8b448cb44f56f266414194c3093fe7267b60",
    "title": "Failure Recovery",
    "section_title": "1. Retry Strategies: Informed retries, not blind #",
    "content": "Decision: Retries should include failure context and self-reflection. On retry, include: Previous attempt's diff Summarized test output / error Instructions: Copy Your previous attempt failed with: [error]. Fix that specific problem while preserving working parts. Optional self-reflection step: Ask model: \"Explain why your last attempt failed and how you will fix it\" Feed that back into the actual implementation prompt",
    "snippet": "Decision: Retries should include failure context and self-reflection. On retry, include: Previous attempt's diff Summarized test output / error Instructions: Copy Your previous attempt failed with:...",
    "tags": [
      "design",
      "failure",
      "retry",
      "recovery"
    ],
    "type": "essay"
  },
  {
    "id": "failure-recovery#2-task-breakdown-minimal-concrete-decomposition",
    "url": "/failure-recovery.html#2-task-breakdown-minimal-concrete-decomposition",
    "section_id": "2-task-breakdown-minimal-concrete-decomposition-e55066de",
    "section_hash": "e55066de00c33e4314e81154fd100c69fb381d97341aee3114e0d2e2a23f41b8",
    "title": "Failure Recovery",
    "section_title": "2. Task Breakdown: Minimal, concrete decomposition #",
    "content": "Decision: Implement a simple breakdown strategy when issues are too large. Trigger breakdown when: Issue times out repeatedly, or Fails after N attempts with \"too big\" signature (many files, large diff) Breakdown agent: Run planner with a different prompt to propose sub-issues: e.g., \"Update schema in A\", \"Update API in B\", \"Update tests in C\" Turn these into child issues, mark parent as an \"epic\" First version (simple): Split by file: if manifest has 5 files, create 5 issues, each scoped to one file Iterate toward more semantic breakdowns later.",
    "snippet": "Decision: Implement a simple breakdown strategy when issues are too large. Trigger breakdown when: Issue times out repeatedly, or Fails after N attempts with \"too big\" signature (many files, large...",
    "tags": [
      "design",
      "failure",
      "retry",
      "recovery"
    ],
    "type": "essay"
  },
  {
    "id": "failure-recovery#3-partial-progress-keep-on-branch-all-or-nothing-to-main",
    "url": "/failure-recovery.html#3-partial-progress-keep-on-branch-all-or-nothing-to-main",
    "section_id": "3-partial-progress-keep-on-branch-all-or-nothing-to-main-dcd4d852",
    "section_hash": "dcd4d852ede0c91806abe945c211187c06793064409786ca0f7e4e9c579a6a61",
    "title": "Failure Recovery",
    "section_title": "3. Partial Progress: Keep on branch, all-or-nothing to main #",
    "content": "Decision: Preserve partial progress in scratch branches, but merge atomically. Design: Each issue has a scratch branch or temp workspace Each attempt commits to that branch If one file fails, you still keep the other 3 as commits in the branch Only when tests + gates pass do you merge that branch to main This preserves partial progress for future attempts without exposing half-baked changes to main.",
    "snippet": "Decision: Preserve partial progress in scratch branches, but merge atomically. Design: Each issue has a scratch branch or temp workspace Each attempt commits to that branch If one file fails, you...",
    "tags": [
      "design",
      "failure",
      "retry",
      "recovery"
    ],
    "type": "essay"
  },
  {
    "id": "failure-recovery#4-model-escalation-simple-escalation-policy",
    "url": "/failure-recovery.html#4-model-escalation-simple-escalation-policy",
    "section_id": "4-model-escalation-simple-escalation-policy-22b0234c",
    "section_hash": "22b0234ca42dcb4f54af9e8e0495f7d70149375b301731a09a936cb553743281",
    "title": "Failure Recovery",
    "section_title": "4. Model Escalation: Simple escalation policy #",
    "content": "Decision: Define a simple model escalation policy. For each issue: First 1–2 attempts: default_model (cheaper) If still failing with \"correctable\" errors (tests, syntax): escalate to strong_model Cap total attempts across all models Keep configurable so users can opt-out if they only have access to one model. Copy [retry] default_model = \"claude-sonnet\" escalation_model = \"claude-opus\" escalate_after_attempts = 2 max_total_attempts = 5",
    "snippet": "Decision: Define a simple model escalation policy. For each issue: First 1–2 attempts: default_model (cheaper) If still failing with \"correctable\" errors (tests, syntax): escalate to strong_model Cap...",
    "tags": [
      "design",
      "failure",
      "retry",
      "recovery"
    ],
    "type": "essay"
  },
  {
    "id": "failure-recovery#5-failure-classification-taxonomy-with-strategy-mapping",
    "url": "/failure-recovery.html#5-failure-classification-taxonomy-with-strategy-mapping",
    "section_id": "5-failure-classification-taxonomy-with-strategy-mapping-be24205d",
    "section_hash": "be24205d56b34fe9a3d59cadab61984ad27bef607f38a5989a4954e1921c0798",
    "title": "Failure Recovery",
    "section_title": "5. Failure Classification: Taxonomy with strategy mapping #",
    "content": "Decision: Introduce a failure taxonomy and map each to a strategy. Taxonomy: Failure Type Strategy SYNTAX_ERROR Re-prompt with same context + \"fix syntax error\" RUNTIME_ERROR Include stack trace, ask to fix TEST_FAILURE Include test output, ask to fix NO_DIFF \"You must change code; your previous attempt changed nothing\" TIMEOUT Break down task / reduce scope MANIFEST_VIOLATION Replan + retry with expanded manifest AGENT_CONFUSION Involve planner or human for clarification",
    "snippet": "Decision: Introduce a failure taxonomy and map each to a strategy. Taxonomy: Failure Type Strategy SYNTAX_ERROR Re-prompt with same context + \"fix syntax error\" RUNTIME_ERROR Include stack trace, ask...",
    "tags": [
      "design",
      "failure",
      "retry",
      "recovery"
    ],
    "type": "essay"
  },
  {
    "id": "failure-recovery#6-human-escalation-clear-threshold-summary",
    "url": "/failure-recovery.html#6-human-escalation-clear-threshold-summary",
    "section_id": "6-human-escalation-clear-threshold-summary-70499551",
    "section_hash": "70499551cb757656d9bc3218d0effcb362e9fc659e21776f6c10742adb87453b",
    "title": "Failure Recovery",
    "section_title": "6. Human Escalation: Clear threshold + summary #",
    "content": "Decision: Define a clear escalation threshold with actionable summary. After N consecutive failed attempts (e.g., 3) or severe failure type: Mark issue as BLOCKED Post a summary: What was tried Errors encountered What the agent thinks is unclear (if any) Pause automation until human: Adds context Edits the issue, or Manually unblocks / retries",
    "snippet": "Decision: Define a clear escalation threshold with actionable summary. After N consecutive failed attempts (e.g., 3) or severe failure type: Mark issue as BLOCKED Post a summary: What was tried...",
    "tags": [
      "design",
      "failure",
      "retry",
      "recovery"
    ],
    "type": "essay"
  },
  {
    "id": "failure-recovery#7-learning-from-failures-lightweight-rule-based-lessons",
    "url": "/failure-recovery.html#7-learning-from-failures-lightweight-rule-based-lessons",
    "section_id": "7-learning-from-failures-lightweight-rule-based-lessons-d1c49f6e",
    "section_hash": "d1c49f6ee92fda41f71b54a8c66e8a3d34d6eb2cb8146d4e21c84bda2f649948",
    "title": "Failure Recovery",
    "section_title": "7. Learning from Failures: Lightweight rule-based lessons #",
    "content": "Decision: Start with manual rule-based \"lessons\", not ML. Maintain a small config/ruleset: Copy [[failure_hints]] label = \"migration\" hint = \"Migration issues often need schema changes first. Check schema files.\" [[failure_hints]] language = \"zig\" error_pattern = \"error.OutOfMemory\" hint = \"Consider using an arena allocator for temporary allocations.\" Periodically review failure logs, add new rules where patterns are obvious. Later: mine logs to auto-discover patterns, but manual rules get 80% of value quickly.",
    "snippet": "Decision: Start with manual rule-based \"lessons\", not ML. Maintain a small config/ruleset: Copy [[failure_hints]] label = \"migration\" hint = \"Migration issues often need schema changes first. Check...",
    "tags": [
      "design",
      "failure",
      "retry",
      "recovery"
    ],
    "type": "essay"
  },
  {
    "id": "failure-recovery#implementation-notes",
    "url": "/failure-recovery.html#implementation-notes",
    "section_id": "implementation-notes-cd9881ab",
    "section_hash": "cd9881abf1c4cf585a4953b8be6b2a1330e4bb2fd6e1927a05e2d7dbcaca74b4",
    "title": "Failure Recovery",
    "section_title": "Implementation Notes #",
    "content": "See src/loop.zig:runIteration retry logic and src/state.zig:IssueState .",
    "snippet": "See src/loop.zig:runIteration retry logic and src/state.zig:IssueState .",
    "tags": [
      "design",
      "failure",
      "retry",
      "recovery"
    ],
    "type": "essay"
  },
  {
    "id": "failure-recovery#todo",
    "url": "/failure-recovery.html#todo",
    "section_id": "todo-691a9db4",
    "section_hash": "691a9db47b0bb77e0c9f739df6c1df87a052caf0e10a98eb0e7bd8179ef79d75",
    "title": "Failure Recovery",
    "section_title": "TODO #",
    "content": "Add failure context to retry prompts Implement self-reflection step Add breakdown agent with file-split strategy Implement scratch branch model for partial progress Add model escalation policy Implement failure taxonomy enum Add BLOCKED status with human escalation summary Add failure_hints config section",
    "snippet": "Add failure context to retry prompts Implement self-reflection step Add breakdown agent with file-split strategy Implement scratch branch model for partial progress Add model escalation policy...",
    "tags": [
      "design",
      "failure",
      "retry",
      "recovery"
    ],
    "type": "essay"
  },
  {
    "id": "manifests#file-manifests",
    "url": "/manifests.html#file-manifests",
    "section_id": "file-manifests-c49f2c45",
    "section_hash": "c49f2c4522c66ce295656b808fbddf59592063762ba56b924adc55cf93bc299e",
    "title": "File Manifests",
    "section_title": "File Manifests #",
    "content": "How noface controls what files each agent can touch.",
    "snippet": "How noface controls what files each agent can touch.",
    "tags": [
      "design",
      "manifests",
      "access-control"
    ],
    "type": "essay"
  },
  {
    "id": "manifests#current-design",
    "url": "/manifests.html#current-design",
    "section_id": "current-design-f43c2d12",
    "section_hash": "f43c2d12814319715ff1679ab13c4e26ad9a43fbf2d150ede4d75e5ad08e5a8d",
    "title": "File Manifests",
    "section_title": "Current Design #",
    "content": "Manifests declare three access levels per issue: PRIMARY_FILES — exclusive write access (locked during execution) READ_FILES — shared read-only access FORBIDDEN_FILES — must never be touched The planner generates manifests by analyzing each issue. After an agent completes, noface runs git diff and verifies compliance. Violations trigger rollback of offending files and retry with a stricter prompt.",
    "snippet": "Manifests declare three access levels per issue: PRIMARY_FILES — exclusive write access (locked during execution) READ_FILES — shared read-only access FORBIDDEN_FILES — must never be touched The...",
    "tags": [
      "design",
      "manifests",
      "access-control"
    ],
    "type": "essay"
  },
  {
    "id": "manifests#relation-to-survey",
    "url": "/manifests.html#relation-to-survey",
    "section_id": "relation-to-survey-119104bd",
    "section_hash": "119104bd801cd616414935a27eaae86dc58ff5c47d4ae571fef16d92bc802c41",
    "title": "File Manifests",
    "section_title": "Relation to Survey #",
    "content": "This implements the Manifest-Driven AI Development (MAID) pattern from orchestration-survey : \"A recent methodology called Manifest-Driven AI Development (MAID) takes this further by treating the manifest as an enforceable contract: after generation, a validator checks that only the declared files/functions were altered and no undeclared edits occurred.\"",
    "snippet": "This implements the Manifest-Driven AI Development (MAID) pattern from orchestration-survey : \"A recent methodology called Manifest-Driven AI Development (MAID) takes this further by treating the...",
    "tags": [
      "design",
      "manifests",
      "access-control"
    ],
    "type": "essay"
  },
  {
    "id": "manifests#1-granularity-file-level-with-soft-function-hints",
    "url": "/manifests.html#1-granularity-file-level-with-soft-function-hints",
    "section_id": "1-granularity-file-level-with-soft-function-hints-a89359cc",
    "section_hash": "a89359ccafc9ee07ed0568b226d9a9e88c8d058d74893455be82cde3c507b30e",
    "title": "File Manifests",
    "section_title": "1. Granularity: File-level with soft function hints #",
    "content": "Decision: Stick to file-level manifests as the hard safety boundary. Add soft function-level hints inside the prompt. Use the manifest as: \"these are the only files you're allowed to change\" Inside each file, include function-level \"edit targets\" in the prompt: Copy You may edit only these functions in foo.zig: [foo, bar]. Line-ranges are brittle (they drift with edits and refactors) Function-level locking requires language-aware parsing and a robust symbol table — nice-to-have for later, not v1 safety",
    "snippet": "Decision: Stick to file-level manifests as the hard safety boundary. Add soft function-level hints inside the prompt. Use the manifest as: \"these are the only files you're allowed to change\" Inside...",
    "tags": [
      "design",
      "manifests",
      "access-control"
    ],
    "type": "essay"
  },
  {
    "id": "manifests#2-manifest-generation-instrumentation-replan-on-miss",
    "url": "/manifests.html#2-manifest-generation-instrumentation-replan-on-miss",
    "section_id": "2-manifest-generation-instrumentation-replan-on-miss-3a1cdb91",
    "section_hash": "3a1cdb917f439fd2a75d81bd44e6e05b4f210eb8396c9fcd1afa0a0c03ac82ae",
    "title": "File Manifests",
    "section_title": "2. Manifest Generation: Instrumentation + replan on miss #",
    "content": "Decision: Treat false negatives as an instrumentation problem first. Log for each issue: manifest_files_predicted files_actually_touched (from diff) Compute: False positives: predicted but unused (acceptable — widens safety sandbox) False negatives: needed but not declared (problematic) Short-term behavior: If the agent attempts to touch a non-manifest file: Reject that attempt Spawn a replan: run planner again with the new file explicitly mentioned e.g. \"You also needed X.zig; update your manifest\" Goal: Tune planner + retrieval so false negatives are rare.",
    "snippet": "Decision: Treat false negatives as an instrumentation problem first. Log for each issue: manifest_files_predicted files_actually_touched (from diff) Compute: False positives: predicted but unused...",
    "tags": [
      "design",
      "manifests",
      "access-control"
    ],
    "type": "essay"
  },
  {
    "id": "manifests#3-manifest-violations-never-accept-as-is",
    "url": "/manifests.html#3-manifest-violations-never-accept-as-is",
    "section_id": "3-manifest-violations-never-accept-as-is-e47cb45c",
    "section_hash": "e47cb45c78fd5c0d0b5e65e68a0174c0ff08c0f9f188d7503ee2cc5be75a1613",
    "title": "File Manifests",
    "section_title": "3. Manifest Violations: Never accept as-is #",
    "content": "Decision: Never accept a violation directly. Use violations as hints for replanning. If the change is clearly correct: Record: \"Agent wanted to touch foo.zig too\" Re-run planner with that fact, generate a new manifest including foo.zig Re-run the implementation under the new manifest Only then accept For interactive mode, offer: Copy Agent touched out-of-manifest file X, the change looks good. ➜ [Accept & expand manifest] / [Re-run] / [Discard] The automation path should always have a manifest-consistent attempt before merging.",
    "snippet": "Decision: Never accept a violation directly. Use violations as hints for replanning. If the change is clearly correct: Record: \"Agent wanted to touch foo.zig too\" Re-run planner with that fact,...",
    "tags": [
      "design",
      "manifests",
      "access-control"
    ],
    "type": "essay"
  },
  {
    "id": "manifests#4-dynamic-expansion-yes-via-orchestrator-mediated-manifest-v2",
    "url": "/manifests.html#4-dynamic-expansion-yes-via-orchestrator-mediated-manifest-v2",
    "section_id": "4-dynamic-expansion-yes-via-orchestrator-mediated-manifest-v2-98c7f6c8",
    "section_hash": "98c7f6c8e9848a5cb98d7b74d35923a1403adc0b5d58c36ff2a4e1f2c7411721",
    "title": "File Manifests",
    "section_title": "4. Dynamic Expansion: Yes, via orchestrator-mediated \"manifest v2\" #",
    "content": "Decision: Allow agents to request additional files mid-run, but only through explicit orchestrator coordination with locking rules applied. Flow: Agent hits missing symbol/doc and outputs: Copy NEED_FILE: src/auth.zig or Copy NEED_DOC: design/auth.md Orchestrator checks locks: If auth.zig is free: Acquire lock Produce manifest v2 including auth.zig Re-prompt the agent with expanded manifest and new file content If locked by another task: Tell agent: \"You cannot access auth.zig because it's locked; proceed without or wait\" This preserves the isolation model: No silent expansion All expansions are explicit, logged, and coordinated with concurrency control",
    "snippet": "Decision: Allow agents to request additional files mid-run, but only through explicit orchestrator coordination with locking rules applied. Flow: Agent hits missing symbol/doc and outputs: Copy...",
    "tags": [
      "design",
      "manifests",
      "access-control"
    ],
    "type": "essay"
  },
  {
    "id": "manifests#implementation-notes",
    "url": "/manifests.html#implementation-notes",
    "section_id": "implementation-notes-17f895c4",
    "section_hash": "17f895c4f1986fa2142a9331ce88b5dcb084cb1eefe7967cb0d75c0e912b4f96",
    "title": "File Manifests",
    "section_title": "Implementation Notes #",
    "content": "See src/state.zig:Manifest and src/loop.zig:verifyManifestCompliance .",
    "snippet": "See src/state.zig:Manifest and src/loop.zig:verifyManifestCompliance .",
    "tags": [
      "design",
      "manifests",
      "access-control"
    ],
    "type": "essay"
  },
  {
    "id": "manifests#todo",
    "url": "/manifests.html#todo",
    "section_id": "todo-a4f0d1d8",
    "section_hash": "a4f0d1d8295ee1b1858baacd285a80bfe145a3b1496ef859ce65728b9c981fb2",
    "title": "File Manifests",
    "section_title": "TODO #",
    "content": "Add manifest instrumentation (predicted vs actual files) Implement replan-on-violation flow Add NEED_FILE / NEED_DOC signal parsing Add function-level hints to prompt builder",
    "snippet": "Add manifest instrumentation (predicted vs actual files) Implement replan-on-violation flow Add NEED_FILE / NEED_DOC signal parsing Add function-level hints to prompt builder",
    "tags": [
      "design",
      "manifests",
      "access-control"
    ],
    "type": "essay"
  },
  {
    "id": "verification#verification",
    "url": "/verification.html#verification",
    "section_id": "verification-56b5b01c",
    "section_hash": "56b5b01caa2f83838b0b631e7e5bf527d958d6682282fd2dd81ee6953c938f9e",
    "title": "Verification",
    "section_title": "Verification #",
    "content": "How noface determines whether an agent succeeded.",
    "snippet": "How noface determines whether an agent succeeded.",
    "tags": [
      "design",
      "verification",
      "testing",
      "acceptance"
    ],
    "type": "essay"
  },
  {
    "id": "verification#current-design",
    "url": "/verification.html#current-design",
    "section_id": "current-design-a6920eab",
    "section_hash": "a6920eab044f60f2c9567887cf8c053a1185c7294626b8e7152adea6b3570d2d",
    "title": "Verification",
    "section_title": "Current Design #",
    "content": "noface uses multiple verification layers: Test execution — runs the configured test command; failure = not done Manifest compliance — git diff checked against declared files; violations = rollback Build check — runs the configured build command (implicit in agent workflow) The agent is instructed to self-verify (run tests, check output) before committing.",
    "snippet": "noface uses multiple verification layers: Test execution — runs the configured test command; failure = not done Manifest compliance — git diff checked against declared files; violations = rollback...",
    "tags": [
      "design",
      "verification",
      "testing",
      "acceptance"
    ],
    "type": "essay"
  },
  {
    "id": "verification#relation-to-survey",
    "url": "/verification.html#relation-to-survey",
    "section_id": "relation-to-survey-61c73350",
    "section_hash": "61c733500744876fc02c003a0af51cac423374f0bda4eca9cd640423bd898eee",
    "title": "Verification",
    "section_title": "Relation to Survey #",
    "content": "The survey emphasizes automated testing as ground truth : \"This ensures that code isn't accepted as 'done' until it passes its tests. Even single-agent approaches like OpenAI's Codex have employed this idea (often called execute-and-fix): run the code, and if an error or failing test is detected, prompt the model to fix it.\" And manifest verification : \"Tools like MAID runner perform static analysis on the diff: did the agent only modify the allowed files and functions?\" The survey also discusses LLM critics as an additional layer — a second agent that reviews the code.",
    "snippet": "The survey emphasizes automated testing as ground truth : \"This ensures that code isn't accepted as 'done' until it passes its tests. Even single-agent approaches like OpenAI's Codex have employed...",
    "tags": [
      "design",
      "verification",
      "testing",
      "acceptance"
    ],
    "type": "essay"
  },
  {
    "id": "verification#1-test-coverage-require-tests-for-new-behavior",
    "url": "/verification.html#1-test-coverage-require-tests-for-new-behavior",
    "section_id": "1-test-coverage-require-tests-for-new-behavior-1d7faa27",
    "section_hash": "1d7faa279a4424da7191fdded93f7c18db6d58a1a0cd4193b65897674dbcafcc",
    "title": "Verification",
    "section_title": "1. Test Coverage: Require tests for new behavior #",
    "content": "Decision: Add test-centric enhancements for changes that add new behavior. For changes that add new behavior: Ask agent to write or update tests as part of the task Optionally run coverage diff if coverage tool exists: If new/changed lines have zero coverage → soft or hard gate Where tooling is limited: At least ensure: \"If tests exist in this module, check that they were updated\" Warn if tests not updated for behavioral changes",
    "snippet": "Decision: Add test-centric enhancements for changes that add new behavior. For changes that add new behavior: Ask agent to write or update tests as part of the task Optionally run coverage diff if...",
    "tags": [
      "design",
      "verification",
      "testing",
      "acceptance"
    ],
    "type": "essay"
  },
  {
    "id": "verification#2-review-pass-optional-but-recommended-for-non-trivial-changes",
    "url": "/verification.html#2-review-pass-optional-but-recommended-for-non-trivial-changes",
    "section_id": "2-review-pass-optional-but-recommended-for-non-trivial-changes-d2d846df",
    "section_hash": "d2d846df9840adfdfbd6d0af2a4c2056a819c8eb11585350b89c7d4a4cb403b8",
    "title": "Verification",
    "section_title": "2. Review Pass: Optional but recommended for non-trivial changes #",
    "content": "Decision: Add dedicated reviewer pass as an optional gate for non-trivial changes. Reviewer inputs: Issue description Old code vs new code diff Test results Reviewer outputs: Verdict: OK / NOT_OK / NEEDS_HUMAN Specific comments Trigger heuristics: Condition Action Large diffs Always review High-risk directories Always review Changes without tests Always review Small, well-tested changes Skip or downgrade",
    "snippet": "Decision: Add dedicated reviewer pass as an optional gate for non-trivial changes. Reviewer inputs: Issue description Old code vs new code diff Test results Reviewer outputs: Verdict: OK / NOT_OK /...",
    "tags": [
      "design",
      "verification",
      "testing",
      "acceptance"
    ],
    "type": "essay"
  },
  {
    "id": "verification#3-static-analysis-integrate-repos-existing-tools",
    "url": "/verification.html#3-static-analysis-integrate-repos-existing-tools",
    "section_id": "3-static-analysis-integrate-repos-existing-tools-7648da39",
    "section_hash": "7648da39a612dee3d56db2a87e78f44f5b2191f550a1d8cdb5e60903c4b03343",
    "title": "Verification",
    "section_title": "3. Static Analysis: Integrate repo's existing tools #",
    "content": "Decision: Integrate whatever static analysis the repo already has. Hard gates (block on failure): Type errors ( zig build , tsc , mypy ) Formatter failures ( zig fmt , prettier ) Soft gates (log, maybe create follow-up): New lint warnings Security scanner findings (unless critical) Copy [verification.static_analysis] hard_gates = [\"zig build\", \"zig fmt --check\"] soft_gates = [\"clippy\", \"eslint\"]",
    "snippet": "Decision: Integrate whatever static analysis the repo already has. Hard gates (block on failure): Type errors ( zig build , tsc , mypy ) Formatter failures ( zig fmt , prettier ) Soft gates (log,...",
    "tags": [
      "design",
      "verification",
      "testing",
      "acceptance"
    ],
    "type": "essay"
  },
  {
    "id": "verification#4-semantic-verification-judge-agent-for-complex-issues",
    "url": "/verification.html#4-semantic-verification-judge-agent-for-complex-issues",
    "section_id": "4-semantic-verification-judge-agent-for-complex-issues-f39b463e",
    "section_hash": "f39b463e5cba88a9a52a0fdab0ae8d2a3c6ae1b0abef13b4fbca059911c8c613",
    "title": "Verification",
    "section_title": "4. Semantic Verification: Judge agent for complex issues #",
    "content": "Decision: Add semantic check step for complex issues. Option 1: Explicit reasoning Have implementer/reviewer write: \"Here is how the change addresses the issue…\" Check coherence between explanation and diff Option 2: Judge agent Input: issue description + old code + new code Question: \"Does this change resolve the described behavior? Is anything missing or unrelated?\" Doesn't need to be perfect; even catching obvious mismatches is a big win.",
    "snippet": "Decision: Add semantic check step for complex issues. Option 1: Explicit reasoning Have implementer/reviewer write: \"Here is how the change addresses the issue…\" Check coherence between explanation...",
    "tags": [
      "design",
      "verification",
      "testing",
      "acceptance"
    ],
    "type": "essay"
  },
  {
    "id": "verification#5-confidence-signals-explicit-confidence-risk-metadata",
    "url": "/verification.html#5-confidence-signals-explicit-confidence-risk-metadata",
    "section_id": "5-confidence-signals-explicit-confidence-risk-metadata-ced49703",
    "section_hash": "ced49703508b9f4cab5454a78ccc3614ecea30956956676f0c5d050fc7e09678",
    "title": "Verification",
    "section_title": "5. Confidence Signals: Explicit confidence + risk metadata #",
    "content": "Decision: Ask agent to output explicit confidence and risks. Prompt addition: Copy After implementing, output: CONFIDENCE: X/5 RISKS: - [list any edge cases or uncertainties] Policy: If confidence ≤ 2/5 → require reviewer + maybe human Also watch for heuristics: Lots of TODOs in output \"I think / maybe\" in comments Weirdly small or huge diffs",
    "snippet": "Decision: Ask agent to output explicit confidence and risks. Prompt addition: Copy After implementing, output: CONFIDENCE: X/5 RISKS: - [list any edge cases or uncertainties] Policy: If confidence ≤...",
    "tags": [
      "design",
      "verification",
      "testing",
      "acceptance"
    ],
    "type": "essay"
  },
  {
    "id": "verification#6-partial-acceptance-hard-vs-soft-gates",
    "url": "/verification.html#6-partial-acceptance-hard-vs-soft-gates",
    "section_id": "6-partial-acceptance-hard-vs-soft-gates-47f2928e",
    "section_hash": "47f2928e23de5cf4ab51970b01fb9d70a24ca8cb3723ad373db3d03064836a0d",
    "title": "Verification",
    "section_title": "6. Partial Acceptance: Hard vs soft gates #",
    "content": "Decision: Define hard vs soft gates; never partially merge. Hard gates (must pass for merge): Tests passing No syntax/build errors Manifest compliance Soft gates (can proceed with warnings): Lint warnings Coverage thresholds Reviewer \"nit\" comments Policy: Automated merge requires all hard gates Soft gate failures: Either block and open follow-up issue, or Allow merge but log warnings and create cleanup issues Avoid partial merges of file subsets; use the branch model from failure-recovery instead.",
    "snippet": "Decision: Define hard vs soft gates; never partially merge. Hard gates (must pass for merge): Tests passing No syntax/build errors Manifest compliance Soft gates (can proceed with warnings): Lint...",
    "tags": [
      "design",
      "verification",
      "testing",
      "acceptance"
    ],
    "type": "essay"
  },
  {
    "id": "verification#7-human-review-gate-risk-classification-for-high-risk-changes",
    "url": "/verification.html#7-human-review-gate-risk-classification-for-high-risk-changes",
    "section_id": "7-human-review-gate-risk-classification-for-high-risk-changes-4fe391d6",
    "section_hash": "4fe391d6c5ca356e0a6f896b29915c4f706cad01d1f05815e494967b2239a24e",
    "title": "Verification",
    "section_title": "7. Human Review Gate: Risk classification for high-risk changes #",
    "content": "Decision: Build risk classification with hard human gate for sensitive areas. High-risk triggers: Files/directories: auth/ , payments/ , secrets/ , infra/ , prod-config/ Labels: security , compliance , breaking-change For high-risk changes: noface never auto-merges Opens PR or surfaces diff with \"requires human approval\" flag Optionally pre-annotated with AI reviewer's comments Copy [verification.human_required] paths = [\"src/auth/\", \"src/payments/\", \"config/prod/\"] labels = [\"security\", \"compliance\"]",
    "snippet": "Decision: Build risk classification with hard human gate for sensitive areas. High-risk triggers: Files/directories: auth/ , payments/ , secrets/ , infra/ , prod-config/ Labels: security , compliance...",
    "tags": [
      "design",
      "verification",
      "testing",
      "acceptance"
    ],
    "type": "essay"
  },
  {
    "id": "verification#implementation-notes",
    "url": "/verification.html#implementation-notes",
    "section_id": "implementation-notes-358700cd",
    "section_hash": "358700cd55483acb8f8d4ccdf9c01bce47c8da244668c0de1802e7a734e95a97",
    "title": "Verification",
    "section_title": "Implementation Notes #",
    "content": "See src/loop.zig:verifyManifestCompliance and prompt instructions for self-testing.",
    "snippet": "See src/loop.zig:verifyManifestCompliance and prompt instructions for self-testing.",
    "tags": [
      "design",
      "verification",
      "testing",
      "acceptance"
    ],
    "type": "essay"
  },
  {
    "id": "verification#todo",
    "url": "/verification.html#todo",
    "section_id": "todo-24fc4f72",
    "section_hash": "24fc4f7259a4c595674344dbb2a8c9c8139fb86529ad88ecc3376fe501d7ca33",
    "title": "Verification",
    "section_title": "TODO #",
    "content": "Add test coverage checking (if coverage tool available) Implement reviewer pass with risk-based triggering Add static analysis integration (hard/soft gates) Add judge agent for semantic verification Parse confidence/risk metadata from agent output Add human review gate for high-risk paths Implement soft gate → follow-up issue creation",
    "snippet": "Add test coverage checking (if coverage tool available) Implement reviewer pass with risk-based triggering Add static analysis integration (hard/soft gates) Add judge agent for semantic verification...",
    "tags": [
      "design",
      "verification",
      "testing",
      "acceptance"
    ],
    "type": "essay"
  },
  {
    "id": "multi-pass#multi-pass-architecture",
    "url": "/multi-pass.html#multi-pass-architecture",
    "section_id": "multi-pass-architecture-4890ea93",
    "section_hash": "4890ea93f322865f5f7bf563b10e06c6baa5e445868b4bde013f9e7c2b4ca8e9",
    "title": "Multi-Pass Architecture",
    "section_title": "Multi-Pass Architecture #",
    "content": "How noface uses multiple agent passes to improve quality.",
    "snippet": "How noface uses multiple agent passes to improve quality.",
    "tags": [
      "design",
      "planner",
      "reviewer",
      "passes"
    ],
    "type": "essay"
  },
  {
    "id": "multi-pass#current-design",
    "url": "/multi-pass.html#current-design",
    "section_id": "current-design-f91ec757",
    "section_hash": "f91ec757d6b85610c59afe6051300c38605cb8d7da9a2f600f59e7a2e1c5ca83",
    "title": "Multi-Pass Architecture",
    "section_title": "Current Design #",
    "content": "noface runs three types of passes: Planner pass (Codex) — every N iterations Analyzes issue backlog Generates file manifests for each issue Groups non-conflicting issues into parallel batches Implementation pass (Claude) — per issue Receives issue + manifest + context Implements the change, runs tests, commits Quality pass (Codex) — every M iterations Scans codebase for tech debt Creates new issues for findings",
    "snippet": "noface runs three types of passes: Planner pass (Codex) — every N iterations Analyzes issue backlog Generates file manifests for each issue Groups non-conflicting issues into parallel batches...",
    "tags": [
      "design",
      "planner",
      "reviewer",
      "passes"
    ],
    "type": "essay"
  },
  {
    "id": "multi-pass#relation-to-survey",
    "url": "/multi-pass.html#relation-to-survey",
    "section_id": "relation-to-survey-cbef4a0e",
    "section_hash": "cbef4a0e31b88b0d44e4cb8ba3a1b1b86437cef2bf65fc192ed9202804400afd",
    "title": "Multi-Pass Architecture",
    "section_title": "Relation to Survey #",
    "content": "This follows the Planner → Implementer pattern from orchestration-survey : \"The self-planning study by Jiang et al. (2025) exemplifies this: the LLM generates concise, structured planning steps from the intent, then in the implementation phase it 'generates code step by step, guided by the preceding planning steps'. This two-pass approach yielded significantly higher correctness than a single-pass solution.\" The survey also discusses Reviewer passes for catching mistakes. noface's quality pass is similar but focuses on proactive debt detection rather than reviewing specific changes.",
    "snippet": "This follows the Planner → Implementer pattern from orchestration-survey : \"The self-planning study by Jiang et al. (2025) exemplifies this: the LLM generates concise, structured planning steps from...",
    "tags": [
      "design",
      "planner",
      "reviewer",
      "passes"
    ],
    "type": "essay"
  },
  {
    "id": "multi-pass#1-pass-intervals-event-driven-adaptive-not-fixed",
    "url": "/multi-pass.html#1-pass-intervals-event-driven-adaptive-not-fixed",
    "section_id": "1-pass-intervals-event-driven-adaptive-not-fixed-65104122",
    "section_hash": "65104122a397e04bf942b1d9e7106f3d94fcc2d5ab7ba4cfafaf8d90175f184b",
    "title": "Multi-Pass Architecture",
    "section_title": "1. Pass Intervals: Event-driven + adaptive (not fixed) #",
    "content": "Decision: Move from fixed intervals to event-driven + adaptive. Planner: Run on-demand for new issues (always) Run periodically when: A manifest violation occurs (planner \"missed\" a file) Large codebase change merged (e.g., big refactor) Quality: Run when: A batch of N issues is completed A spike in failures or bug reports is observed Keep planner_interval and quality_interval as fallback periodic jobs, but teach orchestrator to invoke them in response to signals.",
    "snippet": "Decision: Move from fixed intervals to event-driven + adaptive. Planner: Run on-demand for new issues (always) Run periodically when: A manifest violation occurs (planner \"missed\" a file) Large...",
    "tags": [
      "design",
      "planner",
      "reviewer",
      "passes"
    ],
    "type": "essay"
  },
  {
    "id": "multi-pass#2-reviewer-pass-yes-for-high-risk-changes",
    "url": "/multi-pass.html#2-reviewer-pass-yes-for-high-risk-changes",
    "section_id": "2-reviewer-pass-yes-for-high-risk-changes-424f906f",
    "section_hash": "424f906f81284fdb9aefbe7bc7acdd21148046316ea9b956980b085932a2a23b",
    "title": "Multi-Pass Architecture",
    "section_title": "2. Reviewer Pass: Yes, for high-risk changes #",
    "content": "Decision: Add a dedicated reviewer pass, but only for certain classes of changes. Trigger reviewer when: Diff size > threshold Changed files in security/ , auth/ , infra/ , config/ No tests exist or coverage is low For low-risk, tiny changes where tests are strong: Tests + manifest is probably enough; skip reviewer to save cost",
    "snippet": "Decision: Add a dedicated reviewer pass, but only for certain classes of changes. Trigger reviewer when: Diff size > threshold Changed files in security/ , auth/ , infra/ , config/ No tests exist or...",
    "tags": [
      "design",
      "planner",
      "reviewer",
      "passes"
    ],
    "type": "essay"
  },
  {
    "id": "multi-pass#3-feedback-loops-quality-findings-feed-back-to-implementer",
    "url": "/multi-pass.html#3-feedback-loops-quality-findings-feed-back-to-implementer",
    "section_id": "3-feedback-loops-quality-findings-feed-back-to-implementer-15e0e916",
    "section_hash": "15e0e91684688b6fbe76851ec2720387cac8b795d2b174d3e40df1d9a06094bb",
    "title": "Multi-Pass Architecture",
    "section_title": "3. Feedback Loops: Quality findings feed back to implementer #",
    "content": "Decision: Quality findings should feed back directly to next implementation attempt. Implementation: When quality pass opens \"follow-up issues\", attach: Links to the original issue The quality agent's analysis (e.g., \"didn't handle null case in X\") When implementer picks up follow-up issue, include analysis in prompt: Copy The previous attempt failed because: [list]. Fix these specific problems. This turns the quality pass into a teacher, not just a bug generator.",
    "snippet": "Decision: Quality findings should feed back directly to next implementation attempt. Implementation: When quality pass opens \"follow-up issues\", attach: Links to the original issue The quality...",
    "tags": [
      "design",
      "planner",
      "reviewer",
      "passes"
    ],
    "type": "essay"
  },
  {
    "id": "multi-pass#4-pass-ordering-planner-on-demand-per-issue-not-just-periodic",
    "url": "/multi-pass.html#4-pass-ordering-planner-on-demand-per-issue-not-just-periodic",
    "section_id": "4-pass-ordering-planner-on-demand-per-issue-not-just-periodic-e3a2e535",
    "section_hash": "e3a2e53508d68915a5b8a5f2689ff93e2b4d015cb0f0547a90fb07e3b9306136",
    "title": "Multi-Pass Architecture",
    "section_title": "4. Pass Ordering: Planner on-demand per-issue (not just periodic) #",
    "content": "Decision: Run planner on-demand for manifests, keep periodic for backlog management. Flow: New issue arrives Planner runs, generates manifest + hints Implementation runs Periodic planner still exists for: Rebalancing / reprioritizing backlog Suggesting new refactor issues Relying only on periodic planner is where \"stale manifests\" come from.",
    "snippet": "Decision: Run planner on-demand for manifests, keep periodic for backlog management. Flow: New issue arrives Planner runs, generates manifest + hints Implementation runs Periodic planner still exists...",
    "tags": [
      "design",
      "planner",
      "reviewer",
      "passes"
    ],
    "type": "essay"
  },
  {
    "id": "multi-pass#5-diminishing-returns-data-driven-pass-value-metrics",
    "url": "/multi-pass.html#5-diminishing-returns-data-driven-pass-value-metrics",
    "section_id": "5-diminishing-returns-data-driven-pass-value-metrics-7ebeb943",
    "section_hash": "7ebeb9435c6651e0ad69093141d834d314fad3e9d6c016abb68ce3268dee1def",
    "title": "Multi-Pass Architecture",
    "section_title": "5. Diminishing Returns: Data-driven pass value metrics #",
    "content": "Decision: Log per-pass value metrics and make decisions data-driven. For each pass type (planner, reviewer, quality), count: How often it changes outcome: Reviewer finds a bug → implementation corrected Quality pass yields issues not caught otherwise Track: Average tokens / time per pass Compute: \"Bugs caught per thousand tokens\" or \"per minute\" If reviewer catches issues in 1/50 changes but costs a lot, restrict it to riskier paths.",
    "snippet": "Decision: Log per-pass value metrics and make decisions data-driven. For each pass type (planner, reviewer, quality), count: How often it changes outcome: Reviewer finds a bug → implementation...",
    "tags": [
      "design",
      "planner",
      "reviewer",
      "passes"
    ],
    "type": "essay"
  },
  {
    "id": "multi-pass#6-model-selection-abstract-behind-roles-default-to-strong-model",
    "url": "/multi-pass.html#6-model-selection-abstract-behind-roles-default-to-strong-model",
    "section_id": "6-model-selection-abstract-behind-roles-default-to-strong-model-842b6ca2",
    "section_hash": "842b6ca2318cd325df110cf050c2c73674e0789ec9e7cb321757a4315d790b5b",
    "title": "Multi-Pass Architecture",
    "section_title": "6. Model Selection: Abstract behind roles, default to strong model #",
    "content": "Decision: Short term, keep it simple and unify models where reasoning matters. Design: Copy [models] planner = \"claude\" # strong model for planning implementer = \"claude\" # strong model for implementation reviewer = \"claude\" # can use cheaper if cost-sensitive quality = \"codex\" # can use cheaper for triage Rationale: Planner quality matters a lot; using same strong model as implementer often improves plans Quality/triage can often use cheaper model if cost is a concern Make it easy to A/B test different combinations.",
    "snippet": "Decision: Short term, keep it simple and unify models where reasoning matters. Design: Copy [models] planner = \"claude\" # strong model for planning implementer = \"claude\" # strong model for...",
    "tags": [
      "design",
      "planner",
      "reviewer",
      "passes"
    ],
    "type": "essay"
  },
  {
    "id": "multi-pass#implementation-notes",
    "url": "/multi-pass.html#implementation-notes",
    "section_id": "implementation-notes-75c91769",
    "section_hash": "75c9176949ce9be0f86cff0cbcebcb4673c355207adcc6dac381ec75fb08bf44",
    "title": "Multi-Pass Architecture",
    "section_title": "Implementation Notes #",
    "content": "See src/loop.zig:runPlannerPass , runQualityPass , runIteration .",
    "snippet": "See src/loop.zig:runPlannerPass , runQualityPass , runIteration .",
    "tags": [
      "design",
      "planner",
      "reviewer",
      "passes"
    ],
    "type": "essay"
  },
  {
    "id": "multi-pass#todo",
    "url": "/multi-pass.html#todo",
    "section_id": "todo-14e628d6",
    "section_hash": "14e628d6118ad7afd7ca06e4174f3538c19bbafa2781aa7179e4e558165711c0",
    "title": "Multi-Pass Architecture",
    "section_title": "TODO #",
    "content": "Implement on-demand planner trigger for new issues Add reviewer pass with risk-based triggering Attach quality findings to follow-up issues Add per-pass metrics logging Abstract model selection behind role config",
    "snippet": "Implement on-demand planner trigger for new issues Add reviewer pass with risk-based triggering Attach quality findings to follow-up issues Add per-pass metrics logging Abstract model selection...",
    "tags": [
      "design",
      "planner",
      "reviewer",
      "passes"
    ],
    "type": "essay"
  },
  {
    "id": "index#noface",
    "url": "/index.html#noface",
    "section_id": "noface-bd03f5d6",
    "section_hash": "bd03f5d67ae34eeb8ed3a5e32d5280590496e3cf5b070011ed516ce82e381aea",
    "title": "noface",
    "section_title": "noface #",
    "content": "An autonomous agent orchestrator for software development. noface coordinates black-box agents (Claude Code, Codex) to work on issues from your backlog — in parallel, with conflict detection, crash recovery, and quality gates.",
    "snippet": "An autonomous agent orchestrator for software development. noface coordinates black-box agents (Claude Code, Codex) to work on issues from your backlog — in parallel, with conflict detection, crash...",
    "tags": [],
    "type": "essay"
  },
  {
    "id": "index#research",
    "url": "/index.html#research",
    "section_id": "research-0203af86",
    "section_hash": "0203af86003e51d710f7374618a31c10e95556d416e6c4e7ed1cb449191293a7",
    "title": "noface",
    "section_title": "Research #",
    "content": "orchestration-survey — Literature survey on design patterns for orchestrating code agents",
    "snippet": "orchestration-survey — Literature survey on design patterns for orchestrating code agents",
    "tags": [],
    "type": "essay"
  },
  {
    "id": "index#design",
    "url": "/index.html#design",
    "section_id": "design-bdbdb456",
    "section_hash": "bdbdb456012d830e904b9d24e0558721346f846742cb8ac3646dc1d1da3d00be",
    "title": "noface",
    "section_title": "Design #",
    "content": "Core infrastructure decisions: manifests — File access control via PRIMARY/READ/FORBIDDEN declarations parallel-execution — Batching, locking, and conflict avoidance context-injection — What information agents receive and how multi-pass — Planner → Implementer → Reviewer architecture failure-recovery — Retry, rollback, escalation strategies verification — How we know an agent succeeded",
    "snippet": "Core infrastructure decisions: manifests — File access control via PRIMARY/READ/FORBIDDEN declarations parallel-execution — Batching, locking, and conflict avoidance context-injection — What...",
    "tags": [],
    "type": "essay"
  },
  {
    "id": "index#quick-links",
    "url": "/index.html#quick-links",
    "section_id": "quick-links-f538eff3",
    "section_hash": "f538eff3bbd2905f6d04d28d33196451eb8c33eae0e3abd2d5e88df168936984",
    "title": "noface",
    "section_title": "Quick Links #",
    "content": "GitHub Issue Tracker",
    "snippet": "GitHub Issue Tracker",
    "tags": [],
    "type": "essay"
  }
]